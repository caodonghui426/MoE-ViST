{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n",
    "\n",
    "\n",
    "\n",
    "class config:\n",
    "    debug = False\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # root_path = r'E:\\\\Download\\\\xiangguan' # 存放数据的根目录\n",
    "    root_path = r'/home/junsheng/data/xiangguan' # 存放数据的根目录\n",
    "    n_fold = 5\n",
    "\n",
    "    # model_name = \"sensorViLOnlyTransformerSS\" #仅vilt图像\n",
    "    # model_name = \"sensorOnlyViLTransformerSS\"  #仅vilt传感器\n",
    "    model_name = \"sensorViLTransformerSS\"  #vilt图像+传感器\n",
    "    # wandb \n",
    "    wandb_name = \"vilt|水稻|290图像加传感器\"\n",
    "    code_file = \"test1.py\"\n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Sensor\n",
    "    # senser_input_num = 11 # 翔冠的传感器参数\n",
    "    senser_input_num = 19 # 天航的传感器参数\n",
    "    \n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-3 #0.0015#2e-3 #\n",
    "    weight_decay = 1e-4 # 0.01 ->1e-4\n",
    "    decay_power = 1\n",
    "    max_epoch = 50\n",
    "    # T_max = 8000/train_batch_size*max_epoch \n",
    "    T_max = 1000/train_batch_size*max_epoch \n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "# config = vars(config)\n",
    "# config = dict(config)\n",
    "config\n",
    "\n",
    "if config.debug:\n",
    "    config.max_epoch = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = pretrainedmodels.__dict__[\"resnet152\"](\n",
    "    num_classes=1000, pretrained='imagenet')\n",
    "features = list([resnet_model.conv1, resnet_model.bn1, resnet_model.relu, resnet_model.maxpool, resnet_model.layer1, resnet_model.layer2, resnet_model.layer3,resnet_model.layer4])\n",
    "\n",
    "conv = nn.Conv2d(2048, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "bn = nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "resnet_features = nn.Sequential(*features,conv,bn,relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNNF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnnf1 图片加传感器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNF1(torch.nn.Module):\n",
    "    \n",
    " \n",
    "    def __init__(self,sensor_nums):\n",
    "        super(DNNF1,self).__init__()\n",
    "        self.sensor_linear = torch.nn.Linear(sensor_nums,768)\n",
    "        \n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "\n",
    "        # DNNF1结构\n",
    "        self.linear1=torch.nn.Linear(768+768,64)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.linear2=torch.nn.Linear(64,128)\n",
    "        self.relu2=torch.nn.ReLU()\n",
    "        self.linear3=torch.nn.Linear(128,256)\n",
    "        self.relu3=torch.nn.ReLU()\n",
    "        self.linear4=torch.nn.Linear(256,512)\n",
    "        self.relu4=torch.nn.ReLU()\n",
    "        self.linear5=torch.nn.Linear(512,512)\n",
    "        self.relu5=torch.nn.ReLU()\n",
    "        self.linear6=torch.nn.Linear(512,1024)\n",
    "        self.relu6=torch.nn.ReLU()\n",
    "        self.linear7=torch.nn.Linear(1024,1)\n",
    "\n",
    "\n",
    "    def forward(self,batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,):\n",
    "        sensor_input = batch['sensor'].to(config.device)\n",
    "        sensor_feats = self.sensor_linear(sensor_input)\n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "            (\n",
    "                image_embeds,  # torch.Size([1, 217, 768])\n",
    "                image_masks,  # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "            torch.full_like(image_masks, image_token_type_idx)\n",
    "        )\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = image_embeds\n",
    "        co_masks = image_masks\n",
    "\n",
    "        x = co_embeds.to(config.device)  # torch.Size([1, 145, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)  # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x)  # torch.Size([1, 240, 768])\n",
    "        picture_feats = self.pooler(x)  # torch.Size([1, 768])#图像的特征数据\n",
    "        sensor_feats = sensor_feats.squeeze(dim=1) #torch.Size([1, 1, 768])->[1,768]\n",
    "\n",
    "        x = torch.cat([picture_feats, sensor_feats], dim=1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.linear5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.linear6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.linear7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNF1 picture only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNF1PictureOnly(torch.nn.Module):\n",
    "    \n",
    " \n",
    "    def __init__(self,sensor_nums):\n",
    "        super(DNNF1PictureOnly,self).__init__()\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "\n",
    "        # DNNF1结构\n",
    "        self.linear1=torch.nn.Linear(768,64)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.linear2=torch.nn.Linear(64,128)\n",
    "        self.relu2=torch.nn.ReLU()\n",
    "        self.linear3=torch.nn.Linear(128,256)\n",
    "        self.relu3=torch.nn.ReLU()\n",
    "        self.linear4=torch.nn.Linear(256,512)\n",
    "        self.relu4=torch.nn.ReLU()\n",
    "        self.linear5=torch.nn.Linear(512,512)\n",
    "        self.relu5=torch.nn.ReLU()\n",
    "        self.linear6=torch.nn.Linear(512,1024)\n",
    "        self.relu6=torch.nn.ReLU()\n",
    "        self.linear7=torch.nn.Linear(1024,1)\n",
    "\n",
    "\n",
    "    def forward(self,batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,):\n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "            (\n",
    "                image_embeds,  # torch.Size([1, 217, 768])\n",
    "                image_masks,  # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "            torch.full_like(image_masks, image_token_type_idx)\n",
    "        )\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = image_embeds\n",
    "        co_masks = image_masks\n",
    "\n",
    "        x = co_embeds.to(config.device)  # torch.Size([1, 145, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)  # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x)  # torch.Size([1, 240, 768])\n",
    "        picture_feats = self.pooler(x)  # torch.Size([1, 768])#图像的特征数据\n",
    "\n",
    "        x = picture_feats\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.linear5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.linear6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.linear7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNF1 sensor only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNF1SensorOnly(torch.nn.Module):\n",
    "    \n",
    " \n",
    "    def __init__(self,sensor_nums):\n",
    "        super(DNNF1SensorOnly,self).__init__()\n",
    "        self.sensor_linear = torch.nn.Linear(sensor_nums,768)\n",
    "\n",
    "        # DNNF1结构\n",
    "        self.linear1=torch.nn.Linear(768,64)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.linear2=torch.nn.Linear(64,128)\n",
    "        self.relu2=torch.nn.ReLU()\n",
    "        self.linear3=torch.nn.Linear(128,256)\n",
    "        self.relu3=torch.nn.ReLU()\n",
    "        self.linear4=torch.nn.Linear(256,512)\n",
    "        self.relu4=torch.nn.ReLU()\n",
    "        self.linear5=torch.nn.Linear(512,512)\n",
    "        self.relu5=torch.nn.ReLU()\n",
    "        self.linear6=torch.nn.Linear(512,1024)\n",
    "        self.relu6=torch.nn.ReLU()\n",
    "        self.linear7=torch.nn.Linear(1024,1)\n",
    "\n",
    "\n",
    "    def forward(self,batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,):\n",
    "        sensor_input = batch['sensor'].to(config.device)\n",
    "        sensor_feats = self.sensor_linear(sensor_input)\n",
    "\n",
    "        sensor_feats = sensor_feats.squeeze(dim=1) #torch.Size([1, 1, 768])->[1,768]\n",
    "\n",
    "        x = sensor_feats\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.linear5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.linear6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.linear7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNNF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnnf2 图片加传感器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNF2(torch.nn.Module):\n",
    "    \n",
    " \n",
    "    def __init__(self,sensor_nums):\n",
    "        super(DNNF2,self).__init__()\n",
    "        self.resnet = resnet_features\n",
    "        self.sensor_linear = torch.nn.Linear(sensor_nums,768)\n",
    "        \n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "\n",
    "        # DNNF2结构\n",
    "        self.linear1=torch.nn.Linear(64+64,512)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.linear2=torch.nn.Linear(512,1024)\n",
    "        self.relu2=torch.nn.ReLU()\n",
    "        self.linear3=torch.nn.Linear(1024,1024)\n",
    "        self.relu3=torch.nn.ReLU()\n",
    "        self.linear4=torch.nn.Linear(1024,1)\n",
    "\n",
    "        self.sensor_linear1 = torch.nn.Linear(768,32)\n",
    "        self.sensor_relu1=torch.nn.ReLU()\n",
    "        self.sensor_linear2 = torch.nn.Linear(32,64)\n",
    "        self.sensor_relu2=torch.nn.ReLU()\n",
    "\n",
    "        self.picture_linear1 = torch.nn.Linear(768,32)\n",
    "        self.picture_relu1 = torch.nn.ReLU()\n",
    "        self.picture_linear2 = torch.nn.Linear(32,64)\n",
    "        self.picture_relu2 = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,):\n",
    "        sensor_input = batch['sensor'].to(config.device)\n",
    "        sensor_feats = self.sensor_linear(sensor_input)\n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "            (\n",
    "                image_embeds,  # torch.Size([1, 217, 768])\n",
    "                image_masks,  # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "            torch.full_like(image_masks, image_token_type_idx)\n",
    "        )\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = image_embeds\n",
    "        co_masks = image_masks\n",
    "\n",
    "        x = co_embeds.to(config.device)  # torch.Size([1, 145, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)  # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x)  # torch.Size([1, 240, 768])\n",
    "        picture_feats = self.pooler(x)  # torch.Size([1, 768])#图像的特征数据\n",
    "        sensor_feats = sensor_feats.squeeze(dim=1) #torch.Size([1, 1, 768])->[1,768]\n",
    "\n",
    "        sensor_feats = self.sensor_linear1(sensor_feats)\n",
    "        sensor_feats = self.sensor_relu1(sensor_feats)\n",
    "        sensor_feats = self.sensor_linear2(sensor_feats)\n",
    "        sensor_feats = self.sensor_relu2(sensor_feats)\n",
    "\n",
    "        picture_feats = self.picture_linear1(picture_feats)\n",
    "        picture_feats = self.picture_relu1(picture_feats)\n",
    "        picture_feats = self.picture_linear2(picture_feats)\n",
    "        picture_feats = self.picture_relu2(picture_feats)\n",
    "        x = torch.cat([picture_feats, sensor_feats], dim=1)\n",
    "\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNF2 picture only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNF2PictureOnly(torch.nn.Module):\n",
    "    \n",
    " \n",
    "    def __init__(self,sensor_nums):\n",
    "        super(DNNF2PictureOnly,self).__init__()\n",
    "        self.sensor_linear = torch.nn.Linear(sensor_nums,768)\n",
    "        \n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "\n",
    "        # DNNF2结构\n",
    "        self.linear1=torch.nn.Linear(64,512)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.linear2=torch.nn.Linear(512,1024)\n",
    "        self.relu2=torch.nn.ReLU()\n",
    "        self.linear3=torch.nn.Linear(1024,1024)\n",
    "        self.relu3=torch.nn.ReLU()\n",
    "        self.linear4=torch.nn.Linear(1024,1)\n",
    "\n",
    "\n",
    "        self.picture_linear1 = torch.nn.Linear(768,32)\n",
    "        self.picture_relu1 = torch.nn.ReLU()\n",
    "        self.picture_linear2 = torch.nn.Linear(32,64)\n",
    "        self.picture_relu2 = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,):\n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "            (\n",
    "                image_embeds,  # torch.Size([1, 217, 768])\n",
    "                image_masks,  # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "            torch.full_like(image_masks, image_token_type_idx)\n",
    "        )\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = image_embeds\n",
    "        co_masks = image_masks\n",
    "\n",
    "        x = co_embeds.to(config.device)  # torch.Size([1, 145, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)  # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x)  # torch.Size([1, 240, 768])\n",
    "        picture_feats = self.pooler(x)  # torch.Size([1, 768])#图像的特征数据\n",
    "\n",
    "        picture_feats = self.picture_linear1(picture_feats)\n",
    "        picture_feats = self.picture_relu1(picture_feats)\n",
    "        picture_feats = self.picture_linear2(picture_feats)\n",
    "        picture_feats = self.picture_relu2(picture_feats)\n",
    "        \n",
    "        x = picture_feats\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNF2 sensor only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNF2SensorOnly(torch.nn.Module):\n",
    "    \n",
    " \n",
    "    def __init__(self,sensor_nums):\n",
    "        super(DNNF2SensorOnly,self).__init__()\n",
    "        self.sensor_linear = torch.nn.Linear(sensor_nums,768)\n",
    "\n",
    "\n",
    "        # DNNF2结构\n",
    "        self.linear1=torch.nn.Linear(64,512)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.linear2=torch.nn.Linear(512,1024)\n",
    "        self.relu2=torch.nn.ReLU()\n",
    "        self.linear3=torch.nn.Linear(1024,1024)\n",
    "        self.relu3=torch.nn.ReLU()\n",
    "        self.linear4=torch.nn.Linear(1024,1)\n",
    "\n",
    "        self.sensor_linear1 = torch.nn.Linear(768,32)\n",
    "        self.sensor_relu1=torch.nn.ReLU()\n",
    "        self.sensor_linear2 = torch.nn.Linear(32,64)\n",
    "        self.sensor_relu2=torch.nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,):\n",
    "        sensor_input = batch['sensor'].to(config.device)\n",
    "        sensor_feats = self.sensor_linear(sensor_input)\n",
    "\n",
    "        sensor_feats = sensor_feats.squeeze(dim=1) #torch.Size([1, 1, 768])->[1,768]\n",
    "\n",
    "        sensor_feats = self.sensor_linear1(sensor_feats)\n",
    "        sensor_feats = self.sensor_relu1(sensor_feats)\n",
    "        sensor_feats = self.sensor_linear2(sensor_feats)\n",
    "        sensor_feats = self.sensor_relu2(sensor_feats)\n",
    "\n",
    "\n",
    "        \n",
    "        x = sensor_feats\n",
    "\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3646170/1224883013.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0192]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = DNNF2(sensor_nums=config.senser_input_num)\n",
    "# model = DNNF2SensorOnly(sensor_nums=config.senser_input_num)\n",
    "# model = DNNF2PictureOnly(sensor_nums=config.senser_input_num)\n",
    "# model = DNNF1(sensor_nums=config.senser_input_num)\n",
    "model = DNNF1SensorOnly(sensor_nums=config.senser_input_num)\n",
    "\n",
    "model.to(config.device)\n",
    "sensor = torch.rand(config.senser_input_num)\n",
    "# sensor = torch.ones(config.senser_input_num)\n",
    "# print(sensor)\n",
    "sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "batch = {}\n",
    "batch['sensor'] = sensor\n",
    "batch['image'] = torch.randn((1,3,384,384))\n",
    "model(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch_junsheng_39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29fd19f11c6b89e267402bb3227bc1208f7e2c9719aa03eba13baf7684fe5867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
