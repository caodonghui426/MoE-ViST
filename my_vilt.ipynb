{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives, vilt_utils\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.50666661, -1.50139759,  0.11915306, -0.30950374, -0.53438475,\n",
       "        -0.06768657, -0.74793283,  0.20467237, -0.45091916, -1.08877908],\n",
       "       [ 0.56598458, -0.54937467, -0.38414117,  0.01657885,  1.64225682,\n",
       "        -1.36110744, -0.1206887 , -1.08808457, -0.45819237,  0.76005054],\n",
       "       [-1.69833089,  1.35044429,  0.53775696, -0.63435049,  0.18108845,\n",
       "         0.37900027,  1.97930293,  1.02006467,  1.04244296,  0.71419078],\n",
       "       [ 0.65731969,  0.33131101,  0.4899349 ,  0.98879714, -0.46065317,\n",
       "        -0.11007561, -1.01976618, -0.75229576,  0.37350945, -0.98106462],\n",
       "       [-1.6295783 ,  1.64856472,  0.96765882, -0.90734022,  0.67723962,\n",
       "         0.65437715,  0.29810909,  0.248463  ,  0.7030291 , -2.05005816],\n",
       "       [ 0.95916347, -1.66615008, -1.25896086, -0.43868742,  1.60106288,\n",
       "        -0.95531414,  0.66269453, -0.66592931, -1.47332752, -0.87699256],\n",
       "       [-0.02761034, -0.92245057, -0.83427532,  0.08651758,  1.32005139,\n",
       "         1.54634171,  0.29450088,  1.46640561, -0.10924273,  1.22226878],\n",
       "       [ 0.26019248,  0.91217651, -0.99909135, -1.03627535,  0.92798295,\n",
       "         0.69420377, -0.22471515, -0.39901103,  1.57650843, -0.60586114],\n",
       "       [ 0.38574915,  0.62188346, -0.31833194, -0.52207548, -1.26696789,\n",
       "         0.63723612,  0.31203601, -0.44331224, -0.46561761,  1.00376597],\n",
       "       [-2.83713463,  1.23019947,  0.76781086,  0.81220011,  1.24750129,\n",
       "        -0.79191306,  1.59940337,  0.16057779, -0.5945874 , -0.30217945]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__module__': '__main__',\n",
       " 'exp_name': 'vilt',\n",
       " 'seed': 101,\n",
       " 'batch_size': 4096,\n",
       " 'train_batch_size': 2,\n",
       " 'valid_batch_size': 4,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'train_transform_keys': ['pixelbert'],\n",
       " 'val_transform_keys': ['pixelbert'],\n",
       " 'img_size': 384,\n",
       " 'max_image_len': -1,\n",
       " 'patch_size': 32,\n",
       " 'draw_false_image': 1,\n",
       " 'image_only': False,\n",
       " 'vqav2_label_size': 3129,\n",
       " 'max_text_len': 40,\n",
       " 'tokenizer': 'bert-base-uncased',\n",
       " 'vocab_size': 30522,\n",
       " 'whole_word_masking': False,\n",
       " 'mlm_prob': 0.15,\n",
       " 'draw_false_text': 0,\n",
       " 'vit': 'vit_base_patch32_384',\n",
       " 'hidden_size': 768,\n",
       " 'num_heads': 12,\n",
       " 'num_layers': 12,\n",
       " 'mlp_ratio': 4,\n",
       " 'drop_rate': 0.1,\n",
       " 'optim_type': 'adamw',\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.01,\n",
       " 'decay_power': 1,\n",
       " 'max_epoch': 3,\n",
       " 'max_steps': 25000,\n",
       " 'warmup_steps': 2500,\n",
       " 'end_lr': 0,\n",
       " 'lr_mult': 1,\n",
       " 'get_recall_metric': False,\n",
       " 'resume_from': None,\n",
       " 'fast_dev_run': False,\n",
       " 'val_check_interval': 1.0,\n",
       " 'test_only': False,\n",
       " 'data_root': '',\n",
       " 'log_dir': 'result',\n",
       " 'per_gpu_batchsize': 0,\n",
       " 'num_gpus': 1,\n",
       " 'num_nodes': 1,\n",
       " 'load_path': 'weights/vilt_200k_mlm_itm.ckpt',\n",
       " 'num_workers': 1,\n",
       " 'precision': 16,\n",
       " '__dict__': <attribute '__dict__' of 'config' objects>,\n",
       " '__weakref__': <attribute '__weakref__' of 'config' objects>,\n",
       " '__doc__': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 2\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    decay_power = 1\n",
    "    max_epoch = 3\n",
    "    max_steps = 25000\n",
    "    warmup_steps = 2500\n",
    "    end_lr = 0\n",
    "    lr_mult = 1  # multiply lr for downstream heads\n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "    # PL Trainer Setting\n",
    "    resume_from = None\n",
    "    fast_dev_run = False\n",
    "    val_check_interval = 1.0\n",
    "    test_only = False\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "config = vars(config)\n",
    "config = dict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1.44303634665042, 0.15341536889974547, 0.4967...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sensor       image_path  label\n",
       "0  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "1  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "2  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "3  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "4  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "5  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "6  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "7  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "8  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1\n",
       "9  [1.44303634665042, 0.15341536889974547, 0.4967...  assets/vilt.png      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"sensor\":[np.random.randn(10)]*10,\"image_path\":\"assets/vilt.png\",\"label\":np.random.randint(1,10+1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransforms = transforms.Compose([\n",
    "    transforms.Resize((config[\"img_size\"],config[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.7136, 0.7118, 0.6788],\n",
    "    std=[0.3338, 0.3453, 0.3020],\n",
    "    \n",
    ")\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img =  Image.open(path).convert('RGB')\n",
    "    img = myTransforms(img)\n",
    "    return img\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df         = df\n",
    "        self.label      = label\n",
    "        self.sensors = df['sensor'].tolist()\n",
    "        self.img_paths  = df['image_path'].tolist()   \n",
    "        if self.label:\n",
    "            self.labels = df['label'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path  = self.img_paths[index]\n",
    "        img = load_img(img_path)\n",
    "        sensor = self.sensors[index]\n",
    "        sensor = torch.tensor(sensor).unsqueeze(0) #[1,n]\n",
    "        if self.label:\n",
    "            label = self.labels[index]\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
    "        else:\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BuildDataset(df=df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'],shuffle=True)\n",
    "valid_loader = DataLoader(train_dataset, batch_size=config['valid_batch_size'],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 384, 384])\n",
      "torch.Size([2, 1, 10])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_5108\\3759876696.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    }
   ],
   "source": [
    "img,sensor,label = next(iter(train_loader))\n",
    "print(img.shape)\n",
    "print(sensor.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self, config,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config[\"hidden_size\"]) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config[\"hidden_size\"])\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # if self.config[\"load_path\"] == \"\":\n",
    "        self.transformer = getattr(vit, self.config[\"vit\"])(\n",
    "                pretrained=False, config=self.config\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config[\"hidden_size\"])\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config[\"hidden_size\"],output_class_n)\n",
    "        # ===================== Downstream ===================== #\n",
    "        # if (\n",
    "        #     self.config[\"load_path\"] != \"\"\n",
    "        #     and not self.config[\"test_only\"]\n",
    "        # ):\n",
    "        #     ckpt = torch.load(self.config[\"load_path\"], map_location=\"cpu\")\n",
    "        #     if isinstance(ckpt,OrderedDict):\n",
    "\n",
    "        #         state_dict = ckpt\n",
    "        #     else:\n",
    "        #         state_dict = ckpt[\"state_dict\"]\n",
    "        #     self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        hs = self.config[\"hidden_size\"]\n",
    "\n",
    "        # vilt_utils.set_metrics(self) # 设定模型评价\n",
    "\n",
    "        # ===================== load downstream (test_only) ======================\n",
    "\n",
    "        if self.config[\"load_path\"] != \"\" and self.config[\"test_only\"]:\n",
    "            ckpt = torch.load(self.config[\"load_path\"], map_location=\"cpu\")\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config['device'])\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config['device'])\n",
    "       \n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=self.config[\"max_image_len\"],\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config['device']) # 序列数量\n",
    "        image_masks = image_masks.to(config['device'])\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config['device'])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config['device'])\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        m = nn.Softmax(dim=1)\n",
    "        cls_output = m(cls_output)\n",
    "\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = sensorViLTransformerSS(config,sensor_class_n= 10,output_class_n = 1)\n",
    "model.to(config['device'])\n",
    "print(config['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.mse_loss #均方误差损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
    "    for step, (img, sensor,label) in pbar:         \n",
    "        # img = img.to(device, dtype=torch.float)\n",
    "        # sensor  = sensor.to(device, dtype=torch.float)\n",
    "        # label  = label.to(device, dtype=torch.float)\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred = model(batch)\n",
    "        label = label.to(config['device'])\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        #一坨优化\n",
    "        optimizer.zero_grad()#每一次反向传播之前都要归零梯度\n",
    "        loss.backward()      #反向传播\n",
    "        optimizer.step()     #固定写法\n",
    "        scheduler.step()\n",
    "     \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_mem=f'{mem:0.2f} GB')\n",
    "\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "    history = defaultdict(list)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        \n",
    "        # val_loss, val_scores = valid_one_epoch(model, valid_loader, \n",
    "                                                #  device=CFG.device, \n",
    "                                                #  epoch=epoch)\n",
    "        # val_dice, val_jaccard = val_scores\n",
    "    \n",
    "        history['Train Loss'].append(train_loss)\n",
    "        # history['Valid Loss'].append(val_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        # deep copy the model\n",
    "        # if val_dice >= best_dice:\n",
    "            # print(f\"{c_}Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n",
    "            # best_dice    = val_dice\n",
    "            # best_jaccard = val_jaccard\n",
    "            # best_epoch   = epoch\n",
    "            # run.summary[\"Best Dice\"]    = best_dice\n",
    "            # run.summary[\"Best Jaccard\"] = best_jaccard\n",
    "            # run.summary[\"Best Epoch\"]   = best_epoch\n",
    "            # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            # PATH = os.path.join(CFG.model_output_path, f\"best_epoch-{fold:02d}.bin\")\n",
    "            # torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            # wandb.save(PATH)\n",
    "            # print(f\"Model Saved{sr_} to path:\",PATH)\n",
    "            \n",
    "        # last_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # PATH = os.path.join(CFG.model_output_path,f\"last_epoch-{fold:02d}.bin\")\n",
    "        # torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "    \n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.02, weight_decay=0.0001)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000, \n",
    "                                                   eta_min=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: GeForce GTX 1050\n",
      "\n",
      "Epoch 1/3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_5108\\3759876696.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
      "C:\\Windows\\Temp\\ipykernel_5108\\3170858215.py:16: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = criterion(y_pred['cls_output'], label)\n",
      "Train :   0%|          | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=0'>1</a>\u001b[0m model, history \u001b[39m=\u001b[39m run_training(model, optimizer, scheduler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=1'>2</a>\u001b[0m                                 device\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=2'>3</a>\u001b[0m                                 num_epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 26\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(model, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=7'>8</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=9'>10</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, scheduler, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=10'>11</a>\u001b[0m                                    dataloader\u001b[39m=\u001b[39;49mtrain_loader, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=11'>12</a>\u001b[0m                                    device\u001b[39m=\u001b[39;49mdevice, epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=13'>14</a>\u001b[0m \u001b[39m# val_loss, val_scores = valid_one_epoch(model, valid_loader, \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=14'>15</a>\u001b[0m                                         \u001b[39m#  device=CFG.device, \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=15'>16</a>\u001b[0m                                         \u001b[39m#  epoch=epoch)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=16'>17</a>\u001b[0m \u001b[39m# val_dice, val_jaccard = val_scores\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=18'>19</a>\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=12'>13</a>\u001b[0m batch \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m:img,\u001b[39m\"\u001b[39m\u001b[39msensor\u001b[39m\u001b[39m\"\u001b[39m:sensor}\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=14'>15</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred[\u001b[39m'\u001b[39;49m\u001b[39mcls_output\u001b[39;49m\u001b[39m'\u001b[39;49m], label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=17'>18</a>\u001b[0m \u001b[39m#一坨优化\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000023?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\u001b[39m#每一次反向传播之前都要归零梯度\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2660\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2657\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   2659\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[1;32m-> 2660\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                                device=config['device'],\n",
    "                                num_epochs=config['max_epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensorViLTransformerSS(\n",
      "  (sensor_linear): Linear(in_features=10, out_features=768, bias=True)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n",
      "img.shape: torch.Size([1, 3, 384, 576])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_7544\\582452387.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensor_feats': tensor([[[-7.5509e-01, -9.7323e-01,  3.7818e-01, -8.2663e-01, -2.4597e-01,\n",
      "          -1.1722e+00,  2.8178e-01, -2.2360e-01, -1.2265e+00,  1.5200e-01,\n",
      "           5.3258e-01,  1.4505e-01,  1.0187e-01, -8.5445e-01,  1.6123e+00,\n",
      "           7.1588e-01,  2.9983e-01, -9.0463e-03,  3.7473e-01,  1.3768e+00,\n",
      "           5.8432e-01,  3.6521e-01,  9.8619e-01, -7.3287e-01, -2.4330e+00,\n",
      "           1.3281e-01,  9.6839e-01,  8.2461e-01, -1.0126e+00,  8.3193e-02,\n",
      "          -2.3918e+00, -1.2170e-01,  1.1630e+00,  2.7446e-01,  5.3253e-01,\n",
      "           3.0294e-01, -1.4209e+00,  7.6304e-02, -4.9025e-01,  1.3335e+00,\n",
      "           1.5483e+00,  2.4080e+00,  9.3748e-01,  1.8624e-01, -2.9200e+00,\n",
      "          -5.1484e-01,  6.4928e-01,  1.1549e+00,  8.4096e-01, -1.9144e-01,\n",
      "          -4.4677e-01,  1.1525e+00, -1.0028e+00, -1.3222e+00, -1.4663e+00,\n",
      "          -1.2774e+00, -3.8994e-01, -2.2153e+00,  6.3054e-02, -1.3780e-01,\n",
      "           1.3096e+00, -1.4304e+00, -7.0851e-01,  2.8375e+00, -8.6473e-01,\n",
      "          -3.4861e-01,  2.8357e-01, -5.2282e-01, -4.5762e-01, -5.6705e-01,\n",
      "          -8.6717e-01,  2.7315e-01,  1.0267e+00,  4.5602e-01,  5.2579e-01,\n",
      "          -2.2256e-01, -1.3023e+00,  3.8987e-01, -7.3011e-01,  4.2074e-01,\n",
      "           1.1272e+00, -1.8986e+00,  2.0038e+00, -1.7763e-01, -5.1668e-01,\n",
      "          -1.3257e+00,  8.1088e-01, -8.0602e-01, -2.7478e+00, -1.1956e+00,\n",
      "          -7.5835e-02, -2.1216e+00,  4.0329e-01, -7.0439e-01,  1.2947e+00,\n",
      "          -2.9830e-01, -1.6660e+00, -2.4067e-01, -7.9816e-01, -4.5866e-01,\n",
      "          -1.8774e-01,  5.9385e-01,  1.4124e+00, -4.1217e-01,  1.1181e+00,\n",
      "           7.2827e-01, -2.7000e-01, -2.5519e-01, -1.7786e+00, -7.6652e-01,\n",
      "           5.3257e-01, -1.6098e+00,  9.3498e-01, -8.8438e-01, -2.2098e+00,\n",
      "           2.2408e+00, -2.2975e-01, -1.2132e+00, -2.8366e-02,  6.2778e-01,\n",
      "           9.8983e-01,  1.6100e+00, -2.7009e-01,  3.7313e-01,  1.3163e-01,\n",
      "           4.0959e-01, -5.5625e-01,  4.8121e-01, -1.1598e+00, -1.7599e+00,\n",
      "          -3.9629e-01, -1.0671e+00,  6.5088e-01, -1.0745e-02,  1.4789e+00,\n",
      "           1.7888e+00,  1.3173e+00,  7.1301e-01, -1.1246e+00,  1.2564e-01,\n",
      "           5.5756e-01, -3.3794e-01,  1.3599e+00, -3.3311e-01,  7.1023e-01,\n",
      "          -1.1409e+00,  1.6851e+00, -2.5085e-01, -9.0474e-02, -3.6571e-01,\n",
      "          -2.6316e+00,  1.1424e+00,  7.2622e-01,  2.0203e+00,  1.0532e+00,\n",
      "           2.0548e-01, -8.2473e-01, -2.7446e-01,  6.8000e-01,  1.2803e+00,\n",
      "          -6.9728e-01, -8.0470e-01, -8.6799e-01,  1.8679e+00, -8.0372e-01,\n",
      "           4.5219e-01, -7.1424e-01,  1.2789e+00,  5.7020e-01,  2.7244e-01,\n",
      "           6.3664e-01,  1.0961e+00,  9.7305e-01,  5.3209e-02, -5.2454e-01,\n",
      "          -6.1621e-01,  2.9250e-01,  6.3395e-01, -8.0442e-01, -2.0439e+00,\n",
      "           1.9693e-01,  2.4038e+00,  2.0088e-01, -1.1559e+00,  1.3781e+00,\n",
      "           1.5343e+00,  1.1908e-01, -1.4013e+00,  9.6799e-01,  1.0783e+00,\n",
      "          -1.5357e+00,  3.8129e-01, -5.8266e-01, -9.4696e-01, -1.8190e-01,\n",
      "          -4.3630e-01,  5.8177e-01, -4.8676e-01, -1.9934e+00,  5.0706e-01,\n",
      "          -7.4504e-01, -2.3544e-01,  6.6880e-01, -1.7625e+00,  1.4037e-01,\n",
      "           1.0233e+00, -2.3260e+00,  6.6005e-01, -6.3215e-01, -1.3168e+00,\n",
      "          -8.0731e-01,  2.1740e+00, -1.5158e+00,  6.2014e-01, -3.7502e-01,\n",
      "          -4.6849e-01,  5.1929e-01, -1.9765e-02,  8.8091e-02, -1.3288e+00,\n",
      "          -5.3702e-02, -2.0779e+00,  5.8651e-01, -1.1627e+00, -3.1392e-02,\n",
      "          -1.7639e-01, -4.4874e-01, -1.9769e+00,  1.9719e+00,  7.0998e-01,\n",
      "           9.5327e-01,  1.9906e-03,  1.0464e+00,  1.3873e+00,  8.1742e-01,\n",
      "          -2.2240e+00,  9.8455e-04, -1.4727e+00,  5.3958e-01,  2.3847e-01,\n",
      "           3.4370e+00, -1.2510e-01, -1.6920e-01,  1.3134e+00, -2.1873e-01,\n",
      "           1.3923e-01, -9.2037e-01,  1.1478e+00,  3.8240e-01,  6.9187e-01,\n",
      "           2.9094e-01,  2.9050e-01, -6.0590e-01,  1.3006e+00,  5.4197e-01,\n",
      "          -8.2672e-01, -2.5303e+00, -8.4740e-02,  2.1859e+00, -4.6347e-01,\n",
      "           9.4597e-01, -2.5775e-01,  1.4871e+00,  8.8220e-01,  1.0260e-01,\n",
      "           1.0544e+00,  1.2664e+00, -5.8858e-02,  8.0521e-01,  1.1136e+00,\n",
      "           9.9585e-01,  1.9984e+00,  5.7149e-01,  2.8937e-01,  1.5981e-01,\n",
      "           4.6299e-01, -7.1073e-01, -3.2352e-01,  2.4431e-01, -2.3584e+00,\n",
      "          -5.7306e-01,  4.7928e-03,  3.5898e-01,  1.2726e+00, -7.4484e-01,\n",
      "          -6.0863e-01, -1.2185e+00,  6.3420e-01, -9.4119e-01, -4.5142e-03,\n",
      "           1.3316e+00,  1.4418e+00,  6.3223e-02, -1.4210e+00,  2.3972e-01,\n",
      "           4.7627e-01,  1.1374e+00,  2.4847e-01,  1.4653e+00,  6.6931e-01,\n",
      "          -4.2260e-01, -8.1765e-01, -4.0820e-01,  1.0536e+00, -9.0083e-01,\n",
      "          -4.0381e-01, -4.9433e-01,  6.0850e-01, -1.6389e-01,  1.8885e-01,\n",
      "           4.2603e-01,  4.7845e-01,  6.0128e-01, -1.7016e+00, -4.1041e-01,\n",
      "          -1.2601e+00,  1.3333e-01,  8.6582e-01,  1.1863e+00, -1.1652e+00,\n",
      "           1.6329e+00,  2.6991e-01,  3.4163e-01, -6.7089e-01,  4.2659e-01,\n",
      "           4.1271e-01, -4.7784e-01, -1.1765e+00, -8.0918e-01,  1.3200e+00,\n",
      "          -1.1767e-02,  2.0482e+00, -2.2150e+00,  8.9684e-01,  9.8825e-01,\n",
      "           8.7784e-01,  3.7749e-01,  5.5607e-01, -1.7076e-01, -1.1733e+00,\n",
      "          -3.9001e-01,  1.6342e+00, -1.7828e+00, -1.8686e-01, -7.3946e-01,\n",
      "          -4.6696e-01,  1.5929e+00, -2.7472e-02, -5.2235e-01, -1.3782e+00,\n",
      "           1.2721e+00, -4.2706e-01,  1.1733e+00, -1.1945e+00, -1.0409e+00,\n",
      "           2.4131e+00, -8.4759e-01, -1.5699e+00,  3.8729e-01,  3.5248e-01,\n",
      "          -9.8192e-01, -7.9011e-02,  8.2042e-03, -3.7275e-01, -2.6818e-01,\n",
      "           7.2356e-01,  9.1962e-01, -3.1313e-02,  1.0251e+00, -8.4102e-02,\n",
      "           1.6811e+00,  3.1066e-01, -5.7633e-01,  5.1492e-01,  1.0860e-01,\n",
      "          -5.6864e-01,  2.2002e-01, -1.6260e+00,  3.1123e-01,  1.8794e+00,\n",
      "           2.9518e-01, -2.8110e-01,  1.4873e+00,  4.9515e-02,  7.8462e-01,\n",
      "          -6.7935e-01,  2.9264e-01, -3.3784e-01, -1.3807e+00, -1.5406e+00,\n",
      "          -5.6818e-01, -8.5609e-01,  5.4485e-01, -2.8791e-01,  1.0617e+00,\n",
      "          -9.2920e-01, -3.5199e-01,  4.6399e-01, -6.4806e-02, -1.1816e+00,\n",
      "          -2.1022e-01, -4.6091e-01, -2.3033e-01, -6.1691e-01, -1.2017e+00,\n",
      "           7.0589e-01,  1.3698e+00, -9.6441e-01, -2.7909e-01, -1.6670e+00,\n",
      "           1.1070e-01,  2.0887e-01,  1.8788e+00,  2.9304e+00, -1.6612e+00,\n",
      "           7.4436e-01, -1.6544e+00,  1.3592e+00, -4.8261e-01, -1.2300e+00,\n",
      "           6.9659e-01,  3.8248e-01,  3.8117e-01, -1.4325e+00,  9.9251e-01,\n",
      "           1.0811e+00, -5.7039e-01,  5.3746e-01,  2.5263e+00,  4.2508e-01,\n",
      "           1.3833e+00,  1.0289e+00,  9.2395e-01,  1.2691e+00,  7.9875e-01,\n",
      "          -9.4046e-01, -9.8358e-01,  4.2271e-01,  2.4700e-01, -9.3608e-01,\n",
      "          -1.5364e+00,  5.0366e-01, -7.2567e-01,  3.3402e-01, -9.9863e-01,\n",
      "           4.1397e-02, -6.7356e-01,  9.0958e-01, -1.4049e+00,  6.6888e-02,\n",
      "          -6.2136e-03,  1.9522e-01, -1.5776e-01,  9.5828e-01, -1.0610e+00,\n",
      "           2.2152e-01,  1.5198e-01,  1.5705e+00, -1.6703e-01,  1.7898e-01,\n",
      "           6.8940e-01, -9.3332e-01, -4.7634e-01,  1.7731e+00, -5.8141e-01,\n",
      "          -1.1535e+00,  1.0022e+00,  1.4291e+00, -7.0919e-01,  2.9440e-01,\n",
      "           6.4713e-01, -4.1388e-01,  2.2254e-02, -9.6272e-02,  1.6021e+00,\n",
      "          -8.6213e-01,  1.1503e+00, -1.6822e+00,  1.0220e-01,  1.7138e-01,\n",
      "          -5.2747e-01, -1.3323e+00, -4.3418e-01, -2.7282e-01, -3.9653e-01,\n",
      "          -6.2778e-02, -1.0378e+00, -1.0462e+00, -1.3655e+00,  1.8232e+00,\n",
      "          -9.7045e-01,  9.4347e-02, -7.9258e-01, -1.5684e+00,  2.7986e-01,\n",
      "          -1.8331e+00,  1.5800e+00,  7.6918e-01, -1.2250e+00, -2.8105e-01,\n",
      "           6.4662e-01,  1.6372e+00,  9.7463e-01,  7.3582e-02,  4.3817e-01,\n",
      "          -1.6445e-01, -6.1702e-01,  4.9808e-01, -1.8760e+00,  1.8133e-01,\n",
      "           2.6287e-01,  6.2113e-02, -1.9594e-01, -2.6914e-01,  9.9885e-02,\n",
      "          -6.4278e-01,  1.4640e-01,  6.4555e-01, -2.4474e-01,  2.3587e-01,\n",
      "           8.6397e-01,  1.6432e+00,  4.2484e-01,  7.9408e-01, -2.9623e-02,\n",
      "          -3.9563e-01,  6.5703e-01, -1.0261e+00, -2.1012e+00,  4.9596e-01,\n",
      "           1.4787e+00,  2.3893e-01,  4.3952e-02,  6.8544e-01, -4.2977e-01,\n",
      "          -8.4531e-01, -5.2246e-01,  2.2851e-01,  7.2262e-01, -2.8592e-01,\n",
      "          -7.7949e-02,  6.9260e-01,  1.1927e+00,  7.1877e-01, -1.5098e+00,\n",
      "          -3.0825e+00,  8.1588e-01,  1.0590e+00,  1.5615e+00,  1.1924e+00,\n",
      "          -1.2720e+00,  1.5451e+00,  1.0781e+00, -2.3177e+00, -1.1124e+00,\n",
      "           5.9851e-01, -1.8762e+00, -5.5058e-01,  6.6153e-01,  4.2495e-01,\n",
      "           6.9429e-01,  9.0129e-01, -1.6365e+00, -1.4046e-01, -6.6372e-01,\n",
      "          -1.2753e+00, -4.8547e-01, -6.9872e-01, -1.6332e-01,  1.1109e+00,\n",
      "           1.1106e-01, -4.1001e-01,  7.6951e-01,  1.6678e-01, -1.6026e+00,\n",
      "           3.1379e-01,  5.9072e-01, -1.0413e+00, -1.5082e+00,  7.3582e-01,\n",
      "          -3.7481e-01, -1.0713e+00,  4.7868e-01, -2.9579e-01, -6.2322e-01,\n",
      "           8.0131e-01, -4.3141e-01,  8.7189e-01, -5.5331e-02,  6.7681e-02,\n",
      "           4.6953e-01,  3.2947e-01, -3.9219e-01,  1.1017e+00,  8.5173e-01,\n",
      "           4.8321e-01,  4.9913e-01, -7.4523e-01,  6.8449e-01,  1.4041e+00,\n",
      "           6.4729e-01, -7.8274e-02, -4.8606e-01, -2.3876e+00,  7.4482e-01,\n",
      "          -1.6121e+00,  1.0084e+00, -4.3678e-01, -6.1744e-01,  4.7493e-01,\n",
      "           1.6452e+00, -2.9145e-01,  8.7143e-01, -7.8502e-01,  7.1516e-01,\n",
      "           1.3886e+00,  1.0186e+00,  7.1373e-01, -1.1902e+00,  4.1026e-01,\n",
      "          -6.5570e-01,  5.5723e-02,  1.1442e+00,  2.0432e+00, -3.5949e-01,\n",
      "           3.7026e-01, -3.5205e-01, -1.8626e+00,  9.5135e-01,  2.9348e-01,\n",
      "          -5.5576e-01,  1.0208e+00,  5.2446e-01,  4.4928e-01, -8.5705e-02,\n",
      "          -9.6789e-01, -1.1196e+00,  4.9442e-01, -6.1398e-01, -4.2620e-01,\n",
      "           9.5083e-01, -3.9921e-01, -5.9818e-01,  5.5270e-01,  9.4686e-01,\n",
      "           6.3123e-02,  5.3141e-01, -1.7091e-01,  1.3792e-01,  5.2713e-01,\n",
      "          -2.6268e-01, -5.2490e-01, -6.3131e-03,  7.6002e-02,  2.6897e-01,\n",
      "           7.7479e-01, -2.7779e-01, -1.1273e+00,  2.3819e+00,  3.3164e-01,\n",
      "           1.2161e-01, -2.7171e-01, -3.8074e-01, -3.5135e-02, -2.5186e+00,\n",
      "          -3.2521e-01,  1.6223e+00, -2.2849e-01,  5.8089e-01, -1.1573e+00,\n",
      "          -7.9205e-01,  4.1046e-01, -2.0835e+00, -8.7720e-01, -1.2839e-01,\n",
      "           8.0668e-01,  5.1780e-01,  5.2521e-01,  2.9290e-01, -3.4211e-01,\n",
      "          -1.7378e-01, -2.7447e-01, -2.9172e-01, -6.1953e-01,  1.5945e+00,\n",
      "          -3.6171e-01, -2.0926e+00, -6.9312e-01,  1.9785e+00, -1.4878e+00,\n",
      "           4.7779e-01, -8.4692e-01,  2.9898e-01, -1.8017e-01,  7.3688e-01,\n",
      "          -6.8989e-01, -4.2370e-01,  6.8961e-01,  1.6736e+00,  3.0753e-01,\n",
      "          -4.7748e-01, -1.0283e+00,  9.8472e-02, -1.2808e+00, -8.1987e-01,\n",
      "          -2.5917e-01, -1.5085e+00,  3.3069e-01, -5.7085e-01,  1.1373e-01,\n",
      "          -7.3559e-01, -7.0212e-01, -1.2219e+00, -5.5084e-01,  9.2668e-01,\n",
      "           6.2773e-01, -1.5250e+00, -1.7578e-01, -4.1475e-01,  1.3122e+00,\n",
      "          -2.5386e-01,  1.3750e-01, -2.1585e+00,  9.8840e-01,  9.6354e-01,\n",
      "          -1.3143e+00,  1.1945e+00, -2.2045e-01, -1.2394e+00,  4.2961e-01,\n",
      "          -1.2507e+00,  6.6183e-01,  3.2111e-01, -1.7032e-01, -8.4977e-01,\n",
      "           1.0961e+00, -1.0736e+00,  1.0167e+00,  1.6942e-01, -3.0925e-01,\n",
      "          -4.4577e-01, -1.2173e+00,  1.1374e-01,  6.7482e-01, -5.7687e-01,\n",
      "          -5.5466e-01, -1.5350e+00,  1.2602e+00, -2.3410e-01,  1.3793e+00,\n",
      "          -1.7013e+00, -9.8174e-01,  2.3757e+00, -1.6469e-01, -3.7006e-01,\n",
      "           3.5290e-01, -1.7514e-01, -5.5133e-02,  8.7821e-01, -2.1496e-01,\n",
      "           1.5384e+00,  5.7836e-01,  4.5666e-01,  1.4728e+00, -1.5222e-01,\n",
      "          -9.7551e-01,  2.9519e-01, -6.7642e-01]]], device='cuda:0'), 'image_feats': tensor([[[ 3.3617e-01, -1.8584e-01,  6.3020e-02,  ..., -1.6943e+00,\n",
      "          -5.2928e-01,  1.4250e+00],\n",
      "         [ 1.7134e-01, -6.9470e-01,  1.0684e+00,  ..., -2.3897e+00,\n",
      "          -1.5020e-01,  1.6571e+00],\n",
      "         [-5.6640e-01, -6.8063e-01,  5.3254e-04,  ..., -1.7552e+00,\n",
      "           2.6301e-01,  1.5087e+00],\n",
      "         ...,\n",
      "         [ 1.1911e-01,  2.2737e-01,  6.6809e-01,  ..., -2.0017e+00,\n",
      "          -3.7458e-01,  8.3411e-01],\n",
      "         [ 5.5526e-01,  2.1048e-01,  4.9998e-01,  ..., -2.0075e+00,\n",
      "          -1.2607e+00,  1.2877e+00],\n",
      "         [ 6.1452e-01,  1.7657e-01,  3.2970e-01,  ..., -1.8044e+00,\n",
      "          -1.2945e+00,  1.2905e+00]]], device='cuda:0'), 'cls_feats': tensor([[[-0.7679, -0.1178,  0.6410,  ..., -0.6833,  0.0816, -0.6184],\n",
      "         [-0.1595, -0.0970,  0.4645,  ..., -0.1327, -0.1388, -0.3185],\n",
      "         [-0.5642, -0.3412,  0.7779,  ...,  0.0673,  0.1207, -0.3853],\n",
      "         ...,\n",
      "         [ 0.0773, -0.1215,  0.5064,  ..., -0.2708,  0.0989, -0.6259],\n",
      "         [-0.4156, -0.4462,  0.4785,  ..., -0.5229, -0.2757, -0.6527],\n",
      "         [-0.4487, -0.4840,  0.5228,  ..., -0.5131, -0.2995, -0.6679]]],\n",
      "       device='cuda:0'), 'raw_cls_feats': tensor([[-7.5509e-01, -9.7323e-01,  3.7818e-01, -8.2663e-01, -2.4597e-01,\n",
      "         -1.1722e+00,  2.8178e-01, -2.2360e-01, -1.2265e+00,  1.5200e-01,\n",
      "          5.3258e-01,  1.4505e-01,  1.0187e-01, -8.5445e-01,  1.6123e+00,\n",
      "          7.1588e-01,  2.9983e-01, -9.0463e-03,  3.7473e-01,  1.3768e+00,\n",
      "          5.8432e-01,  3.6521e-01,  9.8619e-01, -7.3287e-01, -2.4330e+00,\n",
      "          1.3281e-01,  9.6839e-01,  8.2461e-01, -1.0126e+00,  8.3193e-02,\n",
      "         -2.3918e+00, -1.2170e-01,  1.1630e+00,  2.7446e-01,  5.3253e-01,\n",
      "          3.0294e-01, -1.4209e+00,  7.6304e-02, -4.9025e-01,  1.3335e+00,\n",
      "          1.5483e+00,  2.4080e+00,  9.3748e-01,  1.8624e-01, -2.9200e+00,\n",
      "         -5.1484e-01,  6.4928e-01,  1.1549e+00,  8.4096e-01, -1.9144e-01,\n",
      "         -4.4677e-01,  1.1525e+00, -1.0028e+00, -1.3222e+00, -1.4663e+00,\n",
      "         -1.2774e+00, -3.8994e-01, -2.2153e+00,  6.3054e-02, -1.3780e-01,\n",
      "          1.3096e+00, -1.4304e+00, -7.0851e-01,  2.8375e+00, -8.6473e-01,\n",
      "         -3.4861e-01,  2.8357e-01, -5.2282e-01, -4.5762e-01, -5.6705e-01,\n",
      "         -8.6717e-01,  2.7315e-01,  1.0267e+00,  4.5602e-01,  5.2579e-01,\n",
      "         -2.2256e-01, -1.3023e+00,  3.8987e-01, -7.3011e-01,  4.2074e-01,\n",
      "          1.1272e+00, -1.8986e+00,  2.0038e+00, -1.7763e-01, -5.1668e-01,\n",
      "         -1.3257e+00,  8.1088e-01, -8.0602e-01, -2.7478e+00, -1.1956e+00,\n",
      "         -7.5835e-02, -2.1216e+00,  4.0329e-01, -7.0439e-01,  1.2947e+00,\n",
      "         -2.9830e-01, -1.6660e+00, -2.4067e-01, -7.9816e-01, -4.5866e-01,\n",
      "         -1.8774e-01,  5.9385e-01,  1.4124e+00, -4.1217e-01,  1.1181e+00,\n",
      "          7.2827e-01, -2.7000e-01, -2.5519e-01, -1.7786e+00, -7.6652e-01,\n",
      "          5.3257e-01, -1.6098e+00,  9.3498e-01, -8.8438e-01, -2.2098e+00,\n",
      "          2.2408e+00, -2.2975e-01, -1.2132e+00, -2.8366e-02,  6.2778e-01,\n",
      "          9.8983e-01,  1.6100e+00, -2.7009e-01,  3.7313e-01,  1.3163e-01,\n",
      "          4.0959e-01, -5.5625e-01,  4.8121e-01, -1.1598e+00, -1.7599e+00,\n",
      "         -3.9629e-01, -1.0671e+00,  6.5088e-01, -1.0745e-02,  1.4789e+00,\n",
      "          1.7888e+00,  1.3173e+00,  7.1301e-01, -1.1246e+00,  1.2564e-01,\n",
      "          5.5756e-01, -3.3794e-01,  1.3599e+00, -3.3311e-01,  7.1023e-01,\n",
      "         -1.1409e+00,  1.6851e+00, -2.5085e-01, -9.0474e-02, -3.6571e-01,\n",
      "         -2.6316e+00,  1.1424e+00,  7.2622e-01,  2.0203e+00,  1.0532e+00,\n",
      "          2.0548e-01, -8.2473e-01, -2.7446e-01,  6.8000e-01,  1.2803e+00,\n",
      "         -6.9728e-01, -8.0470e-01, -8.6799e-01,  1.8679e+00, -8.0372e-01,\n",
      "          4.5219e-01, -7.1424e-01,  1.2789e+00,  5.7020e-01,  2.7244e-01,\n",
      "          6.3664e-01,  1.0961e+00,  9.7305e-01,  5.3209e-02, -5.2454e-01,\n",
      "         -6.1621e-01,  2.9250e-01,  6.3395e-01, -8.0442e-01, -2.0439e+00,\n",
      "          1.9693e-01,  2.4038e+00,  2.0088e-01, -1.1559e+00,  1.3781e+00,\n",
      "          1.5343e+00,  1.1908e-01, -1.4013e+00,  9.6799e-01,  1.0783e+00,\n",
      "         -1.5357e+00,  3.8129e-01, -5.8266e-01, -9.4696e-01, -1.8190e-01,\n",
      "         -4.3630e-01,  5.8177e-01, -4.8676e-01, -1.9934e+00,  5.0706e-01,\n",
      "         -7.4504e-01, -2.3544e-01,  6.6880e-01, -1.7625e+00,  1.4037e-01,\n",
      "          1.0233e+00, -2.3260e+00,  6.6005e-01, -6.3215e-01, -1.3168e+00,\n",
      "         -8.0731e-01,  2.1740e+00, -1.5158e+00,  6.2014e-01, -3.7502e-01,\n",
      "         -4.6849e-01,  5.1929e-01, -1.9765e-02,  8.8091e-02, -1.3288e+00,\n",
      "         -5.3702e-02, -2.0779e+00,  5.8651e-01, -1.1627e+00, -3.1392e-02,\n",
      "         -1.7639e-01, -4.4874e-01, -1.9769e+00,  1.9719e+00,  7.0998e-01,\n",
      "          9.5327e-01,  1.9906e-03,  1.0464e+00,  1.3873e+00,  8.1742e-01,\n",
      "         -2.2240e+00,  9.8455e-04, -1.4727e+00,  5.3958e-01,  2.3847e-01,\n",
      "          3.4370e+00, -1.2510e-01, -1.6920e-01,  1.3134e+00, -2.1873e-01,\n",
      "          1.3923e-01, -9.2037e-01,  1.1478e+00,  3.8240e-01,  6.9187e-01,\n",
      "          2.9094e-01,  2.9050e-01, -6.0590e-01,  1.3006e+00,  5.4197e-01,\n",
      "         -8.2672e-01, -2.5303e+00, -8.4740e-02,  2.1859e+00, -4.6347e-01,\n",
      "          9.4597e-01, -2.5775e-01,  1.4871e+00,  8.8220e-01,  1.0260e-01,\n",
      "          1.0544e+00,  1.2664e+00, -5.8858e-02,  8.0521e-01,  1.1136e+00,\n",
      "          9.9585e-01,  1.9984e+00,  5.7149e-01,  2.8937e-01,  1.5981e-01,\n",
      "          4.6299e-01, -7.1073e-01, -3.2352e-01,  2.4431e-01, -2.3584e+00,\n",
      "         -5.7306e-01,  4.7928e-03,  3.5898e-01,  1.2726e+00, -7.4484e-01,\n",
      "         -6.0863e-01, -1.2185e+00,  6.3420e-01, -9.4119e-01, -4.5142e-03,\n",
      "          1.3316e+00,  1.4418e+00,  6.3223e-02, -1.4210e+00,  2.3972e-01,\n",
      "          4.7627e-01,  1.1374e+00,  2.4847e-01,  1.4653e+00,  6.6931e-01,\n",
      "         -4.2260e-01, -8.1765e-01, -4.0820e-01,  1.0536e+00, -9.0083e-01,\n",
      "         -4.0381e-01, -4.9433e-01,  6.0850e-01, -1.6389e-01,  1.8885e-01,\n",
      "          4.2603e-01,  4.7845e-01,  6.0128e-01, -1.7016e+00, -4.1041e-01,\n",
      "         -1.2601e+00,  1.3333e-01,  8.6582e-01,  1.1863e+00, -1.1652e+00,\n",
      "          1.6329e+00,  2.6991e-01,  3.4163e-01, -6.7089e-01,  4.2659e-01,\n",
      "          4.1271e-01, -4.7784e-01, -1.1765e+00, -8.0918e-01,  1.3200e+00,\n",
      "         -1.1767e-02,  2.0482e+00, -2.2150e+00,  8.9684e-01,  9.8825e-01,\n",
      "          8.7784e-01,  3.7749e-01,  5.5607e-01, -1.7076e-01, -1.1733e+00,\n",
      "         -3.9001e-01,  1.6342e+00, -1.7828e+00, -1.8686e-01, -7.3946e-01,\n",
      "         -4.6696e-01,  1.5929e+00, -2.7472e-02, -5.2235e-01, -1.3782e+00,\n",
      "          1.2721e+00, -4.2706e-01,  1.1733e+00, -1.1945e+00, -1.0409e+00,\n",
      "          2.4131e+00, -8.4759e-01, -1.5699e+00,  3.8729e-01,  3.5248e-01,\n",
      "         -9.8192e-01, -7.9011e-02,  8.2042e-03, -3.7275e-01, -2.6818e-01,\n",
      "          7.2356e-01,  9.1962e-01, -3.1313e-02,  1.0251e+00, -8.4102e-02,\n",
      "          1.6811e+00,  3.1066e-01, -5.7633e-01,  5.1492e-01,  1.0860e-01,\n",
      "         -5.6864e-01,  2.2002e-01, -1.6260e+00,  3.1123e-01,  1.8794e+00,\n",
      "          2.9518e-01, -2.8110e-01,  1.4873e+00,  4.9515e-02,  7.8462e-01,\n",
      "         -6.7935e-01,  2.9264e-01, -3.3784e-01, -1.3807e+00, -1.5406e+00,\n",
      "         -5.6818e-01, -8.5609e-01,  5.4485e-01, -2.8791e-01,  1.0617e+00,\n",
      "         -9.2920e-01, -3.5199e-01,  4.6399e-01, -6.4806e-02, -1.1816e+00,\n",
      "         -2.1022e-01, -4.6091e-01, -2.3033e-01, -6.1691e-01, -1.2017e+00,\n",
      "          7.0589e-01,  1.3698e+00, -9.6441e-01, -2.7909e-01, -1.6670e+00,\n",
      "          1.1070e-01,  2.0887e-01,  1.8788e+00,  2.9304e+00, -1.6612e+00,\n",
      "          7.4436e-01, -1.6544e+00,  1.3592e+00, -4.8261e-01, -1.2300e+00,\n",
      "          6.9659e-01,  3.8248e-01,  3.8117e-01, -1.4325e+00,  9.9251e-01,\n",
      "          1.0811e+00, -5.7039e-01,  5.3746e-01,  2.5263e+00,  4.2508e-01,\n",
      "          1.3833e+00,  1.0289e+00,  9.2395e-01,  1.2691e+00,  7.9875e-01,\n",
      "         -9.4046e-01, -9.8358e-01,  4.2271e-01,  2.4700e-01, -9.3608e-01,\n",
      "         -1.5364e+00,  5.0366e-01, -7.2567e-01,  3.3402e-01, -9.9863e-01,\n",
      "          4.1397e-02, -6.7356e-01,  9.0958e-01, -1.4049e+00,  6.6888e-02,\n",
      "         -6.2136e-03,  1.9522e-01, -1.5776e-01,  9.5828e-01, -1.0610e+00,\n",
      "          2.2152e-01,  1.5198e-01,  1.5705e+00, -1.6703e-01,  1.7898e-01,\n",
      "          6.8940e-01, -9.3332e-01, -4.7634e-01,  1.7731e+00, -5.8141e-01,\n",
      "         -1.1535e+00,  1.0022e+00,  1.4291e+00, -7.0919e-01,  2.9440e-01,\n",
      "          6.4713e-01, -4.1388e-01,  2.2254e-02, -9.6272e-02,  1.6021e+00,\n",
      "         -8.6213e-01,  1.1503e+00, -1.6822e+00,  1.0220e-01,  1.7138e-01,\n",
      "         -5.2747e-01, -1.3323e+00, -4.3418e-01, -2.7282e-01, -3.9653e-01,\n",
      "         -6.2778e-02, -1.0378e+00, -1.0462e+00, -1.3655e+00,  1.8232e+00,\n",
      "         -9.7045e-01,  9.4347e-02, -7.9258e-01, -1.5684e+00,  2.7986e-01,\n",
      "         -1.8331e+00,  1.5800e+00,  7.6918e-01, -1.2250e+00, -2.8105e-01,\n",
      "          6.4662e-01,  1.6372e+00,  9.7463e-01,  7.3582e-02,  4.3817e-01,\n",
      "         -1.6445e-01, -6.1702e-01,  4.9808e-01, -1.8760e+00,  1.8133e-01,\n",
      "          2.6287e-01,  6.2113e-02, -1.9594e-01, -2.6914e-01,  9.9885e-02,\n",
      "         -6.4278e-01,  1.4640e-01,  6.4555e-01, -2.4474e-01,  2.3587e-01,\n",
      "          8.6397e-01,  1.6432e+00,  4.2484e-01,  7.9408e-01, -2.9623e-02,\n",
      "         -3.9563e-01,  6.5703e-01, -1.0261e+00, -2.1012e+00,  4.9596e-01,\n",
      "          1.4787e+00,  2.3893e-01,  4.3952e-02,  6.8544e-01, -4.2977e-01,\n",
      "         -8.4531e-01, -5.2246e-01,  2.2851e-01,  7.2262e-01, -2.8592e-01,\n",
      "         -7.7949e-02,  6.9260e-01,  1.1927e+00,  7.1877e-01, -1.5098e+00,\n",
      "         -3.0825e+00,  8.1588e-01,  1.0590e+00,  1.5615e+00,  1.1924e+00,\n",
      "         -1.2720e+00,  1.5451e+00,  1.0781e+00, -2.3177e+00, -1.1124e+00,\n",
      "          5.9851e-01, -1.8762e+00, -5.5058e-01,  6.6153e-01,  4.2495e-01,\n",
      "          6.9429e-01,  9.0129e-01, -1.6365e+00, -1.4046e-01, -6.6372e-01,\n",
      "         -1.2753e+00, -4.8547e-01, -6.9872e-01, -1.6332e-01,  1.1109e+00,\n",
      "          1.1106e-01, -4.1001e-01,  7.6951e-01,  1.6678e-01, -1.6026e+00,\n",
      "          3.1379e-01,  5.9072e-01, -1.0413e+00, -1.5082e+00,  7.3582e-01,\n",
      "         -3.7481e-01, -1.0713e+00,  4.7868e-01, -2.9579e-01, -6.2322e-01,\n",
      "          8.0131e-01, -4.3141e-01,  8.7189e-01, -5.5331e-02,  6.7681e-02,\n",
      "          4.6953e-01,  3.2947e-01, -3.9219e-01,  1.1017e+00,  8.5173e-01,\n",
      "          4.8321e-01,  4.9913e-01, -7.4523e-01,  6.8449e-01,  1.4041e+00,\n",
      "          6.4729e-01, -7.8274e-02, -4.8606e-01, -2.3876e+00,  7.4482e-01,\n",
      "         -1.6121e+00,  1.0084e+00, -4.3678e-01, -6.1744e-01,  4.7493e-01,\n",
      "          1.6452e+00, -2.9145e-01,  8.7143e-01, -7.8502e-01,  7.1516e-01,\n",
      "          1.3886e+00,  1.0186e+00,  7.1373e-01, -1.1902e+00,  4.1026e-01,\n",
      "         -6.5570e-01,  5.5723e-02,  1.1442e+00,  2.0432e+00, -3.5949e-01,\n",
      "          3.7026e-01, -3.5205e-01, -1.8626e+00,  9.5135e-01,  2.9348e-01,\n",
      "         -5.5576e-01,  1.0208e+00,  5.2446e-01,  4.4928e-01, -8.5705e-02,\n",
      "         -9.6789e-01, -1.1196e+00,  4.9442e-01, -6.1398e-01, -4.2620e-01,\n",
      "          9.5083e-01, -3.9921e-01, -5.9818e-01,  5.5270e-01,  9.4686e-01,\n",
      "          6.3123e-02,  5.3141e-01, -1.7091e-01,  1.3792e-01,  5.2713e-01,\n",
      "         -2.6268e-01, -5.2490e-01, -6.3131e-03,  7.6002e-02,  2.6897e-01,\n",
      "          7.7479e-01, -2.7779e-01, -1.1273e+00,  2.3819e+00,  3.3164e-01,\n",
      "          1.2161e-01, -2.7171e-01, -3.8074e-01, -3.5135e-02, -2.5186e+00,\n",
      "         -3.2521e-01,  1.6223e+00, -2.2849e-01,  5.8089e-01, -1.1573e+00,\n",
      "         -7.9205e-01,  4.1046e-01, -2.0835e+00, -8.7720e-01, -1.2839e-01,\n",
      "          8.0668e-01,  5.1780e-01,  5.2521e-01,  2.9290e-01, -3.4211e-01,\n",
      "         -1.7378e-01, -2.7447e-01, -2.9172e-01, -6.1953e-01,  1.5945e+00,\n",
      "         -3.6171e-01, -2.0926e+00, -6.9312e-01,  1.9785e+00, -1.4878e+00,\n",
      "          4.7779e-01, -8.4692e-01,  2.9898e-01, -1.8017e-01,  7.3688e-01,\n",
      "         -6.8989e-01, -4.2370e-01,  6.8961e-01,  1.6736e+00,  3.0753e-01,\n",
      "         -4.7748e-01, -1.0283e+00,  9.8472e-02, -1.2808e+00, -8.1987e-01,\n",
      "         -2.5917e-01, -1.5085e+00,  3.3069e-01, -5.7085e-01,  1.1373e-01,\n",
      "         -7.3559e-01, -7.0212e-01, -1.2219e+00, -5.5084e-01,  9.2668e-01,\n",
      "          6.2773e-01, -1.5250e+00, -1.7578e-01, -4.1475e-01,  1.3122e+00,\n",
      "         -2.5386e-01,  1.3750e-01, -2.1585e+00,  9.8840e-01,  9.6354e-01,\n",
      "         -1.3143e+00,  1.1945e+00, -2.2045e-01, -1.2394e+00,  4.2961e-01,\n",
      "         -1.2507e+00,  6.6183e-01,  3.2111e-01, -1.7032e-01, -8.4977e-01,\n",
      "          1.0961e+00, -1.0736e+00,  1.0167e+00,  1.6942e-01, -3.0925e-01,\n",
      "         -4.4577e-01, -1.2173e+00,  1.1374e-01,  6.7482e-01, -5.7687e-01,\n",
      "         -5.5466e-01, -1.5350e+00,  1.2602e+00, -2.3410e-01,  1.3793e+00,\n",
      "         -1.7013e+00, -9.8174e-01,  2.3757e+00, -1.6469e-01, -3.7006e-01,\n",
      "          3.5290e-01, -1.7514e-01, -5.5133e-02,  8.7821e-01, -2.1496e-01,\n",
      "          1.5384e+00,  5.7836e-01,  4.5666e-01,  1.4728e+00, -1.5222e-01,\n",
      "         -9.7551e-01,  2.9519e-01, -6.7642e-01]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0'), 'patch_index': (tensor([[[ 7, 14],\n",
      "         [ 9, 14],\n",
      "         [ 0,  2],\n",
      "         [ 8,  0],\n",
      "         [ 3,  7],\n",
      "         [ 2,  8],\n",
      "         [ 7, 15],\n",
      "         [ 2, 12],\n",
      "         [11,  8],\n",
      "         [ 8,  8],\n",
      "         [ 1, 10],\n",
      "         [ 8, 11],\n",
      "         [ 8, 17],\n",
      "         [ 7, 17],\n",
      "         [ 3,  0],\n",
      "         [ 4,  9],\n",
      "         [ 2,  4],\n",
      "         [ 8, 12],\n",
      "         [ 4, 16],\n",
      "         [ 1, 15],\n",
      "         [ 9,  4],\n",
      "         [11,  3],\n",
      "         [ 7,  4],\n",
      "         [ 0,  4],\n",
      "         [ 4,  0],\n",
      "         [10,  2],\n",
      "         [ 7, 12],\n",
      "         [ 4,  5],\n",
      "         [ 1, 11],\n",
      "         [ 3,  6],\n",
      "         [ 3, 14],\n",
      "         [ 4, 12],\n",
      "         [10,  1],\n",
      "         [ 9, 10],\n",
      "         [ 3,  9],\n",
      "         [ 1, 14],\n",
      "         [ 1,  5],\n",
      "         [ 8,  5],\n",
      "         [ 7,  7],\n",
      "         [10, 16],\n",
      "         [ 5,  3],\n",
      "         [10, 10],\n",
      "         [ 7,  1],\n",
      "         [ 6, 14],\n",
      "         [ 0,  3],\n",
      "         [ 0,  7],\n",
      "         [ 8,  2],\n",
      "         [ 3, 11],\n",
      "         [ 1, 12],\n",
      "         [ 7,  2],\n",
      "         [ 9,  6],\n",
      "         [ 5, 14],\n",
      "         [ 6, 13],\n",
      "         [ 7,  8],\n",
      "         [ 8, 15],\n",
      "         [ 0, 16],\n",
      "         [10,  0],\n",
      "         [ 3,  4],\n",
      "         [ 8, 16],\n",
      "         [ 6,  0],\n",
      "         [ 4, 17],\n",
      "         [ 9,  1],\n",
      "         [ 0, 11],\n",
      "         [ 4, 13],\n",
      "         [ 5, 10],\n",
      "         [ 5, 13],\n",
      "         [11, 13],\n",
      "         [ 0,  5],\n",
      "         [ 8, 13],\n",
      "         [ 6,  7],\n",
      "         [11, 14],\n",
      "         [ 3, 16],\n",
      "         [ 4, 15],\n",
      "         [ 4,  4],\n",
      "         [ 8,  4],\n",
      "         [ 3,  3],\n",
      "         [ 6, 15],\n",
      "         [ 1,  0],\n",
      "         [ 5,  4],\n",
      "         [ 2, 16],\n",
      "         [ 4,  7],\n",
      "         [ 3, 12],\n",
      "         [11, 12],\n",
      "         [ 8, 14],\n",
      "         [ 1, 13],\n",
      "         [ 9,  9],\n",
      "         [ 7, 16],\n",
      "         [ 9,  0],\n",
      "         [ 9, 16],\n",
      "         [11,  2],\n",
      "         [ 2, 17],\n",
      "         [11,  0],\n",
      "         [ 9, 12],\n",
      "         [11, 17],\n",
      "         [ 0, 12],\n",
      "         [ 8,  7],\n",
      "         [ 7,  6],\n",
      "         [ 7,  5],\n",
      "         [11, 16],\n",
      "         [ 5,  5],\n",
      "         [ 0, 14],\n",
      "         [10, 11],\n",
      "         [ 9,  5],\n",
      "         [ 9, 17],\n",
      "         [ 0,  8],\n",
      "         [ 1,  4],\n",
      "         [ 6,  4],\n",
      "         [ 4, 10],\n",
      "         [ 2,  3],\n",
      "         [ 0,  6],\n",
      "         [ 5, 17],\n",
      "         [ 6,  2],\n",
      "         [ 5,  2],\n",
      "         [ 2,  2],\n",
      "         [ 1, 16],\n",
      "         [ 6, 17],\n",
      "         [ 2,  6],\n",
      "         [10, 14],\n",
      "         [ 5,  8],\n",
      "         [ 5, 16],\n",
      "         [ 6,  8],\n",
      "         [ 5,  9],\n",
      "         [ 9, 15],\n",
      "         [ 1,  1],\n",
      "         [10, 15],\n",
      "         [ 6, 16],\n",
      "         [ 6, 12],\n",
      "         [ 0, 15],\n",
      "         [ 9,  8],\n",
      "         [ 3,  8],\n",
      "         [ 2, 13],\n",
      "         [11,  5],\n",
      "         [ 1, 17],\n",
      "         [ 2,  9],\n",
      "         [ 8,  1],\n",
      "         [ 8,  3],\n",
      "         [11, 11],\n",
      "         [11,  9],\n",
      "         [ 4,  8],\n",
      "         [11,  6],\n",
      "         [ 9, 13],\n",
      "         [ 7, 13],\n",
      "         [10, 13],\n",
      "         [ 8,  6],\n",
      "         [11,  4],\n",
      "         [ 7,  9],\n",
      "         [ 3, 13],\n",
      "         [ 6,  5],\n",
      "         [ 5,  6],\n",
      "         [10, 17],\n",
      "         [ 9,  3],\n",
      "         [ 3,  5],\n",
      "         [10,  4],\n",
      "         [ 1,  2],\n",
      "         [10,  8],\n",
      "         [ 1,  6],\n",
      "         [ 0, 13],\n",
      "         [ 6,  3],\n",
      "         [ 1,  7],\n",
      "         [10,  5],\n",
      "         [ 9, 11],\n",
      "         [ 7,  0],\n",
      "         [10,  3],\n",
      "         [ 2,  1],\n",
      "         [ 7,  3],\n",
      "         [ 6, 10],\n",
      "         [ 8, 10],\n",
      "         [11,  7],\n",
      "         [ 0,  9],\n",
      "         [ 2,  7],\n",
      "         [ 8,  9],\n",
      "         [ 3, 15],\n",
      "         [ 4,  6],\n",
      "         [11, 10],\n",
      "         [ 5, 11],\n",
      "         [11,  1],\n",
      "         [ 4,  3],\n",
      "         [ 6,  1],\n",
      "         [ 0,  1],\n",
      "         [ 5,  0],\n",
      "         [10, 12],\n",
      "         [ 5, 15],\n",
      "         [ 4, 14],\n",
      "         [ 7, 10],\n",
      "         [ 2, 15],\n",
      "         [ 3,  1],\n",
      "         [ 0,  0],\n",
      "         [ 4,  2],\n",
      "         [ 2, 11],\n",
      "         [10,  6],\n",
      "         [ 9,  2],\n",
      "         [ 6,  9],\n",
      "         [10,  9],\n",
      "         [ 2,  5],\n",
      "         [ 6,  6],\n",
      "         [ 5, 12],\n",
      "         [ 3,  2],\n",
      "         [ 6, 11],\n",
      "         [ 2, 14],\n",
      "         [ 2, 10],\n",
      "         [ 3, 10],\n",
      "         [ 5,  1],\n",
      "         [10,  7],\n",
      "         [ 9,  7],\n",
      "         [ 5,  7],\n",
      "         [11, 15],\n",
      "         [ 4,  1],\n",
      "         [ 2,  0],\n",
      "         [ 4, 11],\n",
      "         [ 1,  9],\n",
      "         [ 7, 11],\n",
      "         [ 1,  3],\n",
      "         [ 3, 17],\n",
      "         [ 1,  8],\n",
      "         [ 0, 17],\n",
      "         [ 0, 10]]]), (12, 18)), 'cls_output': Softmax(\n",
      "  dim=tensor([[[-0.0699, -0.1057, -0.0797,  ..., -0.0046, -0.3337, -0.1174],\n",
      "           [-0.1945,  0.2441,  0.2055,  ..., -0.2626, -0.1387, -0.2208],\n",
      "           [-0.1678, -0.2428,  0.4122,  ...,  0.1431, -0.2457, -0.0646],\n",
      "           ...,\n",
      "           [ 0.0900, -0.2486,  0.3441,  ..., -0.0887, -0.2175,  0.1614],\n",
      "           [-0.1123, -0.1232,  0.3236,  ..., -0.0893, -0.1007, -0.0373],\n",
      "           [-0.1358, -0.1656,  0.3347,  ..., -0.0651, -0.0927, -0.0532]]],\n",
      "         device='cuda:0')\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), 'embedding_test_dict.pt')\n",
    "print(model)\n",
    "model.eval()\n",
    "device = config[\"device\"]\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = dict()\n",
    "    batch[\"image\"] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n",
    "\n",
    "examples=[\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "        ],\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "        ],\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "        ],\n",
    "    ],\n",
    "\n",
    "n = 1\n",
    "sensor = torch.randn(1,1,10)\n",
    "out = infer(examples[0][n][0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Softmax(\n",
       "   dim=tensor([[[-1.8438e-01, -9.2471e-02,  4.3540e-01,  ..., -7.6145e-02,\n",
       "             -6.3851e-01, -4.9614e-02],\n",
       "            [-1.9364e-01,  2.4313e-01,  2.0601e-01,  ..., -2.6711e-01,\n",
       "             -1.3727e-01, -2.2247e-01],\n",
       "            [-6.7506e-03, -2.4224e-01,  3.1885e-01,  ...,  2.7405e-01,\n",
       "             -2.3918e-01,  1.7587e-01],\n",
       "            ...,\n",
       "            [ 4.6834e-02, -1.5985e-01,  3.6535e-01,  ...,  2.0709e-01,\n",
       "             -3.6177e-01,  1.5007e-01],\n",
       "            [-3.6224e-02, -2.2269e-01,  4.1821e-01,  ...,  2.3669e-01,\n",
       "             -3.3319e-01, -4.4112e-04],\n",
       "            [-9.7966e-02, -2.1492e-01,  5.8596e-01,  ...,  1.7200e-02,\n",
       "             -1.3107e-01,  2.6534e-01]]], device='cuda:0')\n",
       " )]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e32618030883af29da10316c76c83da9a02f65dadf9a9b09d160d0d4f5840e5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
