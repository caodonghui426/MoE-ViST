{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives, vilt_utils\n",
    "import vilt.modules.vision_transformer as vit\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "import functools\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.161093  , -0.24083708,  0.91282567, -0.97608361,  0.29592485,\n",
       "         1.15318448, -0.59924333, -0.03219254,  0.01972192, -0.07911064],\n",
       "       [ 1.02444176, -1.3482362 ,  0.94035703,  0.34383371, -0.63181084,\n",
       "         0.28753242, -0.26098508,  2.09037779,  1.56004152,  0.83282207],\n",
       "       [ 0.2323145 , -0.90450038, -0.30615317,  1.17343039, -0.38506551,\n",
       "        -0.70331913,  1.46763276, -0.55619138,  0.56193668, -0.56138982],\n",
       "       [ 0.52521358,  0.74834498,  1.88742697,  0.38882795, -0.78047017,\n",
       "         0.55418769, -1.13800381, -0.74340768,  0.61187822,  2.08931988],\n",
       "       [-0.6735744 , -0.33053795,  0.13347479, -1.90288583, -0.08608288,\n",
       "        -1.51133187, -0.77719436, -1.94840518, -0.27304251,  0.79108911],\n",
       "       [ 0.72127906,  0.35585508,  0.25127807, -0.18907931, -1.00928263,\n",
       "         0.40817041, -0.78491208,  1.38257991,  0.0681436 ,  1.64792824],\n",
       "       [-0.46853214, -0.65257996,  0.52348112, -0.19072742,  1.54590548,\n",
       "        -1.05373066,  0.62715803, -0.20530583, -1.13392497,  0.48012994],\n",
       "       [-1.19004209,  0.66354656,  0.44962102,  0.79247285,  2.24511986,\n",
       "         0.85805624,  1.47423468,  1.18582633,  1.25153543, -0.56383468],\n",
       "       [ 0.55963853,  0.235512  , -0.30389132,  0.2449308 , -0.04606553,\n",
       "        -1.63811257,  1.59393496,  0.31588512, -0.39913868, -1.44737317],\n",
       "       [-1.92339185,  1.509956  , -0.41086978,  1.9979913 , -0.18050767,\n",
       "         1.62100288,  0.68706252, -0.48521553,  1.49038447,  0.45425593]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__module__': '__main__',\n",
       " 'exp_name': 'vilt',\n",
       " 'seed': 101,\n",
       " 'batch_size': 4096,\n",
       " 'train_batch_size': 2,\n",
       " 'valid_batch_size': 4,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'train_transform_keys': ['pixelbert'],\n",
       " 'val_transform_keys': ['pixelbert'],\n",
       " 'img_size': 384,\n",
       " 'max_image_len': -1,\n",
       " 'patch_size': 32,\n",
       " 'draw_false_image': 1,\n",
       " 'image_only': False,\n",
       " 'vqav2_label_size': 3129,\n",
       " 'max_text_len': 40,\n",
       " 'tokenizer': 'bert-base-uncased',\n",
       " 'vocab_size': 30522,\n",
       " 'whole_word_masking': False,\n",
       " 'mlm_prob': 0.15,\n",
       " 'draw_false_text': 0,\n",
       " 'vit': 'vit_base_patch32_384',\n",
       " 'hidden_size': 768,\n",
       " 'num_heads': 12,\n",
       " 'num_layers': 12,\n",
       " 'mlp_ratio': 4,\n",
       " 'drop_rate': 0.1,\n",
       " 'optim_type': 'adamw',\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.01,\n",
       " 'decay_power': 1,\n",
       " 'max_epoch': 100,\n",
       " 'max_steps': 25000,\n",
       " 'warmup_steps': 2500,\n",
       " 'end_lr': 0,\n",
       " 'lr_mult': 1,\n",
       " 'get_recall_metric': False,\n",
       " 'resume_from': None,\n",
       " 'fast_dev_run': False,\n",
       " 'val_check_interval': 1.0,\n",
       " 'test_only': False,\n",
       " 'data_root': '',\n",
       " 'log_dir': 'result',\n",
       " 'per_gpu_batchsize': 0,\n",
       " 'num_gpus': 1,\n",
       " 'num_nodes': 1,\n",
       " 'load_path': 'weights/vilt_200k_mlm_itm.ckpt',\n",
       " 'num_workers': 8,\n",
       " 'precision': 16,\n",
       " '__dict__': <attribute '__dict__' of 'config' objects>,\n",
       " '__weakref__': <attribute '__weakref__' of 'config' objects>,\n",
       " '__doc__': None}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 2\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    decay_power = 1\n",
    "    max_epoch = 100\n",
    "    max_steps = 25000\n",
    "    warmup_steps = 2500\n",
    "    end_lr = 0\n",
    "    lr_mult = 1  # multiply lr for downstream heads\n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "    # PL Trainer Setting\n",
    "    resume_from = None\n",
    "    fast_dev_run = False\n",
    "    val_check_interval = 1.0\n",
    "    test_only = False\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 8\n",
    "    precision = 16\n",
    "\n",
    "config = vars(config)\n",
    "config = dict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.8292005181824278, -1.1510237389360418, 0.39...</td>\n",
       "      <td>assets/vilt.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sensor       image_path  label\n",
       "0  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "1  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "2  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "3  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "4  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "5  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "6  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "7  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "8  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1\n",
       "9  [0.8292005181824278, -1.1510237389360418, 0.39...  assets/vilt.png      1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"sensor\":[np.random.randn(10)]*10,\"image_path\":\"assets/vilt.png\",\"label\":np.random.randint(1,10+1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransforms = transforms.Compose([\n",
    "    transforms.Resize((config[\"img_size\"],config[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.7136, 0.7118, 0.6788],\n",
    "    std=[0.3338, 0.3453, 0.3020],\n",
    "    \n",
    ")\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img =  Image.open(path).convert('RGB')\n",
    "    img = myTransforms(img)\n",
    "    return img\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df         = df\n",
    "        self.label      = label\n",
    "        self.sensors = df['sensor'].tolist()\n",
    "        self.img_paths  = df['image_path'].tolist()   \n",
    "        if self.label:\n",
    "            self.labels = df['label'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path  = self.img_paths[index]\n",
    "        img = load_img(img_path)\n",
    "        sensor = self.sensors[index]\n",
    "        if self.label:\n",
    "            label = self.labels[index]\n",
    "\n",
    "            return torch.tensor(img), torch.tensor(sensor),torch.tensor(label)\n",
    "        else:\n",
    "            return torch.tensor(img), torch.tensor(sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BuildDataset(df=df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'],shuffle=True)\n",
    "valid_loader = DataLoader(train_dataset, batch_size=config['valid_batch_size'],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 384, 384])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_5640\\3605180798.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img), torch.tensor(sensor),torch.tensor(label)\n"
     ]
    }
   ],
   "source": [
    "img,sensor,label = next(iter(train_loader))\n",
    "print(img.shape)\n",
    "print(sensor.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()\n",
    "        self.config = config\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config[\"hidden_size\"]) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config[\"hidden_size\"])\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # if self.config[\"load_path\"] == \"\":\n",
    "        self.transformer = getattr(vit, self.config[\"vit\"])(\n",
    "                pretrained=False, config=self.config\n",
    "            )\n",
    "       \n",
    "\n",
    "        self.pooler = heads.Pooler(config[\"hidden_size\"])\n",
    "        self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config[\"hidden_size\"],output_class_n)\n",
    "        # ===================== Downstream ===================== #\n",
    "        # if (\n",
    "        #     self.config[\"load_path\"] != \"\"\n",
    "        #     and not self.config[\"test_only\"]\n",
    "        # ):\n",
    "        #     ckpt = torch.load(self.config[\"load_path\"], map_location=\"cpu\")\n",
    "        #     if isinstance(ckpt,OrderedDict):\n",
    "\n",
    "        #         state_dict = ckpt\n",
    "        #     else:\n",
    "        #         state_dict = ckpt[\"state_dict\"]\n",
    "        #     self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        hs = self.config[\"hidden_size\"]\n",
    "\n",
    "        # vilt_utils.set_metrics(self) # 设定模型评价\n",
    "\n",
    "        # ===================== load downstream (test_only) ======================\n",
    "\n",
    "        if self.config[\"load_path\"] != \"\" and self.config[\"test_only\"]:\n",
    "            ckpt = torch.load(self.config[\"load_path\"], map_location=\"cpu\")\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    " \n",
    "        sensor_embeds = self.sensor_linear(batch['sensor']) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"][0]\n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=self.config[\"max_image_len\"],\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        cls_output = nn.Softmax(self.classifier(cls_feats))\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # vilt_utils.set_task(self)\n",
    "        output = self(batch)\n",
    "        loss = self.loss(output['cls_output'], x)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        # vilt_utils.epoch_wrapup(self)\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # vilt_utils.set_task(self)\n",
    "        output = self(batch)\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        # vilt_utils.epoch_wrapup(self)\n",
    "        pass\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # vilt_utils.set_task(self)\n",
    "        # output = self(batch)\n",
    "        # ret = dict()\n",
    "        # return ret\n",
    "        pass\n",
    "\n",
    "    def test_epoch_end(self, outs):\n",
    "        model_name = self.config[\"load_path\"].split(\"/\")[-1][:-5]\n",
    "\n",
    "        # if self.config[\"loss_names\"][\"vqa\"] > 0:\n",
    "        #     objectives.vqa_test_wrapup(outs, model_name)\n",
    "        # vilt_utils.epoch_wrapup(self)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"定义优化器\n",
    "        \"\"\"\n",
    "        # return vilt_utils.set_schedule(self)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sensorViLTransformerSS(config,sensor_class_n= 12,output_class_n = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using environment variable NODE_RANK for node rank (0).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=config[\"num_gpus\"],\n",
    "    # num_nodes=config[\"num_nodes\"], # number of GPU nodes for distributed training.\n",
    "    # precision=config[\"precision\"], #  Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.\n",
    "    # accelerator=\"ddp\",\n",
    "    # benchmark=True, # If true enables cudnn.benchmark.\n",
    "    # deterministic=True,\n",
    "    max_epochs=config[\"max_epoch\"],\n",
    "    # max_steps=config[\"max_steps\"],\n",
    "    # callbacks=callbacks, # Add a list of callbacks.\n",
    "    # logger=logger,\n",
    "    # prepare_data_per_node=False,\n",
    "    # replace_sampler_ddp=False,\n",
    "    # accumulate_grad_batches=grad_steps,\n",
    "    # log_every_n_steps=10,\n",
    "    # flush_logs_every_n_steps=10,\n",
    "    # resume_from_checkpoint=config[\"resume_from\"],\n",
    "    # weights_summary=\"top\",\n",
    "    # fast_dev_run=config[\"fast_dev_run\"],\n",
    "    # val_check_interval=config[\"val_check_interval\"],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AttributeError' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:189\u001b[0m, in \u001b[0;36mTensorBoardLogger.log_metrics\u001b[1;34m(self, metrics, step)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperiment\u001b[39m.\u001b[39madd_scalar(k, v, step)\n\u001b[0;32m    190\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loggers\\base.py:39\u001b[0m, in \u001b[0;36mrank_zero_experiment.<locals>.experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m \u001b[39mreturn\u001b[39;00m get_experiment() \u001b[39mor\u001b[39;00m DummyExperiment()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:39\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m rank_zero_only\u001b[39m.\u001b[39mrank \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loggers\\base.py:38\u001b[0m, in \u001b[0;36mrank_zero_experiment.<locals>.experiment.<locals>.get_experiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m@rank_zero_only\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_experiment\u001b[39m():\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:143\u001b[0m, in \u001b[0;36mTensorBoardLogger.experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment \u001b[39m=\u001b[39m SummaryWriter(log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_dir, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_kwargs)\n\u001b[0;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:221\u001b[0m, in \u001b[0;36mSummaryWriter.__init__\u001b[1;34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_file_writer()\n\u001b[0;32m    223\u001b[0m \u001b[39m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:251\u001b[0m, in \u001b[0;36mSummaryWriter._get_file_writer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m FileWriter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_dir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_queue,\n\u001b[0;32m    252\u001b[0m                                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_secs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename_suffix)\n\u001b[0;32m    253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer\u001b[39m.\u001b[39mget_logdir(): \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer}\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:61\u001b[0m, in \u001b[0;36mFileWriter.__init__\u001b[1;34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m     60\u001b[0m log_dir \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(log_dir)\n\u001b[1;32m---> 61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_writer \u001b[39m=\u001b[39m EventFileWriter(\n\u001b[0;32m     62\u001b[0m     log_dir, max_queue, flush_secs, filename_suffix)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py:72\u001b[0m, in \u001b[0;36mEventFileWriter.__init__\u001b[1;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logdir \u001b[39m=\u001b[39m logdir\n\u001b[1;32m---> 72\u001b[0m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mmakedirs(logdir)\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_name \u001b[39m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     75\u001b[0m         logdir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[39m+\u001b[39m filename_suffix\n\u001b[0;32m     85\u001b[0m )  \u001b[39m# noqa E128\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(load_once(\u001b[39mself\u001b[39;49m), attr_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'io'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39m# if not _config[\"test_only\"]:\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#ch0000015?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model,train_dataloader\u001b[39m=\u001b[39mtrain_loader,val_dataloaders\u001b[39m=\u001b[39mvalid_loader)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:473\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# TRAIN\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mon_fit_start\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 473\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator_backend\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator_backend\u001b[39m.\u001b[39mteardown()\n\u001b[0;32m    476\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\accelerators\\gpu_accelerator.py:63\u001b[0m, in \u001b[0;36mGPUAccelerator.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[0;32m     62\u001b[0m \u001b[39m# set up training routine\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain_loop\u001b[39m.\u001b[39;49msetup_training(model)\n\u001b[0;32m     65\u001b[0m \u001b[39m# train or test\u001b[39;00m\n\u001b[0;32m     66\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_or_test()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py:157\u001b[0m, in \u001b[0;36mTrainLoop.setup_training\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m# log hyper-parameters\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlogger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[39m# save exp to get started (this is where the first experiment logs are written)\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mlogger\u001b[39m.\u001b[39;49mlog_hyperparams(ref_model\u001b[39m.\u001b[39;49mhparams_initial)\n\u001b[0;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_graph(ref_model)\n\u001b[0;32m    159\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39msave()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:39\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m rank_zero_only\u001b[39m.\u001b[39mrank \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:168\u001b[0m, in \u001b[0;36mTensorBoardLogger.log_hyperparams\u001b[1;34m(self, params, metrics)\u001b[0m\n\u001b[0;32m    165\u001b[0m     metrics \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mhp_metric\u001b[39m\u001b[39m\"\u001b[39m: metrics}\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m metrics:\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_metrics(metrics, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m    169\u001b[0m     exp, ssi, sei \u001b[39m=\u001b[39m hparams(params, metrics)\n\u001b[0;32m    170\u001b[0m     writer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39m_get_file_writer()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:39\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m rank_zero_only\u001b[39m.\u001b[39mrank \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:192\u001b[0m, in \u001b[0;36mTensorBoardLogger.log_metrics\u001b[1;34m(self, metrics, step)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    191\u001b[0m     m \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m you tried to log \u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m}\u001b[39;00m\u001b[39m which is not currently supported. Try a dict or a scalar/tensor.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mtype\u001b[39m(e)(e\u001b[39m.\u001b[39;49mmessage \u001b[39m+\u001b[39m m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AttributeError' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "# if not _config[\"test_only\"]:\n",
    "trainer.fit(model,train_dataloader=train_loader,val_dataloaders=valid_loader)\n",
    "# else:\n",
    "    # trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensorViLTransformerSS(\n",
      "  (sensor_linear): Linear(in_features=12, out_features=768, bias=True)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_11636\\3993119832.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 384, 576])\n",
      "{'sensor_feats': tensor([[[ 1.8914e-01, -2.1735e-01,  8.5255e-01,  7.8298e-01, -4.3437e-01,\n",
      "          -9.9152e-01,  1.1405e+00,  1.2102e+00, -3.5829e-03,  1.8816e-01,\n",
      "          -1.7830e+00,  1.3710e+00, -6.4979e-01,  1.2611e+00,  8.5380e-01,\n",
      "          -5.1768e-01, -9.0013e-01, -1.4700e+00, -1.1877e+00,  1.1053e+00,\n",
      "          -1.5470e-01, -4.5214e-01, -6.6290e-01, -8.2520e-01, -5.8180e-01,\n",
      "          -1.2151e+00, -1.8665e+00,  1.1014e+00,  1.5840e-01,  7.0513e-01,\n",
      "           7.5815e-01, -1.1844e+00, -1.2740e+00, -1.9909e-01,  7.3141e-01,\n",
      "           5.3559e-01, -1.2204e-01, -7.3502e-01, -5.4481e-01,  3.3622e-01,\n",
      "           1.3244e+00,  1.0313e+00, -1.3161e+00, -9.8514e-01,  5.0247e-01,\n",
      "          -2.6440e+00,  1.3343e+00,  2.8385e-01, -6.6683e-01, -2.1295e-01,\n",
      "           1.1146e-01,  2.5773e-01, -7.6813e-01,  1.3432e-02, -7.8291e-02,\n",
      "          -6.4118e-01, -3.0406e-01, -2.7933e-01, -4.9455e-01, -7.9737e-01,\n",
      "           4.8862e-01, -1.6017e+00,  4.5616e-02,  1.2852e-01, -3.5534e-01,\n",
      "           5.0064e-01,  1.2754e+00, -1.2589e+00,  6.2993e-01, -7.5133e-01,\n",
      "           5.2190e-01,  5.7029e-01, -6.4824e-01, -4.6513e-01,  1.7397e-01,\n",
      "          -5.8831e-01,  1.1852e+00, -4.5569e-01,  1.0603e+00, -1.8375e-02,\n",
      "          -4.9949e-01, -1.2847e+00,  1.6924e+00, -1.5352e+00,  2.4939e-01,\n",
      "           6.7886e-01,  1.8251e-02, -8.6937e-03, -1.1066e+00, -1.2033e+00,\n",
      "          -5.9378e-01,  1.5640e-01, -1.0685e+00,  5.7379e-01,  9.7807e-01,\n",
      "          -1.3645e+00,  5.3530e-01,  1.7626e-01,  4.7728e-01, -1.4157e+00,\n",
      "          -1.0075e+00, -5.3611e-01,  1.9340e+00, -1.7657e+00, -1.8910e-01,\n",
      "           8.2328e-01,  8.4448e-01, -2.2276e+00,  9.0489e-01, -6.4669e-01,\n",
      "           5.3152e-01,  1.5801e+00, -6.0394e-01, -9.8520e-02,  5.3477e-01,\n",
      "          -1.7394e-01,  6.6862e-01,  1.1678e+00, -1.1180e-01,  7.5388e-01,\n",
      "          -1.1493e+00, -9.1123e-01, -4.4047e-01,  1.3192e+00,  4.1134e-01,\n",
      "          -1.4427e+00, -2.1898e-02,  3.0476e-01, -8.0926e-01,  5.2712e-01,\n",
      "          -2.4103e-01, -4.5474e-01,  4.4463e-01, -9.2391e-01,  1.3500e+00,\n",
      "           1.3104e+00, -2.1827e-01, -6.7447e-01,  1.0268e+00, -1.6455e+00,\n",
      "           7.3753e-01, -1.0066e+00, -2.6431e-01, -3.9765e-01,  9.1322e-02,\n",
      "          -2.1819e-02,  1.4516e+00,  9.0437e-01, -1.9092e+00,  1.9088e+00,\n",
      "          -5.9874e-01,  3.8362e-02,  1.1966e-01, -8.5761e-01,  2.4668e+00,\n",
      "          -1.0748e+00,  7.0560e-01, -4.9809e-01, -1.6450e+00,  6.6465e-01,\n",
      "          -2.3304e-01, -1.7917e-01,  8.8541e-01, -1.2430e+00, -1.7078e+00,\n",
      "          -9.9595e-02,  7.1037e-01,  5.0880e-01, -3.6718e-01, -1.4640e+00,\n",
      "           2.4973e-01, -9.5384e-01,  4.8832e-02, -1.3636e+00, -3.0455e-01,\n",
      "          -1.7374e+00,  2.2744e+00, -1.4202e+00, -6.0189e-01, -2.7875e-01,\n",
      "          -1.0942e+00, -1.1437e+00,  2.4685e+00, -1.0017e+00,  1.2032e+00,\n",
      "           1.1385e-01, -1.3064e+00, -9.0674e-02,  5.9708e-01,  1.3152e+00,\n",
      "          -1.5614e+00, -2.9447e-01, -2.0251e-01, -1.1082e+00,  4.4503e-01,\n",
      "          -7.7376e-01, -2.8024e-01, -6.6477e-01, -1.2452e+00,  6.0086e-01,\n",
      "          -4.3244e-01,  1.1261e+00, -1.0565e+00,  2.0493e-01,  1.4919e-01,\n",
      "           9.7864e-01, -2.8177e+00,  1.4764e+00,  1.1833e+00,  1.9852e+00,\n",
      "          -5.2643e-01, -2.2712e-01, -1.4056e-01, -1.6355e+00,  7.0809e-02,\n",
      "           2.5168e-01, -5.8031e-01,  1.0403e+00,  9.7952e-01,  6.0929e-01,\n",
      "          -1.0119e+00,  4.5209e-01,  1.1244e+00, -2.4012e-01,  2.9348e-01,\n",
      "          -1.7701e-01, -1.7219e-01,  1.0666e-01, -1.8008e-01,  1.3283e+00,\n",
      "          -6.1238e-01,  2.6802e-01, -6.2041e-01, -2.5920e+00,  3.8670e-01,\n",
      "           8.0854e-01, -6.8211e-02, -6.8373e-02, -5.8167e-01, -3.0487e+00,\n",
      "           6.4038e-01,  1.5422e+00,  1.3273e+00, -7.0839e-01,  4.9784e-01,\n",
      "           4.7091e-01, -1.6697e+00, -1.7389e+00, -9.5921e-02,  9.2121e-01,\n",
      "           4.2697e-01, -1.2327e+00,  1.8867e-02, -1.4314e-01, -1.5740e+00,\n",
      "          -5.3723e-01, -1.5045e+00,  1.7231e+00,  1.0551e+00, -5.8574e-01,\n",
      "          -2.7458e+00,  9.4770e-01, -7.0174e-02,  5.8300e-01,  2.9411e-01,\n",
      "          -3.0651e-02, -9.8715e-01, -4.9715e-01, -4.9194e-01,  4.3904e-01,\n",
      "          -5.6256e-01, -1.8311e+00, -9.8708e-01, -1.0895e+00,  3.8159e-01,\n",
      "          -1.2679e+00, -8.5537e-01, -2.9960e-02,  2.5119e+00, -3.3735e-01,\n",
      "          -2.0201e+00, -3.7752e-01,  1.0377e-01, -4.5703e-01, -1.5010e+00,\n",
      "           1.0118e+00,  1.7851e+00, -1.0936e-01,  8.0994e-01, -1.1943e+00,\n",
      "           1.9385e+00, -1.4950e-01, -2.9377e-01, -1.4506e+00, -6.6784e-01,\n",
      "          -2.0624e+00,  1.1300e+00,  1.2249e+00, -2.5745e-01, -1.3580e-01,\n",
      "          -1.3019e+00, -5.6963e-01, -3.7000e-01, -1.9712e-01, -6.8866e-01,\n",
      "          -9.7986e-01, -6.3387e-01,  1.2959e+00,  2.4582e-01,  8.2192e-01,\n",
      "           9.2341e-01, -8.5542e-02,  8.9503e-01, -1.0474e+00,  2.7299e+00,\n",
      "           1.1049e+00,  1.1082e+00,  8.5202e-01,  1.0676e-01, -1.6310e-01,\n",
      "          -7.9308e-01, -2.8111e+00, -1.6286e-02, -5.9292e-01, -1.4141e+00,\n",
      "          -7.4058e-01,  1.5050e+00,  3.0351e-01,  7.0391e-01,  5.6099e-01,\n",
      "           1.2788e+00, -1.0151e+00, -3.8940e-01,  1.2114e+00,  1.7235e-01,\n",
      "          -1.3934e+00,  1.4214e-01, -1.5539e+00, -4.7705e-01, -2.0107e-01,\n",
      "           5.1737e-01,  4.5248e-01, -3.1565e-01,  1.7143e+00,  1.0975e+00,\n",
      "           1.6400e+00, -5.0238e-01,  1.0490e+00,  8.1858e-01,  1.1583e+00,\n",
      "           9.3227e-02,  8.1205e-02,  1.5435e+00,  9.1901e-01,  1.0855e+00,\n",
      "          -5.7090e-01,  5.8131e-01,  4.0467e-01, -6.7619e-01,  1.7438e+00,\n",
      "          -8.3540e-01,  1.3628e+00,  2.8027e-01,  5.8497e-01,  2.2489e+00,\n",
      "          -1.1250e+00, -1.1383e+00,  9.3503e-01,  6.5489e-01,  7.4130e-01,\n",
      "          -1.2203e+00,  8.5096e-01, -6.4574e-01,  6.3889e-01, -9.5148e-01,\n",
      "          -4.2341e-01,  1.3222e+00,  3.4383e-01, -8.1987e-01, -1.8029e-01,\n",
      "          -1.2967e+00, -2.0508e+00, -8.4069e-01, -1.4096e+00, -6.6758e-01,\n",
      "           6.4392e-01,  6.3644e-01, -9.2489e-01,  1.0850e+00,  6.7400e-02,\n",
      "          -2.8510e-01,  5.5134e-01,  1.4983e+00, -1.4135e+00,  1.7541e+00,\n",
      "           4.4188e-01,  1.8207e+00, -5.9658e-01,  1.5468e-01, -2.3706e-01,\n",
      "          -3.3558e-01, -1.7438e+00, -8.2266e-01, -1.4848e+00,  5.1557e-01,\n",
      "           9.3779e-01,  1.2867e+00, -8.5333e-01,  5.4711e-01,  9.6911e-01,\n",
      "           2.0865e+00,  1.8779e+00,  1.4166e+00,  1.7335e-01, -9.5915e-02,\n",
      "           9.3557e-01, -8.3773e-02, -9.6599e-02, -6.4023e-01, -4.8290e-01,\n",
      "          -1.6831e-01,  8.0311e-01, -1.0912e+00, -7.1184e-01, -9.9941e-01,\n",
      "           2.4587e-01,  2.5156e+00, -8.3495e-01,  1.7179e-01,  2.1467e-01,\n",
      "           2.9124e-01, -6.1840e-01, -2.7166e+00,  9.6906e-02, -3.5402e-01,\n",
      "           7.7622e-01, -8.7859e-01,  1.4919e+00, -1.1382e+00, -5.3122e-01,\n",
      "          -6.6127e-01, -9.7543e-01,  3.2159e-01, -3.0993e-01, -6.4086e-01,\n",
      "          -5.4569e-01, -2.2172e+00,  1.2511e+00,  2.6732e-01, -1.8016e+00,\n",
      "          -2.7174e-02, -4.7302e-01,  3.9626e-01,  1.1247e+00,  1.5914e-01,\n",
      "           8.4659e-01, -1.0259e+00, -1.1902e+00, -5.3448e-03, -9.1189e-01,\n",
      "          -6.1509e-01, -8.9558e-01,  1.5736e+00, -6.4655e-01, -1.0306e+00,\n",
      "          -6.0610e-01,  2.4788e+00, -7.1951e-02,  3.7908e-01,  3.2637e-01,\n",
      "           1.7535e+00,  4.7062e-01, -2.8230e-01,  1.3392e+00,  1.1419e+00,\n",
      "          -1.5650e-01,  9.8326e-01,  2.5390e-01,  7.3165e-01,  1.3475e+00,\n",
      "          -6.1201e-01, -9.7115e-01,  8.2015e-01,  2.8834e-01,  7.2748e-02,\n",
      "           8.2096e-02,  1.0522e-01, -1.2469e+00, -4.0489e-01,  1.6870e+00,\n",
      "           2.0807e-01,  3.2647e-01,  8.6556e-01,  5.2858e-01,  1.5795e+00,\n",
      "           1.5141e+00,  5.9084e-01,  8.4838e-01,  3.5207e-01,  1.3715e+00,\n",
      "           4.3704e-01, -3.9102e-01, -3.9459e-01,  1.7541e-01, -2.0083e-01,\n",
      "           9.8534e-01, -9.9782e-01, -1.9880e+00,  5.5017e-01,  6.8539e-01,\n",
      "           9.5570e-01,  1.9598e+00,  4.6553e-01, -7.4715e-01, -2.5830e-01,\n",
      "          -4.5271e-02, -9.9500e-01,  7.0199e-02, -1.2143e+00, -1.7874e-01,\n",
      "          -1.4952e+00,  1.3012e+00,  9.5228e-01,  4.6716e-02, -8.4319e-01,\n",
      "          -8.4183e-01,  6.8555e-01,  3.4453e-01,  1.1041e+00,  1.5358e+00,\n",
      "          -1.2697e+00, -2.6857e-01,  7.8946e-01, -6.4109e-01, -2.9078e-01,\n",
      "           6.2779e-02,  8.9780e-01,  4.7209e-01, -7.8663e-01, -1.3347e+00,\n",
      "           1.3487e+00, -1.8979e+00,  4.0163e-01, -3.8484e-01,  1.8912e+00,\n",
      "          -1.1453e+00, -5.0207e-01,  2.4453e-02, -1.4218e+00, -2.4351e-02,\n",
      "          -4.3177e-01,  2.1467e-01, -5.9991e-02, -7.9255e-01, -4.2164e-01,\n",
      "          -5.6286e-01, -1.9324e-01, -1.4438e+00,  2.9775e+00,  5.4152e-01,\n",
      "          -2.0633e-01, -1.0600e-01,  5.9384e-01, -3.9282e-01, -9.3391e-01,\n",
      "          -1.6650e+00,  1.0015e+00,  2.9172e-01,  1.4159e+00,  6.0496e-01,\n",
      "          -6.8626e-02,  4.0170e-01,  7.0261e-01, -5.3553e-04,  3.4430e-01,\n",
      "           9.7466e-01, -7.6821e-01, -5.4593e-01,  1.7289e-01, -7.2383e-01,\n",
      "           1.3517e-01, -1.1063e+00, -1.9684e-01,  2.6796e-01, -9.1107e-01,\n",
      "           1.3376e+00, -1.3310e+00, -2.2052e+00,  9.4421e-01,  7.0152e-01,\n",
      "          -3.8085e-01,  6.3128e-01, -1.0958e-01,  5.5696e-01, -1.3194e+00,\n",
      "           1.2821e+00,  3.1376e-01, -1.5934e+00, -1.6551e+00, -3.7859e-01,\n",
      "          -1.2653e+00, -2.9073e-01,  9.7406e-01,  8.4697e-01,  4.6142e-01,\n",
      "          -7.0729e-01,  6.9224e-01,  1.1812e+00,  3.4167e-01,  4.0578e-01,\n",
      "           2.2782e-01, -2.1966e+00,  2.9547e-01, -2.0527e-01, -2.4542e-01,\n",
      "           1.9924e-02, -4.3126e-01, -1.6301e+00, -7.9188e-01, -6.8111e-01,\n",
      "          -6.4806e-01,  8.7972e-01,  2.4612e-01,  8.8949e-01,  5.5849e-01,\n",
      "           1.2161e+00, -5.2092e-01,  5.3265e-01,  8.6378e-01,  8.4785e-01,\n",
      "          -8.4069e-01,  2.0698e+00,  1.1632e+00,  2.3445e-01,  1.4491e+00,\n",
      "           3.8487e-02, -7.8888e-01, -8.8485e-01,  1.0045e+00,  4.9586e-01,\n",
      "          -1.0455e+00, -1.1998e-01, -4.8991e-02,  4.9541e-01,  1.3558e+00,\n",
      "          -5.3402e-01,  1.2692e+00,  6.2110e-01,  8.1976e-01, -1.8841e+00,\n",
      "          -2.3763e+00, -3.2109e-02, -1.3611e-01, -7.6145e-02,  8.9103e-01,\n",
      "           6.5652e-01, -9.8815e-01,  5.6435e-01, -6.0324e-01, -1.9997e+00,\n",
      "           1.5768e+00, -1.4878e+00,  1.9180e+00,  9.6125e-01, -6.9369e-01,\n",
      "           6.1794e-01, -7.4613e-01,  1.9749e+00, -6.3729e-01, -2.6626e-01,\n",
      "           2.4032e+00,  3.8748e-01,  7.8673e-01, -7.6715e-01, -5.7858e-01,\n",
      "           3.9599e-01,  4.2218e-01,  8.9233e-01,  1.0090e+00, -1.0966e-01,\n",
      "          -1.7521e-04,  5.2129e-01,  8.6305e-01,  8.8692e-02, -9.9860e-01,\n",
      "          -2.3508e-01, -5.8331e-01, -6.2345e-01, -5.8783e-01,  2.9335e-01,\n",
      "           5.2367e-02, -7.4142e-01, -6.3577e-01,  5.9773e-02,  3.1196e-01,\n",
      "           5.7047e-01, -1.6126e+00, -5.7365e-01,  1.2015e+00,  1.0504e+00,\n",
      "          -8.0045e-01, -2.4582e-02,  7.7081e-01,  1.2386e+00, -2.2893e-01,\n",
      "           1.3746e+00, -6.2525e-01, -9.5797e-02, -1.2482e+00, -2.8906e-02,\n",
      "          -8.2914e-01, -4.5606e-01,  4.9164e-01,  2.4255e-01, -8.2125e-01,\n",
      "          -8.3145e-01,  1.3358e+00,  1.4213e+00, -3.5928e-01, -1.3461e+00,\n",
      "           1.0884e+00, -1.2623e-01, -1.9687e-01, -9.6178e-01,  1.2108e+00,\n",
      "          -8.3075e-03,  2.3496e+00,  2.7372e+00, -5.5096e-01, -2.2081e+00,\n",
      "          -6.9814e-02,  5.2885e-01, -2.0263e+00, -1.8272e-01, -5.8070e-01,\n",
      "           1.0570e+00,  5.2799e-01, -8.3911e-01, -3.3153e-01,  1.3664e+00,\n",
      "          -1.2721e+00, -1.1356e+00,  4.0569e-01,  4.9243e-01, -1.1343e+00,\n",
      "          -2.0365e-01,  5.9447e-01,  1.1166e+00, -6.6291e-01, -5.4991e-01,\n",
      "           1.4363e+00,  3.1334e-01, -4.7462e-02, -2.3511e-01, -4.9545e-01,\n",
      "          -3.5445e-02,  1.5242e+00, -5.8129e-01, -2.9134e-01,  2.5352e-01,\n",
      "           1.1486e+00,  1.0579e-01,  1.3206e+00,  1.1868e+00,  2.9403e+00,\n",
      "           8.8215e-02, -1.6688e-01,  1.5672e+00]]], device='cuda:0'), 'image_feats': tensor([[[ 0.3002,  2.1232,  0.2909,  ..., -0.6346, -0.6917,  0.6597],\n",
      "         [-0.2008,  1.1671,  0.1530,  ...,  0.8046, -0.2188, -0.2269],\n",
      "         [-0.4920,  1.2982,  1.2296,  ..., -0.5249,  0.4874,  0.9032],\n",
      "         ...,\n",
      "         [-0.9126,  1.5898,  0.8639,  ..., -1.4687, -0.3470,  0.3398],\n",
      "         [-0.1696,  1.0916,  1.0759,  ..., -2.0225,  0.3313,  0.5710],\n",
      "         [ 0.0238,  0.9513,  1.4915,  ..., -1.0963,  0.6053,  0.2538]]],\n",
      "       device='cuda:0'), 'cls_feats': tensor([[ 3.1541e-01,  4.7793e-03,  6.6127e-02,  1.1913e-01, -7.4532e-01,\n",
      "         -2.0839e-01,  4.1324e-01, -4.8539e-01,  5.0921e-01,  8.1034e-02,\n",
      "         -4.1078e-01,  6.5245e-02,  3.4463e-01, -7.4885e-01,  8.8269e-01,\n",
      "         -3.8858e-01, -6.5933e-01,  7.4338e-01, -1.1461e-01, -8.3105e-01,\n",
      "          6.5028e-01, -3.7184e-01,  6.6408e-01,  4.9112e-01,  4.0607e-01,\n",
      "          2.6199e-01,  3.2282e-02, -8.5118e-02,  5.2472e-01, -1.8331e-01,\n",
      "         -4.9296e-01,  7.2526e-01, -4.6996e-01, -5.9289e-02,  3.4935e-01,\n",
      "         -2.5706e-01, -9.1757e-01, -2.4561e-01,  6.4767e-01,  3.7980e-01,\n",
      "         -3.5197e-01,  1.7096e-01,  2.0104e-01, -1.5612e-01, -7.2310e-01,\n",
      "          5.8763e-02, -3.6604e-01,  1.9818e-01, -2.4112e-01,  3.4822e-02,\n",
      "         -3.2955e-01,  1.9599e-01,  1.2088e-01,  6.9376e-01, -5.9247e-01,\n",
      "          7.1784e-01,  4.6051e-01,  8.4668e-01,  5.0153e-01, -3.3521e-01,\n",
      "         -3.1847e-01, -1.8781e-01, -5.6096e-01,  4.0854e-01, -2.7414e-01,\n",
      "         -3.2143e-01,  6.3871e-01,  2.2128e-02,  4.5485e-01,  1.2306e-02,\n",
      "         -5.2430e-01, -3.9389e-01, -4.7927e-01, -4.0395e-01,  3.3521e-01,\n",
      "          5.3522e-01,  5.2740e-01, -1.0430e-01,  4.1005e-01, -1.7495e-02,\n",
      "          7.7925e-01,  1.2259e-01,  3.3458e-01, -7.1052e-02,  1.5613e-01,\n",
      "         -5.4360e-01,  3.2063e-02, -2.6202e-01, -9.2001e-02,  8.7160e-01,\n",
      "          3.8981e-01,  2.3106e-01,  3.6198e-01,  1.3965e-01,  7.1315e-01,\n",
      "          2.8101e-01, -4.4324e-01,  8.0621e-01,  5.6212e-01,  5.4455e-01,\n",
      "          7.6715e-01,  1.8954e-01, -8.3684e-02, -5.3501e-01,  8.1197e-03,\n",
      "          5.7039e-01,  4.8273e-01, -1.2313e-01,  5.3492e-01,  6.9658e-02,\n",
      "          5.3677e-01,  7.0426e-01, -2.1182e-01, -2.1651e-01,  9.4213e-02,\n",
      "         -8.1100e-01,  1.9185e-01, -4.2620e-01, -5.7447e-01, -2.6722e-01,\n",
      "         -3.2025e-01,  3.6387e-01,  2.6848e-01,  2.1621e-01, -8.0403e-01,\n",
      "          1.9866e-01,  5.6059e-01,  3.5219e-01, -4.5887e-01, -2.8009e-01,\n",
      "         -4.5762e-01,  4.7259e-01, -3.2120e-01,  3.7327e-01, -2.2505e-01,\n",
      "         -4.1074e-01,  3.0648e-01, -4.5271e-01,  4.3442e-01, -3.3643e-01,\n",
      "         -5.2122e-01, -5.0803e-02, -7.7825e-01,  6.4139e-01, -5.1628e-01,\n",
      "          4.9585e-01, -4.6202e-01, -7.2027e-01,  5.7178e-02,  5.0513e-01,\n",
      "          7.9736e-01, -5.2851e-01, -7.4053e-01,  4.9407e-01,  2.5739e-01,\n",
      "         -5.8195e-01,  3.3555e-01, -8.3030e-01, -4.3576e-01,  2.6485e-01,\n",
      "         -2.9713e-01, -6.2740e-01, -4.3428e-01,  1.0666e-01, -2.6081e-01,\n",
      "         -3.0736e-01, -3.3271e-01,  2.9029e-01,  6.3953e-01, -5.3415e-01,\n",
      "          4.6238e-01, -4.4261e-01,  1.4011e-01,  3.2893e-02,  7.4119e-01,\n",
      "          2.4294e-01,  6.7014e-01,  2.5168e-01, -9.5105e-01,  5.0141e-01,\n",
      "         -3.8318e-02, -2.7020e-01,  5.5333e-01,  3.2620e-01,  4.1340e-01,\n",
      "         -4.9446e-01,  8.6798e-01,  2.8650e-01,  5.9927e-01,  6.7093e-01,\n",
      "          4.6406e-01, -1.3790e-01,  4.0577e-01,  1.2591e-02,  3.1496e-01,\n",
      "          6.8130e-01,  3.2446e-01,  8.4498e-01,  5.8261e-01,  1.1745e-01,\n",
      "          1.3635e-01, -8.7119e-01,  1.2259e-01,  7.4959e-01, -1.5952e-01,\n",
      "         -2.2006e-01,  1.2470e-02,  9.3139e-02,  7.2998e-01,  6.8545e-01,\n",
      "          1.5703e-01,  5.6646e-01,  4.9519e-02, -3.6969e-02,  2.4570e-01,\n",
      "         -5.2429e-01, -8.8971e-01, -4.7317e-02,  4.0215e-01,  8.2849e-01,\n",
      "          9.2021e-02, -1.5697e-01,  3.4595e-01, -3.5168e-01,  3.8445e-01,\n",
      "          2.4234e-01, -1.6666e-02,  8.4805e-02, -1.9190e-01, -6.0087e-01,\n",
      "         -8.4180e-01,  2.7106e-01, -3.2489e-01, -4.1329e-01, -1.1389e-01,\n",
      "          2.5785e-01,  4.3714e-01, -4.3532e-01, -5.6142e-01, -2.0571e-01,\n",
      "          7.2961e-01,  1.4415e-01, -4.8876e-01,  8.0755e-01,  5.0331e-01,\n",
      "          3.8500e-02,  2.7586e-01, -4.7591e-01, -6.4807e-01, -1.2571e-01,\n",
      "         -7.3449e-01, -1.7775e-01,  4.4062e-01, -2.9409e-01, -1.9216e-01,\n",
      "         -4.3316e-01, -2.2027e-01,  6.1721e-01, -5.4354e-01, -2.4963e-01,\n",
      "         -4.8410e-01, -2.5412e-01, -4.8374e-01, -7.1149e-01,  3.0053e-01,\n",
      "         -6.7575e-01,  5.5114e-02,  9.3341e-01,  3.5317e-01, -1.8438e-01,\n",
      "          3.6155e-01,  8.2057e-01,  1.0617e-01,  6.3657e-01,  7.9142e-02,\n",
      "         -2.0119e-01,  4.4792e-01,  2.9025e-01, -7.6475e-01, -3.8601e-01,\n",
      "          4.2877e-02, -4.3928e-01, -7.3938e-01,  2.4365e-01,  1.8506e-01,\n",
      "         -5.9752e-01, -3.9791e-01, -2.5453e-01, -3.3239e-01,  6.3850e-01,\n",
      "          1.5049e-01,  1.5545e-01,  3.8806e-01,  4.5607e-01,  2.0213e-01,\n",
      "          1.6726e-01,  4.7684e-01,  1.2406e-02,  3.3179e-01,  5.6200e-01,\n",
      "         -1.0789e-01, -1.9705e-01, -5.3548e-01,  4.3323e-01, -6.4670e-01,\n",
      "         -1.7655e-02,  7.1280e-01,  5.0465e-01,  2.2937e-01,  3.2745e-02,\n",
      "          8.2635e-01, -8.3159e-02, -2.6116e-01, -3.8709e-01,  3.8429e-01,\n",
      "          2.0236e-02,  6.8339e-01, -7.5291e-01, -6.1762e-01, -2.4344e-01,\n",
      "         -1.7999e-01, -6.1144e-01,  5.7776e-01,  3.1751e-01, -2.9171e-01,\n",
      "          7.8755e-02,  1.6952e-01, -7.6342e-01,  6.2580e-01,  5.5686e-02,\n",
      "          3.5322e-01,  4.7388e-01, -6.8295e-01, -3.4666e-01, -4.4063e-01,\n",
      "         -3.1253e-01,  4.3789e-01, -7.4076e-01,  1.3593e-02,  6.1977e-01,\n",
      "         -1.9506e-02, -5.2833e-01, -5.7929e-01,  1.3701e-01, -9.1771e-02,\n",
      "          7.5212e-01, -1.9993e-01,  6.6055e-01, -1.7643e-01, -5.3268e-01,\n",
      "         -3.5199e-01,  7.7698e-01, -5.3685e-02,  2.0082e-01, -1.6840e-01,\n",
      "         -6.8474e-01, -1.8815e-01, -7.4153e-01, -2.0299e-01,  1.0220e-01,\n",
      "          3.3932e-01, -2.9535e-01, -7.2119e-01,  4.6477e-01, -6.2790e-01,\n",
      "          4.1532e-01,  4.2970e-01, -2.8939e-01, -4.7679e-01, -2.6589e-01,\n",
      "          1.4645e-01,  5.5262e-02,  7.7289e-02, -4.4748e-01, -6.9033e-01,\n",
      "          5.1056e-01, -1.4049e-01,  4.6752e-01, -2.4557e-01, -1.4950e-01,\n",
      "         -6.8493e-01,  2.7487e-03,  7.8414e-01,  3.3380e-01, -1.1538e-01,\n",
      "          4.7734e-01, -5.7335e-01,  2.0059e-01, -6.4488e-01, -5.9957e-01,\n",
      "          6.2299e-01, -8.7583e-02, -4.1344e-01, -5.4033e-01, -5.5663e-02,\n",
      "         -1.3770e-01, -1.1558e-01, -1.0104e-01,  5.4423e-01, -1.8616e-01,\n",
      "          4.3651e-01,  2.1349e-01,  2.6806e-01,  4.7591e-01,  6.3424e-01,\n",
      "         -5.5343e-01,  3.5586e-01,  6.2143e-01, -1.1734e-01,  1.4195e-01,\n",
      "         -1.4938e-01, -6.3064e-02,  2.7105e-01, -4.4239e-03, -2.0002e-01,\n",
      "         -8.9646e-02, -2.2528e-01, -5.2518e-01, -1.3090e-01,  7.1227e-02,\n",
      "          2.3443e-01, -4.2063e-01, -1.0642e-02,  8.6443e-02,  7.3410e-01,\n",
      "          1.4842e-02, -5.2330e-01, -5.4118e-01, -2.9793e-01,  2.4763e-01,\n",
      "         -2.7453e-03,  6.1103e-01, -2.3617e-01, -7.8844e-01, -6.2919e-02,\n",
      "          3.6729e-01, -5.5876e-02, -2.2260e-01, -1.8229e-01,  2.1220e-01,\n",
      "          4.1813e-01, -6.1974e-01,  9.9382e-02, -2.5196e-01,  3.9110e-01,\n",
      "         -2.3448e-01, -5.9202e-01,  6.1784e-01, -7.7181e-01, -5.6684e-01,\n",
      "          3.0239e-01,  8.4692e-01, -4.8456e-01,  4.4046e-01,  1.4182e-01,\n",
      "         -2.4576e-01,  6.2470e-01, -3.8027e-02,  1.7387e-01, -5.0700e-01,\n",
      "         -8.1804e-01, -4.2615e-01, -1.4988e-01,  3.1896e-01, -3.1758e-01,\n",
      "          3.9622e-01,  2.1735e-01,  2.9944e-01, -4.0515e-01,  1.1155e-01,\n",
      "          1.9236e-01,  2.3309e-02, -1.7077e-01, -4.2784e-01, -3.5411e-02,\n",
      "         -1.0179e-01, -3.3210e-01,  6.6313e-02,  2.1857e-01,  7.8658e-01,\n",
      "          1.0216e-01,  2.8737e-01,  9.7467e-02,  7.7112e-01, -7.1064e-01,\n",
      "          2.0539e-01, -5.5897e-01,  6.3947e-01, -1.6319e-01,  6.3881e-01,\n",
      "          3.8599e-01,  5.1443e-01, -4.2996e-01, -4.2272e-01, -4.2374e-01,\n",
      "          4.0887e-01, -8.7376e-01, -8.2520e-02, -5.7938e-01, -7.9168e-02,\n",
      "         -3.9378e-02, -4.0274e-03, -2.4985e-01, -4.0850e-02,  7.0724e-01,\n",
      "          7.6778e-01, -1.6651e-01, -1.5683e-01, -1.3408e-01,  5.4501e-01,\n",
      "          4.7362e-01, -1.5946e-01, -4.8963e-01, -1.6482e-01, -1.6363e-01,\n",
      "          4.6294e-01,  4.3454e-01, -6.4054e-01, -6.6919e-01,  4.6957e-01,\n",
      "         -2.2442e-02, -2.3919e-01, -4.1749e-02, -7.8864e-01,  9.4495e-02,\n",
      "          4.0318e-01,  4.0288e-01,  2.9345e-01,  4.0641e-01,  2.0039e-01,\n",
      "          2.9924e-01,  1.0264e-01,  8.2564e-02,  4.6762e-01,  2.4426e-01,\n",
      "          1.8251e-02, -2.1673e-01, -3.7469e-01, -2.5460e-02,  2.8161e-01,\n",
      "         -4.1863e-01,  1.3681e-01,  1.2552e-01,  8.7132e-03, -3.4451e-01,\n",
      "         -7.1140e-01, -6.7831e-02, -1.8845e-01, -3.2011e-01, -1.4217e-01,\n",
      "         -2.8924e-01, -2.3618e-01, -2.2000e-01, -5.2184e-01,  6.0584e-01,\n",
      "          3.1197e-01, -3.0305e-01,  3.6401e-01, -6.4646e-01,  6.9926e-01,\n",
      "          6.0133e-01, -3.9746e-02,  6.7693e-01,  4.5036e-01,  6.0794e-01,\n",
      "         -2.4627e-01,  4.5034e-01,  7.4546e-01,  4.7821e-01, -5.2903e-01,\n",
      "          1.8352e-01, -2.6535e-01,  3.2123e-01,  8.3942e-01,  1.1103e-01,\n",
      "          1.2812e-01,  2.1977e-02, -3.7716e-01,  9.0793e-01, -2.7534e-01,\n",
      "         -6.4886e-01, -7.5839e-01, -4.7851e-01, -3.0572e-01, -3.8641e-01,\n",
      "          6.6982e-01,  5.9817e-01,  5.6842e-01,  3.1115e-01, -4.0794e-01,\n",
      "         -7.1037e-02, -1.7321e-01, -1.3927e-01, -7.3615e-01,  2.0445e-01,\n",
      "         -2.4379e-01, -2.2110e-01,  7.8856e-01,  7.6340e-01,  3.6149e-01,\n",
      "          8.3788e-01, -5.9779e-02,  1.9219e-01,  8.9480e-01,  8.2806e-01,\n",
      "         -2.2347e-01, -6.0459e-01,  4.5883e-01,  4.2689e-01, -1.0120e-01,\n",
      "         -1.1001e-01,  5.8060e-01,  4.0470e-01,  8.4432e-02,  3.8540e-02,\n",
      "          3.4842e-01,  4.0483e-01,  7.1910e-01,  8.3570e-01,  3.1597e-01,\n",
      "         -8.5420e-01, -3.1603e-01,  6.3026e-01,  6.9663e-01, -1.9663e-01,\n",
      "         -7.2962e-01,  7.8946e-01,  5.4569e-01,  3.9149e-01, -8.9092e-01,\n",
      "         -7.0411e-01, -2.3397e-01,  7.7714e-01, -4.6983e-01,  1.1648e-01,\n",
      "          4.8814e-01,  9.2180e-01,  1.9078e-02, -4.4532e-02,  1.5125e-01,\n",
      "         -5.3183e-02,  8.9377e-01, -4.2096e-01,  2.3558e-02,  6.7067e-01,\n",
      "         -7.1311e-01,  1.9988e-01,  1.0003e-01,  9.6629e-01,  5.3949e-01,\n",
      "         -4.0463e-02, -4.0374e-02, -1.6626e-01,  4.3744e-01, -7.7108e-01,\n",
      "         -7.2736e-01,  4.1154e-01,  3.0644e-01,  1.8901e-01,  1.8799e-01,\n",
      "         -6.1128e-01, -5.0877e-01,  9.4167e-02,  5.4171e-01, -1.2973e-01,\n",
      "          2.6264e-01,  4.6057e-01, -4.9904e-01, -3.0018e-01,  1.9462e-01,\n",
      "          6.3953e-01, -7.0760e-01,  1.6691e-01, -8.7276e-02, -1.1122e-01,\n",
      "         -1.2844e-01,  4.8886e-01,  5.8666e-02, -6.9107e-01, -3.2738e-01,\n",
      "         -4.3623e-01,  6.6915e-01, -3.6006e-01,  7.0885e-01,  1.9124e-01,\n",
      "         -9.3159e-02, -2.2782e-01,  7.7852e-02,  2.0480e-01, -2.0893e-01,\n",
      "         -5.0177e-01, -3.9284e-01,  4.7029e-01, -8.5298e-01, -7.4328e-01,\n",
      "          6.3102e-01,  1.5373e-01,  9.3779e-02, -1.7615e-01,  3.5321e-01,\n",
      "         -2.4828e-01, -1.1041e-01, -5.7014e-01, -6.2045e-03,  3.5195e-01,\n",
      "          4.4160e-01, -5.4301e-01, -5.2769e-02,  3.1363e-02, -2.1486e-01,\n",
      "          1.3348e-01,  3.8687e-01, -1.3673e-02, -8.9089e-01, -7.4312e-01,\n",
      "          6.0570e-02,  2.2562e-01,  2.0292e-01, -4.2863e-02, -7.0303e-01,\n",
      "         -3.8423e-01,  5.5769e-01,  2.9030e-01, -5.4737e-01, -3.1823e-01,\n",
      "          1.8871e-01,  8.1620e-01, -1.9189e-01,  9.0241e-02,  7.8814e-01,\n",
      "         -1.8345e-02, -4.0470e-01, -7.6646e-01, -1.5998e-02, -3.5600e-01,\n",
      "         -2.1030e-01, -4.0545e-02, -4.7139e-01, -2.8551e-01,  5.8364e-01,\n",
      "         -1.4654e-01, -6.5156e-01, -5.4380e-01,  5.7821e-01,  5.5482e-01,\n",
      "         -6.7597e-01, -7.9120e-02,  4.0992e-02, -3.6560e-01,  6.9683e-01,\n",
      "         -4.4814e-04, -4.3676e-01,  2.7642e-01,  2.4094e-01,  7.6871e-02,\n",
      "         -4.3478e-01, -8.4344e-01,  2.6368e-01, -7.8341e-01, -1.9024e-01,\n",
      "          7.0675e-02,  9.5790e-02, -5.4749e-01, -4.5364e-01, -8.2974e-01,\n",
      "         -8.5352e-01, -3.6503e-01,  2.5954e-01]], device='cuda:0'), 'raw_cls_feats': tensor([[ 1.8914e-01, -2.1735e-01,  8.5255e-01,  7.8298e-01, -4.3437e-01,\n",
      "         -9.9152e-01,  1.1405e+00,  1.2102e+00, -3.5829e-03,  1.8816e-01,\n",
      "         -1.7830e+00,  1.3710e+00, -6.4979e-01,  1.2611e+00,  8.5380e-01,\n",
      "         -5.1768e-01, -9.0013e-01, -1.4700e+00, -1.1877e+00,  1.1053e+00,\n",
      "         -1.5470e-01, -4.5214e-01, -6.6290e-01, -8.2520e-01, -5.8180e-01,\n",
      "         -1.2151e+00, -1.8665e+00,  1.1014e+00,  1.5840e-01,  7.0513e-01,\n",
      "          7.5815e-01, -1.1844e+00, -1.2740e+00, -1.9909e-01,  7.3141e-01,\n",
      "          5.3559e-01, -1.2204e-01, -7.3502e-01, -5.4481e-01,  3.3622e-01,\n",
      "          1.3244e+00,  1.0313e+00, -1.3161e+00, -9.8514e-01,  5.0247e-01,\n",
      "         -2.6440e+00,  1.3343e+00,  2.8385e-01, -6.6683e-01, -2.1295e-01,\n",
      "          1.1146e-01,  2.5773e-01, -7.6813e-01,  1.3432e-02, -7.8291e-02,\n",
      "         -6.4118e-01, -3.0406e-01, -2.7933e-01, -4.9455e-01, -7.9737e-01,\n",
      "          4.8862e-01, -1.6017e+00,  4.5616e-02,  1.2852e-01, -3.5534e-01,\n",
      "          5.0064e-01,  1.2754e+00, -1.2589e+00,  6.2993e-01, -7.5133e-01,\n",
      "          5.2190e-01,  5.7029e-01, -6.4824e-01, -4.6513e-01,  1.7397e-01,\n",
      "         -5.8831e-01,  1.1852e+00, -4.5569e-01,  1.0603e+00, -1.8375e-02,\n",
      "         -4.9949e-01, -1.2847e+00,  1.6924e+00, -1.5352e+00,  2.4939e-01,\n",
      "          6.7886e-01,  1.8251e-02, -8.6937e-03, -1.1066e+00, -1.2033e+00,\n",
      "         -5.9378e-01,  1.5640e-01, -1.0685e+00,  5.7379e-01,  9.7807e-01,\n",
      "         -1.3645e+00,  5.3530e-01,  1.7626e-01,  4.7728e-01, -1.4157e+00,\n",
      "         -1.0075e+00, -5.3611e-01,  1.9340e+00, -1.7657e+00, -1.8910e-01,\n",
      "          8.2328e-01,  8.4448e-01, -2.2276e+00,  9.0489e-01, -6.4669e-01,\n",
      "          5.3152e-01,  1.5801e+00, -6.0394e-01, -9.8520e-02,  5.3477e-01,\n",
      "         -1.7394e-01,  6.6862e-01,  1.1678e+00, -1.1180e-01,  7.5388e-01,\n",
      "         -1.1493e+00, -9.1123e-01, -4.4047e-01,  1.3192e+00,  4.1134e-01,\n",
      "         -1.4427e+00, -2.1898e-02,  3.0476e-01, -8.0926e-01,  5.2712e-01,\n",
      "         -2.4103e-01, -4.5474e-01,  4.4463e-01, -9.2391e-01,  1.3500e+00,\n",
      "          1.3104e+00, -2.1827e-01, -6.7447e-01,  1.0268e+00, -1.6455e+00,\n",
      "          7.3753e-01, -1.0066e+00, -2.6431e-01, -3.9765e-01,  9.1322e-02,\n",
      "         -2.1819e-02,  1.4516e+00,  9.0437e-01, -1.9092e+00,  1.9088e+00,\n",
      "         -5.9874e-01,  3.8362e-02,  1.1966e-01, -8.5761e-01,  2.4668e+00,\n",
      "         -1.0748e+00,  7.0560e-01, -4.9809e-01, -1.6450e+00,  6.6465e-01,\n",
      "         -2.3304e-01, -1.7917e-01,  8.8541e-01, -1.2430e+00, -1.7078e+00,\n",
      "         -9.9595e-02,  7.1037e-01,  5.0880e-01, -3.6718e-01, -1.4640e+00,\n",
      "          2.4973e-01, -9.5384e-01,  4.8832e-02, -1.3636e+00, -3.0455e-01,\n",
      "         -1.7374e+00,  2.2744e+00, -1.4202e+00, -6.0189e-01, -2.7875e-01,\n",
      "         -1.0942e+00, -1.1437e+00,  2.4685e+00, -1.0017e+00,  1.2032e+00,\n",
      "          1.1385e-01, -1.3064e+00, -9.0674e-02,  5.9708e-01,  1.3152e+00,\n",
      "         -1.5614e+00, -2.9447e-01, -2.0251e-01, -1.1082e+00,  4.4503e-01,\n",
      "         -7.7376e-01, -2.8024e-01, -6.6477e-01, -1.2452e+00,  6.0086e-01,\n",
      "         -4.3244e-01,  1.1261e+00, -1.0565e+00,  2.0493e-01,  1.4919e-01,\n",
      "          9.7864e-01, -2.8177e+00,  1.4764e+00,  1.1833e+00,  1.9852e+00,\n",
      "         -5.2643e-01, -2.2712e-01, -1.4056e-01, -1.6355e+00,  7.0809e-02,\n",
      "          2.5168e-01, -5.8031e-01,  1.0403e+00,  9.7952e-01,  6.0929e-01,\n",
      "         -1.0119e+00,  4.5209e-01,  1.1244e+00, -2.4012e-01,  2.9348e-01,\n",
      "         -1.7701e-01, -1.7219e-01,  1.0666e-01, -1.8008e-01,  1.3283e+00,\n",
      "         -6.1238e-01,  2.6802e-01, -6.2041e-01, -2.5920e+00,  3.8670e-01,\n",
      "          8.0854e-01, -6.8211e-02, -6.8373e-02, -5.8167e-01, -3.0487e+00,\n",
      "          6.4038e-01,  1.5422e+00,  1.3273e+00, -7.0839e-01,  4.9784e-01,\n",
      "          4.7091e-01, -1.6697e+00, -1.7389e+00, -9.5921e-02,  9.2121e-01,\n",
      "          4.2697e-01, -1.2327e+00,  1.8867e-02, -1.4314e-01, -1.5740e+00,\n",
      "         -5.3723e-01, -1.5045e+00,  1.7231e+00,  1.0551e+00, -5.8574e-01,\n",
      "         -2.7458e+00,  9.4770e-01, -7.0174e-02,  5.8300e-01,  2.9411e-01,\n",
      "         -3.0651e-02, -9.8715e-01, -4.9715e-01, -4.9194e-01,  4.3904e-01,\n",
      "         -5.6256e-01, -1.8311e+00, -9.8708e-01, -1.0895e+00,  3.8159e-01,\n",
      "         -1.2679e+00, -8.5537e-01, -2.9960e-02,  2.5119e+00, -3.3735e-01,\n",
      "         -2.0201e+00, -3.7752e-01,  1.0377e-01, -4.5703e-01, -1.5010e+00,\n",
      "          1.0118e+00,  1.7851e+00, -1.0936e-01,  8.0994e-01, -1.1943e+00,\n",
      "          1.9385e+00, -1.4950e-01, -2.9377e-01, -1.4506e+00, -6.6784e-01,\n",
      "         -2.0624e+00,  1.1300e+00,  1.2249e+00, -2.5745e-01, -1.3580e-01,\n",
      "         -1.3019e+00, -5.6963e-01, -3.7000e-01, -1.9712e-01, -6.8866e-01,\n",
      "         -9.7986e-01, -6.3387e-01,  1.2959e+00,  2.4582e-01,  8.2192e-01,\n",
      "          9.2341e-01, -8.5542e-02,  8.9503e-01, -1.0474e+00,  2.7299e+00,\n",
      "          1.1049e+00,  1.1082e+00,  8.5202e-01,  1.0676e-01, -1.6310e-01,\n",
      "         -7.9308e-01, -2.8111e+00, -1.6286e-02, -5.9292e-01, -1.4141e+00,\n",
      "         -7.4058e-01,  1.5050e+00,  3.0351e-01,  7.0391e-01,  5.6099e-01,\n",
      "          1.2788e+00, -1.0151e+00, -3.8940e-01,  1.2114e+00,  1.7235e-01,\n",
      "         -1.3934e+00,  1.4214e-01, -1.5539e+00, -4.7705e-01, -2.0107e-01,\n",
      "          5.1737e-01,  4.5248e-01, -3.1565e-01,  1.7143e+00,  1.0975e+00,\n",
      "          1.6400e+00, -5.0238e-01,  1.0490e+00,  8.1858e-01,  1.1583e+00,\n",
      "          9.3227e-02,  8.1205e-02,  1.5435e+00,  9.1901e-01,  1.0855e+00,\n",
      "         -5.7090e-01,  5.8131e-01,  4.0467e-01, -6.7619e-01,  1.7438e+00,\n",
      "         -8.3540e-01,  1.3628e+00,  2.8027e-01,  5.8497e-01,  2.2489e+00,\n",
      "         -1.1250e+00, -1.1383e+00,  9.3503e-01,  6.5489e-01,  7.4130e-01,\n",
      "         -1.2203e+00,  8.5096e-01, -6.4574e-01,  6.3889e-01, -9.5148e-01,\n",
      "         -4.2341e-01,  1.3222e+00,  3.4383e-01, -8.1987e-01, -1.8029e-01,\n",
      "         -1.2967e+00, -2.0508e+00, -8.4069e-01, -1.4096e+00, -6.6758e-01,\n",
      "          6.4392e-01,  6.3644e-01, -9.2489e-01,  1.0850e+00,  6.7400e-02,\n",
      "         -2.8510e-01,  5.5134e-01,  1.4983e+00, -1.4135e+00,  1.7541e+00,\n",
      "          4.4188e-01,  1.8207e+00, -5.9658e-01,  1.5468e-01, -2.3706e-01,\n",
      "         -3.3558e-01, -1.7438e+00, -8.2266e-01, -1.4848e+00,  5.1557e-01,\n",
      "          9.3779e-01,  1.2867e+00, -8.5333e-01,  5.4711e-01,  9.6911e-01,\n",
      "          2.0865e+00,  1.8779e+00,  1.4166e+00,  1.7335e-01, -9.5915e-02,\n",
      "          9.3557e-01, -8.3773e-02, -9.6599e-02, -6.4023e-01, -4.8290e-01,\n",
      "         -1.6831e-01,  8.0311e-01, -1.0912e+00, -7.1184e-01, -9.9941e-01,\n",
      "          2.4587e-01,  2.5156e+00, -8.3495e-01,  1.7179e-01,  2.1467e-01,\n",
      "          2.9124e-01, -6.1840e-01, -2.7166e+00,  9.6906e-02, -3.5402e-01,\n",
      "          7.7622e-01, -8.7859e-01,  1.4919e+00, -1.1382e+00, -5.3122e-01,\n",
      "         -6.6127e-01, -9.7543e-01,  3.2159e-01, -3.0993e-01, -6.4086e-01,\n",
      "         -5.4569e-01, -2.2172e+00,  1.2511e+00,  2.6732e-01, -1.8016e+00,\n",
      "         -2.7174e-02, -4.7302e-01,  3.9626e-01,  1.1247e+00,  1.5914e-01,\n",
      "          8.4659e-01, -1.0259e+00, -1.1902e+00, -5.3448e-03, -9.1189e-01,\n",
      "         -6.1509e-01, -8.9558e-01,  1.5736e+00, -6.4655e-01, -1.0306e+00,\n",
      "         -6.0610e-01,  2.4788e+00, -7.1951e-02,  3.7908e-01,  3.2637e-01,\n",
      "          1.7535e+00,  4.7062e-01, -2.8230e-01,  1.3392e+00,  1.1419e+00,\n",
      "         -1.5650e-01,  9.8326e-01,  2.5390e-01,  7.3165e-01,  1.3475e+00,\n",
      "         -6.1201e-01, -9.7115e-01,  8.2015e-01,  2.8834e-01,  7.2748e-02,\n",
      "          8.2096e-02,  1.0522e-01, -1.2469e+00, -4.0489e-01,  1.6870e+00,\n",
      "          2.0807e-01,  3.2647e-01,  8.6556e-01,  5.2858e-01,  1.5795e+00,\n",
      "          1.5141e+00,  5.9084e-01,  8.4838e-01,  3.5207e-01,  1.3715e+00,\n",
      "          4.3704e-01, -3.9102e-01, -3.9459e-01,  1.7541e-01, -2.0083e-01,\n",
      "          9.8534e-01, -9.9782e-01, -1.9880e+00,  5.5017e-01,  6.8539e-01,\n",
      "          9.5570e-01,  1.9598e+00,  4.6553e-01, -7.4715e-01, -2.5830e-01,\n",
      "         -4.5271e-02, -9.9500e-01,  7.0199e-02, -1.2143e+00, -1.7874e-01,\n",
      "         -1.4952e+00,  1.3012e+00,  9.5228e-01,  4.6716e-02, -8.4319e-01,\n",
      "         -8.4183e-01,  6.8555e-01,  3.4453e-01,  1.1041e+00,  1.5358e+00,\n",
      "         -1.2697e+00, -2.6857e-01,  7.8946e-01, -6.4109e-01, -2.9078e-01,\n",
      "          6.2779e-02,  8.9780e-01,  4.7209e-01, -7.8663e-01, -1.3347e+00,\n",
      "          1.3487e+00, -1.8979e+00,  4.0163e-01, -3.8484e-01,  1.8912e+00,\n",
      "         -1.1453e+00, -5.0207e-01,  2.4453e-02, -1.4218e+00, -2.4351e-02,\n",
      "         -4.3177e-01,  2.1467e-01, -5.9991e-02, -7.9255e-01, -4.2164e-01,\n",
      "         -5.6286e-01, -1.9324e-01, -1.4438e+00,  2.9775e+00,  5.4152e-01,\n",
      "         -2.0633e-01, -1.0600e-01,  5.9384e-01, -3.9282e-01, -9.3391e-01,\n",
      "         -1.6650e+00,  1.0015e+00,  2.9172e-01,  1.4159e+00,  6.0496e-01,\n",
      "         -6.8626e-02,  4.0170e-01,  7.0261e-01, -5.3553e-04,  3.4430e-01,\n",
      "          9.7466e-01, -7.6821e-01, -5.4593e-01,  1.7289e-01, -7.2383e-01,\n",
      "          1.3517e-01, -1.1063e+00, -1.9684e-01,  2.6796e-01, -9.1107e-01,\n",
      "          1.3376e+00, -1.3310e+00, -2.2052e+00,  9.4421e-01,  7.0152e-01,\n",
      "         -3.8085e-01,  6.3128e-01, -1.0958e-01,  5.5696e-01, -1.3194e+00,\n",
      "          1.2821e+00,  3.1376e-01, -1.5934e+00, -1.6551e+00, -3.7859e-01,\n",
      "         -1.2653e+00, -2.9073e-01,  9.7406e-01,  8.4697e-01,  4.6142e-01,\n",
      "         -7.0729e-01,  6.9224e-01,  1.1812e+00,  3.4167e-01,  4.0578e-01,\n",
      "          2.2782e-01, -2.1966e+00,  2.9547e-01, -2.0527e-01, -2.4542e-01,\n",
      "          1.9924e-02, -4.3126e-01, -1.6301e+00, -7.9188e-01, -6.8111e-01,\n",
      "         -6.4806e-01,  8.7972e-01,  2.4612e-01,  8.8949e-01,  5.5849e-01,\n",
      "          1.2161e+00, -5.2092e-01,  5.3265e-01,  8.6378e-01,  8.4785e-01,\n",
      "         -8.4069e-01,  2.0698e+00,  1.1632e+00,  2.3445e-01,  1.4491e+00,\n",
      "          3.8487e-02, -7.8888e-01, -8.8485e-01,  1.0045e+00,  4.9586e-01,\n",
      "         -1.0455e+00, -1.1998e-01, -4.8991e-02,  4.9541e-01,  1.3558e+00,\n",
      "         -5.3402e-01,  1.2692e+00,  6.2110e-01,  8.1976e-01, -1.8841e+00,\n",
      "         -2.3763e+00, -3.2109e-02, -1.3611e-01, -7.6145e-02,  8.9103e-01,\n",
      "          6.5652e-01, -9.8815e-01,  5.6435e-01, -6.0324e-01, -1.9997e+00,\n",
      "          1.5768e+00, -1.4878e+00,  1.9180e+00,  9.6125e-01, -6.9369e-01,\n",
      "          6.1794e-01, -7.4613e-01,  1.9749e+00, -6.3729e-01, -2.6626e-01,\n",
      "          2.4032e+00,  3.8748e-01,  7.8673e-01, -7.6715e-01, -5.7858e-01,\n",
      "          3.9599e-01,  4.2218e-01,  8.9233e-01,  1.0090e+00, -1.0966e-01,\n",
      "         -1.7521e-04,  5.2129e-01,  8.6305e-01,  8.8692e-02, -9.9860e-01,\n",
      "         -2.3508e-01, -5.8331e-01, -6.2345e-01, -5.8783e-01,  2.9335e-01,\n",
      "          5.2367e-02, -7.4142e-01, -6.3577e-01,  5.9773e-02,  3.1196e-01,\n",
      "          5.7047e-01, -1.6126e+00, -5.7365e-01,  1.2015e+00,  1.0504e+00,\n",
      "         -8.0045e-01, -2.4582e-02,  7.7081e-01,  1.2386e+00, -2.2893e-01,\n",
      "          1.3746e+00, -6.2525e-01, -9.5797e-02, -1.2482e+00, -2.8906e-02,\n",
      "         -8.2914e-01, -4.5606e-01,  4.9164e-01,  2.4255e-01, -8.2125e-01,\n",
      "         -8.3145e-01,  1.3358e+00,  1.4213e+00, -3.5928e-01, -1.3461e+00,\n",
      "          1.0884e+00, -1.2623e-01, -1.9687e-01, -9.6178e-01,  1.2108e+00,\n",
      "         -8.3075e-03,  2.3496e+00,  2.7372e+00, -5.5096e-01, -2.2081e+00,\n",
      "         -6.9814e-02,  5.2885e-01, -2.0263e+00, -1.8272e-01, -5.8070e-01,\n",
      "          1.0570e+00,  5.2799e-01, -8.3911e-01, -3.3153e-01,  1.3664e+00,\n",
      "         -1.2721e+00, -1.1356e+00,  4.0569e-01,  4.9243e-01, -1.1343e+00,\n",
      "         -2.0365e-01,  5.9447e-01,  1.1166e+00, -6.6291e-01, -5.4991e-01,\n",
      "          1.4363e+00,  3.1334e-01, -4.7462e-02, -2.3511e-01, -4.9545e-01,\n",
      "         -3.5445e-02,  1.5242e+00, -5.8129e-01, -2.9134e-01,  2.5352e-01,\n",
      "          1.1486e+00,  1.0579e-01,  1.3206e+00,  1.1868e+00,  2.9403e+00,\n",
      "          8.8215e-02, -1.6688e-01,  1.5672e+00]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0'), 'patch_index': (tensor([[[ 2, 14],\n",
      "         [10,  5],\n",
      "         [ 8,  0],\n",
      "         [ 1,  9],\n",
      "         [ 0, 13],\n",
      "         [ 6, 11],\n",
      "         [ 5,  6],\n",
      "         [ 2,  8],\n",
      "         [ 1,  3],\n",
      "         [ 1,  0],\n",
      "         [10, 11],\n",
      "         [ 7,  5],\n",
      "         [ 3,  0],\n",
      "         [ 9,  2],\n",
      "         [ 1, 14],\n",
      "         [ 2,  9],\n",
      "         [ 8,  2],\n",
      "         [ 6,  2],\n",
      "         [ 9,  6],\n",
      "         [ 7,  4],\n",
      "         [10,  3],\n",
      "         [ 3,  6],\n",
      "         [ 6, 13],\n",
      "         [ 3, 17],\n",
      "         [ 1, 12],\n",
      "         [ 1, 10],\n",
      "         [10,  0],\n",
      "         [ 9, 14],\n",
      "         [ 3,  9],\n",
      "         [10, 15],\n",
      "         [ 2,  4],\n",
      "         [ 5, 16],\n",
      "         [ 7,  8],\n",
      "         [ 6, 15],\n",
      "         [ 7,  6],\n",
      "         [ 5, 12],\n",
      "         [10,  2],\n",
      "         [10, 13],\n",
      "         [ 3, 11],\n",
      "         [10,  1],\n",
      "         [ 5,  9],\n",
      "         [10,  7],\n",
      "         [ 9,  0],\n",
      "         [ 2, 13],\n",
      "         [ 0,  2],\n",
      "         [ 1,  8],\n",
      "         [ 8,  8],\n",
      "         [ 7, 17],\n",
      "         [ 8, 11],\n",
      "         [ 0, 16],\n",
      "         [10, 14],\n",
      "         [ 4, 17],\n",
      "         [ 3, 12],\n",
      "         [ 7, 11],\n",
      "         [ 4,  2],\n",
      "         [11, 10],\n",
      "         [ 1,  6],\n",
      "         [ 5, 10],\n",
      "         [ 1,  4],\n",
      "         [ 9, 17],\n",
      "         [ 8, 15],\n",
      "         [ 2, 16],\n",
      "         [ 9,  3],\n",
      "         [10, 16],\n",
      "         [11,  3],\n",
      "         [ 5, 11],\n",
      "         [ 5,  4],\n",
      "         [ 3, 10],\n",
      "         [ 8,  7],\n",
      "         [ 8,  9],\n",
      "         [ 0,  3],\n",
      "         [ 6, 10],\n",
      "         [ 5,  5],\n",
      "         [ 5, 17],\n",
      "         [ 9,  8],\n",
      "         [ 9, 11],\n",
      "         [ 5,  3],\n",
      "         [ 2, 11],\n",
      "         [11,  8],\n",
      "         [ 9,  1],\n",
      "         [ 3,  5],\n",
      "         [ 5, 15],\n",
      "         [ 7,  9],\n",
      "         [ 9,  4],\n",
      "         [11,  9],\n",
      "         [ 6,  7],\n",
      "         [ 0,  7],\n",
      "         [ 9, 10],\n",
      "         [ 3,  7],\n",
      "         [ 9, 12],\n",
      "         [ 9,  7],\n",
      "         [10,  4],\n",
      "         [ 2, 12],\n",
      "         [ 6,  6],\n",
      "         [11,  2],\n",
      "         [ 7, 10],\n",
      "         [ 6, 12],\n",
      "         [ 2,  5],\n",
      "         [ 9, 16],\n",
      "         [ 2,  0],\n",
      "         [ 8,  4],\n",
      "         [ 4, 11],\n",
      "         [ 7,  2],\n",
      "         [ 1, 16],\n",
      "         [ 6,  8],\n",
      "         [ 4,  4],\n",
      "         [ 0, 12],\n",
      "         [10, 12],\n",
      "         [ 1,  7],\n",
      "         [ 3, 16],\n",
      "         [ 3, 13],\n",
      "         [ 3,  2],\n",
      "         [ 1, 11],\n",
      "         [ 6,  3],\n",
      "         [ 4,  6],\n",
      "         [ 5, 14],\n",
      "         [ 2, 15],\n",
      "         [ 9,  9],\n",
      "         [ 0,  9],\n",
      "         [ 7, 14],\n",
      "         [ 3,  3],\n",
      "         [ 7,  0],\n",
      "         [11,  0],\n",
      "         [ 4,  9],\n",
      "         [ 0,  5],\n",
      "         [ 8,  3],\n",
      "         [ 2, 17],\n",
      "         [ 9, 13],\n",
      "         [ 4,  7],\n",
      "         [11, 13],\n",
      "         [ 7, 16],\n",
      "         [10, 10],\n",
      "         [ 4, 14],\n",
      "         [ 2,  7],\n",
      "         [ 4,  5],\n",
      "         [ 0, 11],\n",
      "         [ 8,  1],\n",
      "         [ 6, 17],\n",
      "         [ 7, 15],\n",
      "         [ 6,  0],\n",
      "         [ 4, 16],\n",
      "         [ 0,  4],\n",
      "         [ 3,  4],\n",
      "         [ 0,  0],\n",
      "         [ 4,  0],\n",
      "         [ 3, 14],\n",
      "         [ 0,  1],\n",
      "         [ 4,  1],\n",
      "         [ 0, 17],\n",
      "         [ 7,  7],\n",
      "         [11, 15],\n",
      "         [11,  7],\n",
      "         [ 4, 13],\n",
      "         [ 8, 10],\n",
      "         [ 2,  1],\n",
      "         [ 8,  5],\n",
      "         [11, 12],\n",
      "         [ 8, 16],\n",
      "         [ 1, 13],\n",
      "         [ 2, 10],\n",
      "         [ 7,  3],\n",
      "         [ 8, 13],\n",
      "         [ 1, 15],\n",
      "         [ 8, 14],\n",
      "         [ 9,  5],\n",
      "         [ 3,  1],\n",
      "         [11,  1],\n",
      "         [ 5, 13],\n",
      "         [ 4, 12],\n",
      "         [ 0, 10],\n",
      "         [10,  8],\n",
      "         [ 1,  2],\n",
      "         [ 0,  6],\n",
      "         [ 8, 17],\n",
      "         [11, 14],\n",
      "         [ 5,  2],\n",
      "         [ 5,  8],\n",
      "         [ 7, 13],\n",
      "         [11,  5],\n",
      "         [ 4,  8],\n",
      "         [ 0, 14],\n",
      "         [ 5,  7],\n",
      "         [ 4, 10],\n",
      "         [ 0,  8],\n",
      "         [ 2,  6],\n",
      "         [ 6,  1],\n",
      "         [ 6, 16],\n",
      "         [10,  9],\n",
      "         [ 5,  0],\n",
      "         [ 2,  2],\n",
      "         [ 8, 12],\n",
      "         [ 5,  1],\n",
      "         [11, 16],\n",
      "         [ 7,  1],\n",
      "         [11, 11],\n",
      "         [ 6,  9],\n",
      "         [ 4,  3],\n",
      "         [11,  6],\n",
      "         [ 3, 15],\n",
      "         [ 2,  3],\n",
      "         [ 4, 15],\n",
      "         [ 7, 12],\n",
      "         [ 6,  5],\n",
      "         [10,  6],\n",
      "         [ 1,  5],\n",
      "         [ 6,  4],\n",
      "         [10, 17],\n",
      "         [11, 17],\n",
      "         [11,  4],\n",
      "         [ 1,  1],\n",
      "         [ 9, 15],\n",
      "         [ 1, 17],\n",
      "         [ 0, 15],\n",
      "         [ 3,  8],\n",
      "         [ 6, 14],\n",
      "         [ 8,  6]]]), (12, 18)), 'cls_output': Softmax(\n",
      "  dim=tensor([[ 0.0816,  0.3358,  0.2837, -0.5238,  0.1824,  0.5166,  0.0827,  0.0929,\n",
      "           -0.3147]], device='cuda:0')\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), 'embedding_test_dict.pt')\n",
    "print(model)\n",
    "model.setup(\"test\")\n",
    "model.eval()\n",
    "device = config[\"device\"]\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = {\"text\": [\"\"], \"image\": [None]}\n",
    "    batch[\"image\"][0] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n",
    "\n",
    "examples=[\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "            \"a display of flowers growing out and over the [MASK] [MASK] in front of [MASK] on a [MASK] [MASK].\",\n",
    "            0,\n",
    "        ],\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "            \"a a a display of flowers growing out and over the retaining wall in front of cottages on a cloudy day\",\n",
    "            4,\n",
    "        ],\n",
    "    ],\n",
    "\n",
    "n = 1\n",
    "sensor = torch.randn(1,1,12)\n",
    "out = infer(examples[0][n][0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Softmax(\n",
       "   dim=tensor([[ 0.0816,  0.3358,  0.2837, -0.5238,  0.1824,  0.5166,  0.0827,  0.0929,\n",
       "            -0.3147]], device='cuda:0')\n",
       " )]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e32618030883af29da10316c76c83da9a02f65dadf9a9b09d160d0d4f5840e5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
