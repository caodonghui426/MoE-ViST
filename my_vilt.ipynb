{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junsheng/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    debug = False\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # root_path = r'E:\\\\Download\\\\xiangguan' # 存放数据的根目录\n",
    "    root_path = r'/home/junsheng/data/xiangguan' # 存放数据的根目录\n",
    "    n_fold = 5\n",
    "    \n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Sensor\n",
    "    senser_input_num = 11\n",
    "    \n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-4 # 0.01 ->1e-4\n",
    "    decay_power = 1\n",
    "    max_epoch = 50\n",
    "    max_steps = 25000\n",
    "    warmup_steps = 2500\n",
    "    end_lr = 0\n",
    "    lr_mult = 1  # multiply lr for downstream heads\n",
    "    T_max = 8000/train_batch_size*max_epoch\n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "# config = vars(config)\n",
    "# config = dict(config)\n",
    "config\n",
    "\n",
    "if config.debug:\n",
    "    config.max_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)  # 为CPU设置随机种子\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)  # 为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有GPU设置随机种子\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "setup_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_MODE\"] = 'dryrun' # 离线模式\n",
    "try:\n",
    "    # wandb.log(key=\"*******\") # if debug\n",
    "    wandb.login() # storage in ~/.netrc file\n",
    "    anonymous = None\n",
    "except:\n",
    "    anonymous = \"must\"\n",
    "    print('\\nGet your W&B access token from here: https://wandb.ai/authorize\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>collect_time</th>\n",
       "      <th>url</th>\n",
       "      <th>picture_id</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>illuminance</th>\n",
       "      <th>soil_temperature</th>\n",
       "      <th>soil_humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>photosynthetic</th>\n",
       "      <th>sun_exposure_time</th>\n",
       "      <th>COz</th>\n",
       "      <th>soil_ph</th>\n",
       "      <th>weather_id</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label</th>\n",
       "      <th>image_path</th>\n",
       "      <th>raw_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xiangguanD3</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.66</td>\n",
       "      <td>51.78</td>\n",
       "      <td>76.73</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.02</td>\n",
       "      <td>981.09</td>\n",
       "      <td>2.48</td>\n",
       "      <td>7.92</td>\n",
       "      <td>0.1</td>\n",
       "      <td>659.34</td>\n",
       "      <td>6.80</td>\n",
       "      <td>45288.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/junsheng/data/xiangguan/pic/xiangguanD3-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xiangguanD3</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.76</td>\n",
       "      <td>56.13</td>\n",
       "      <td>226.02</td>\n",
       "      <td>8.32</td>\n",
       "      <td>8.02</td>\n",
       "      <td>980.99</td>\n",
       "      <td>3.37</td>\n",
       "      <td>28.71</td>\n",
       "      <td>0.1</td>\n",
       "      <td>660.33</td>\n",
       "      <td>6.80</td>\n",
       "      <td>45292.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/junsheng/data/xiangguan/pic/xiangguanD3-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xiangguanD4</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.70</td>\n",
       "      <td>52.00</td>\n",
       "      <td>240.80</td>\n",
       "      <td>5.50</td>\n",
       "      <td>26.10</td>\n",
       "      <td>75.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>158.00</td>\n",
       "      <td>6.82</td>\n",
       "      <td>45289.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/junsheng/data/xiangguan/pic/xiangguanD4-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xiangguanD4</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.90</td>\n",
       "      <td>55.80</td>\n",
       "      <td>670.80</td>\n",
       "      <td>5.50</td>\n",
       "      <td>26.00</td>\n",
       "      <td>15.30</td>\n",
       "      <td>2.37</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>157.00</td>\n",
       "      <td>6.82</td>\n",
       "      <td>45293.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/junsheng/data/xiangguan/pic/xiangguanD4-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xiangguanD3</td>\n",
       "      <td>14/5/2021 05</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.76</td>\n",
       "      <td>59.90</td>\n",
       "      <td>428.97</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.02</td>\n",
       "      <td>980.99</td>\n",
       "      <td>2.13</td>\n",
       "      <td>57.42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>659.34</td>\n",
       "      <td>6.80</td>\n",
       "      <td>45296.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/junsheng/data/xiangguan/pic/xiangguanD3-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     device_id  collect_time  \\\n",
       "0  xiangguanD3  14/5/2021 04   \n",
       "1  xiangguanD3  14/5/2021 04   \n",
       "2  xiangguanD4  14/5/2021 04   \n",
       "3  xiangguanD4  14/5/2021 04   \n",
       "4  xiangguanD3  14/5/2021 05   \n",
       "\n",
       "                                                 url  picture_id  temperature  \\\n",
       "0  https://daqing-haikang-image.oss-cn-hangzhou.a...        34.0        13.66   \n",
       "1  https://daqing-haikang-image.oss-cn-hangzhou.a...        34.0        13.76   \n",
       "2  https://daqing-haikang-image.oss-cn-hangzhou.a...        35.0        13.70   \n",
       "3  https://daqing-haikang-image.oss-cn-hangzhou.a...        35.0        13.90   \n",
       "4  https://daqing-haikang-image.oss-cn-hangzhou.a...        38.0        13.76   \n",
       "\n",
       "   humidity  illuminance  soil_temperature  soil_humidity  pressure  \\\n",
       "0     51.78        76.73              8.42           8.02    981.09   \n",
       "1     56.13       226.02              8.32           8.02    980.99   \n",
       "2     52.00       240.80              5.50          26.10     75.00   \n",
       "3     55.80       670.80              5.50          26.00     15.30   \n",
       "4     59.90       428.97              8.22           8.02    980.99   \n",
       "\n",
       "   wind_speed  photosynthetic  sun_exposure_time     COz  soil_ph  weather_id  \\\n",
       "0        2.48            7.92                0.1  659.34     6.80     45288.0   \n",
       "1        3.37           28.71                0.1  660.33     6.80     45292.0   \n",
       "2        1.85            6.00                0.1  158.00     6.82     45289.0   \n",
       "3        2.37           18.00                0.1  157.00     6.82     45293.0   \n",
       "4        2.13           57.42                0.1  659.34     6.80     45296.0   \n",
       "\n",
       "  label_name  label                                         image_path  \\\n",
       "0         出苗      0  /home/junsheng/data/xiangguan/pic/xiangguanD3-...   \n",
       "1         出苗      0  /home/junsheng/data/xiangguan/pic/xiangguanD3-...   \n",
       "2         出苗      0  /home/junsheng/data/xiangguan/pic/xiangguanD4-...   \n",
       "3         出苗      0  /home/junsheng/data/xiangguan/pic/xiangguanD4-...   \n",
       "4         出苗      0  /home/junsheng/data/xiangguan/pic/xiangguanD3-...   \n",
       "\n",
       "   raw_label  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xiangguan = pd.read_csv(\"xiangguan-corn-2.csv\")\n",
    "df_xiangguan['image_path'] = df_xiangguan['url'].map(lambda x:os.path.join(config.root_path,'pic',x.split('/')[-1]))\n",
    "df_xiangguan['raw_label'] = df_xiangguan['label'].astype(dtype='int64')\n",
    "# del df_xiangguan['label_name']\n",
    "df_xiangguan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997\n",
      "7997\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 检查图片下载的全不全\n",
    "pic = df_xiangguan.image_path.map(lambda x:x.split('/')[-1]).unique()\n",
    "print(len(pic))\n",
    "file_ls = os.listdir(\"/home/junsheng/data/xiangguan/pic\")\n",
    "print(len(file_ls))\n",
    "ret = list(set(pic) ^ set(file_ls))\n",
    "print(ret) #差集\n",
    "assert len(pic)==len(file_ls),\"请检查下载的图片，缺了{}个\".format(len(pic)-len(file_ls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化非object列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['picture_id', 'temperature', 'humidity', 'illuminance', 'soil_temperature', 'soil_humidity', 'pressure', 'wind_speed', 'photosynthetic', 'sun_exposure_time', 'COz', 'soil_ph', 'weather_id', 'label']\n",
      "{'picture_id': (34.0, 23524.0), 'temperature': (4.9, 44.6), 'humidity': (13.2, 98.9), 'illuminance': (0.0, 28165.0), 'soil_temperature': (0.0, 20.6), 'soil_humidity': (0.0, 58.5), 'pressure': (0.0, 3914.6), 'wind_speed': (0.0, 8.66), 'photosynthetic': (0.0, 1822.0), 'sun_exposure_time': (0.0, 9.9), 'COz': (0.0, 55000.0), 'soil_ph': (5.02, 14.0), 'weather_id': (45288.0, 65382.0), 'label': (0, 3)}\n"
     ]
    }
   ],
   "source": [
    "number_title = []\n",
    "recorder = {}\n",
    "for title in df_xiangguan:\n",
    "    # print(df_xiangguan[title].head())\n",
    "    if title == 'raw_label':\n",
    "        continue\n",
    "    if df_xiangguan[title].dtype != \"object\":\n",
    "        \n",
    "        number_title.append(title)\n",
    "        x_min = df_xiangguan[title].min()\n",
    "        x_max = df_xiangguan[title].max()\n",
    "        # print(x_min,x_max)\n",
    "        recorder[title] = (x_min,x_max)\n",
    "        df_xiangguan[title] = df_xiangguan[title].map(lambda x:(x-x_min)/(x_max - x_min))\n",
    "print(number_title)\n",
    "print(recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 11\n"
     ]
    }
   ],
   "source": [
    "xiangguan_sensor = ['temperature', 'humidity', 'illuminance', 'soil_temperature', 'soil_humidity', 'pressure', 'wind_speed', 'photosynthetic', 'sun_exposure_time', 'COz', 'soil_ph']\n",
    "\n",
    "df_xiangguan['sensor'] = df_xiangguan[xiangguan_sensor].values.tolist()\n",
    "print(\"input dim:\",len(xiangguan_sensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17169, 21)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_xiangguan\n",
    "if config.debug:\n",
    "    df = df[:100]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0.0    3434\n",
       "1.0    3434\n",
       "2.0    3434\n",
       "3.0    3434\n",
       "4.0    3433\n",
       "Name: label_name, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=config.seed)  \n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df,df.label_name)):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "df.groupby(['fold'])['label_name'].count()# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv(\"test_fold.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransforms = transforms.Compose([\n",
    "    transforms.Resize((config.img_size,config.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.7136, 0.7118, 0.6788],\n",
    "    std=[0.3338, 0.3453, 0.3020],\n",
    "    \n",
    ")\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img =  Image.open(path).convert('RGB')\n",
    "    img = myTransforms(img)\n",
    "    return img\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df         = df\n",
    "        self.label      = label\n",
    "        self.sensors = df['sensor'].tolist()\n",
    "        self.img_paths  = df['image_path'].tolist()   \n",
    "        if self.label:\n",
    "            self.labels = df['label'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path  = self.img_paths[index]\n",
    "        img = load_img(img_path)\n",
    "        sensor = self.sensors[index]\n",
    "        sensor = torch.tensor(sensor).unsqueeze(0) #[1,n]\n",
    "        if self.label:\n",
    "            label = self.labels[index]\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
    "        else:\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataloader(fold:int):\n",
    "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "\n",
    "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    print(\"train_df.shape:\",train_df.shape)\n",
    "    print(\"valid_df.shape:\",valid_df.shape)\n",
    "\n",
    "    train_data  = BuildDataset(df=train_df,label=True)\n",
    "    valid_data = BuildDataset(df=valid_df,label=True)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=config.train_batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=config.valid_batch_size,shuffle=False)\n",
    "    # test_loader = DataLoader(test_data, batch_size=config.test_batch_size,shuffle=False)\n",
    "    return train_loader,valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (13735, 22)\n",
      "valid_df.shape: (3434, 22)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = BuildDataset(df=df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.train_batch_size,shuffle=True)\n",
    "# valid_loader = DataLoader(train_dataset, batch_size=config.valid_batch_size,shuffle=True)\n",
    "train_loader,valid_loader = fetch_dataloader(fold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 384, 384])\n",
      "torch.Size([32, 1, 11])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "img,sensor,label = next(iter(train_loader))\n",
    "print(img.shape)\n",
    "print(sensor.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device)\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pretrained weights exist or were found for this model. Using random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 sensorViLTransformerSS(\n",
      "  (sensor_linear): Linear(in_features=11, out_features=768, bias=True)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n",
      "1 Linear(in_features=11, out_features=768, bias=True)\n",
      "2 Embedding(2, 768)\n",
      "3 VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "4 PatchEmbed(\n",
      "  (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      ")\n",
      "5 Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "6 Dropout(p=0.1, inplace=False)\n",
      "7 ModuleList(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "8 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "9 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "10 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "11 Linear(in_features=768, out_features=2304, bias=True)\n",
      "12 Dropout(p=0.0, inplace=False)\n",
      "13 Linear(in_features=768, out_features=768, bias=True)\n",
      "14 Dropout(p=0.1, inplace=False)\n",
      "15 Identity()\n",
      "16 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "17 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "18 Linear(in_features=768, out_features=3072, bias=True)\n",
      "19 GELU(approximate=none)\n",
      "20 Linear(in_features=3072, out_features=768, bias=True)\n",
      "21 Dropout(p=0.1, inplace=False)\n",
      "22 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "23 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "24 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "25 Linear(in_features=768, out_features=2304, bias=True)\n",
      "26 Dropout(p=0.0, inplace=False)\n",
      "27 Linear(in_features=768, out_features=768, bias=True)\n",
      "28 Dropout(p=0.1, inplace=False)\n",
      "29 Identity()\n",
      "30 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "31 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "32 Linear(in_features=768, out_features=3072, bias=True)\n",
      "33 GELU(approximate=none)\n",
      "34 Linear(in_features=3072, out_features=768, bias=True)\n",
      "35 Dropout(p=0.1, inplace=False)\n",
      "36 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "37 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "38 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "39 Linear(in_features=768, out_features=2304, bias=True)\n",
      "40 Dropout(p=0.0, inplace=False)\n",
      "41 Linear(in_features=768, out_features=768, bias=True)\n",
      "42 Dropout(p=0.1, inplace=False)\n",
      "43 Identity()\n",
      "44 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "45 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "46 Linear(in_features=768, out_features=3072, bias=True)\n",
      "47 GELU(approximate=none)\n",
      "48 Linear(in_features=3072, out_features=768, bias=True)\n",
      "49 Dropout(p=0.1, inplace=False)\n",
      "50 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "51 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "52 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "53 Linear(in_features=768, out_features=2304, bias=True)\n",
      "54 Dropout(p=0.0, inplace=False)\n",
      "55 Linear(in_features=768, out_features=768, bias=True)\n",
      "56 Dropout(p=0.1, inplace=False)\n",
      "57 Identity()\n",
      "58 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "59 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "60 Linear(in_features=768, out_features=3072, bias=True)\n",
      "61 GELU(approximate=none)\n",
      "62 Linear(in_features=3072, out_features=768, bias=True)\n",
      "63 Dropout(p=0.1, inplace=False)\n",
      "64 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "65 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "66 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "67 Linear(in_features=768, out_features=2304, bias=True)\n",
      "68 Dropout(p=0.0, inplace=False)\n",
      "69 Linear(in_features=768, out_features=768, bias=True)\n",
      "70 Dropout(p=0.1, inplace=False)\n",
      "71 Identity()\n",
      "72 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "73 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "74 Linear(in_features=768, out_features=3072, bias=True)\n",
      "75 GELU(approximate=none)\n",
      "76 Linear(in_features=3072, out_features=768, bias=True)\n",
      "77 Dropout(p=0.1, inplace=False)\n",
      "78 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "79 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "80 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "81 Linear(in_features=768, out_features=2304, bias=True)\n",
      "82 Dropout(p=0.0, inplace=False)\n",
      "83 Linear(in_features=768, out_features=768, bias=True)\n",
      "84 Dropout(p=0.1, inplace=False)\n",
      "85 Identity()\n",
      "86 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "87 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "88 Linear(in_features=768, out_features=3072, bias=True)\n",
      "89 GELU(approximate=none)\n",
      "90 Linear(in_features=3072, out_features=768, bias=True)\n",
      "91 Dropout(p=0.1, inplace=False)\n",
      "92 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "93 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "94 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "95 Linear(in_features=768, out_features=2304, bias=True)\n",
      "96 Dropout(p=0.0, inplace=False)\n",
      "97 Linear(in_features=768, out_features=768, bias=True)\n",
      "98 Dropout(p=0.1, inplace=False)\n",
      "99 Identity()\n",
      "100 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "101 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "102 Linear(in_features=768, out_features=3072, bias=True)\n",
      "103 GELU(approximate=none)\n",
      "104 Linear(in_features=3072, out_features=768, bias=True)\n",
      "105 Dropout(p=0.1, inplace=False)\n",
      "106 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "107 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "108 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "109 Linear(in_features=768, out_features=2304, bias=True)\n",
      "110 Dropout(p=0.0, inplace=False)\n",
      "111 Linear(in_features=768, out_features=768, bias=True)\n",
      "112 Dropout(p=0.1, inplace=False)\n",
      "113 Identity()\n",
      "114 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "115 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "116 Linear(in_features=768, out_features=3072, bias=True)\n",
      "117 GELU(approximate=none)\n",
      "118 Linear(in_features=3072, out_features=768, bias=True)\n",
      "119 Dropout(p=0.1, inplace=False)\n",
      "120 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "121 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "122 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "123 Linear(in_features=768, out_features=2304, bias=True)\n",
      "124 Dropout(p=0.0, inplace=False)\n",
      "125 Linear(in_features=768, out_features=768, bias=True)\n",
      "126 Dropout(p=0.1, inplace=False)\n",
      "127 Identity()\n",
      "128 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "129 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "130 Linear(in_features=768, out_features=3072, bias=True)\n",
      "131 GELU(approximate=none)\n",
      "132 Linear(in_features=3072, out_features=768, bias=True)\n",
      "133 Dropout(p=0.1, inplace=False)\n",
      "134 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "135 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "136 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "137 Linear(in_features=768, out_features=2304, bias=True)\n",
      "138 Dropout(p=0.0, inplace=False)\n",
      "139 Linear(in_features=768, out_features=768, bias=True)\n",
      "140 Dropout(p=0.1, inplace=False)\n",
      "141 Identity()\n",
      "142 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "143 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "144 Linear(in_features=768, out_features=3072, bias=True)\n",
      "145 GELU(approximate=none)\n",
      "146 Linear(in_features=3072, out_features=768, bias=True)\n",
      "147 Dropout(p=0.1, inplace=False)\n",
      "148 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "149 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "150 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "151 Linear(in_features=768, out_features=2304, bias=True)\n",
      "152 Dropout(p=0.0, inplace=False)\n",
      "153 Linear(in_features=768, out_features=768, bias=True)\n",
      "154 Dropout(p=0.1, inplace=False)\n",
      "155 Identity()\n",
      "156 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "157 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "158 Linear(in_features=768, out_features=3072, bias=True)\n",
      "159 GELU(approximate=none)\n",
      "160 Linear(in_features=3072, out_features=768, bias=True)\n",
      "161 Dropout(p=0.1, inplace=False)\n",
      "162 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "163 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "164 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "165 Linear(in_features=768, out_features=2304, bias=True)\n",
      "166 Dropout(p=0.0, inplace=False)\n",
      "167 Linear(in_features=768, out_features=768, bias=True)\n",
      "168 Dropout(p=0.1, inplace=False)\n",
      "169 Identity()\n",
      "170 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "171 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "172 Linear(in_features=768, out_features=3072, bias=True)\n",
      "173 GELU(approximate=none)\n",
      "174 Linear(in_features=3072, out_features=768, bias=True)\n",
      "175 Dropout(p=0.1, inplace=False)\n",
      "176 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "177 Linear(in_features=768, out_features=768, bias=True)\n",
      "178 Tanh()\n",
      "179 Pooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "180 Linear(in_features=768, out_features=768, bias=True)\n",
      "181 Tanh()\n",
      "182 Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = sensorViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "model.to(config.device)\n",
    "print(config.device)\n",
    "for i,m in enumerate(model.modules()):\n",
    "    print(i,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.mse_loss #均方误差损失函数\n",
    "# criterion = F.mae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
    "    for step, (img, sensor,label) in pbar:         \n",
    "        # img = img.to(device, dtype=torch.float)\n",
    "        # sensor  = sensor.to(device, dtype=torch.float)\n",
    "        # label  = label.to(device, dtype=torch.float)\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        #一坨优化\n",
    "        optimizer.zero_grad()#每一次反向传播之前都要归零梯度\n",
    "        loss.backward()      #反向传播\n",
    "        optimizer.step()     #固定写法\n",
    "        scheduler.step()\n",
    "     \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_mem=f'{mem:0.2f} GB')\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    val_scores = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
    "    for step, (img, sensor,label) in pbar:               \n",
    "        \n",
    "        \n",
    "        batch_size = img.size(0)\n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred  = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        \n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_memory=f'{mem:0.2f} GB')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "     # init wandb\n",
    "    run = wandb.init(project=\"vilt\",\n",
    "                    config={k: v for k, v in dict(vars(config)).items() if '__' not in k},\n",
    "                    # config={k: v for k, v in dict(config).items() if '__' not in k},\n",
    "                    anonymous=anonymous,\n",
    "                    # name=f\"vilt|fold-{config.valid_fold}\",\n",
    "                    name=f\"vilt|\",\n",
    "                    # group=config.wandb_group,\n",
    "                    )\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_valid_loss = 9999\n",
    "    history = defaultdict(list)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        val_loss = valid_one_epoch(model,valid_loader,device=device,optimizer=optimizer)\n",
    "        history['Train Loss'].append(train_loss)\n",
    "        history['Valid Loss'].append(val_loss)\n",
    "\n",
    "        wandb.log({\"Train Loss\": train_loss,\n",
    "                    \"Valid Loss\": val_loss,\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "                })\n",
    "        if best_valid_loss > val_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            # model_file_path = os.path.join(wandb.run.dir,\"epoch-{}-{}.bin\".format(epoch,wandb.run.id))\n",
    "            model_file_path = os.path.join(wandb.run.dir,\"epoch-best.bin\")\n",
    "            run.summary[\"Best Epoch\"] = epoch\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(\"model save to\", model_file_path)\n",
    "    run.finish()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=config.T_max, \n",
    "                                                   eta_min=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model, history = run_training(model, optimizer, scheduler,device=config.device,num_epochs=config.max_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 384, 384]) torch.Size([4, 1, 11]) tensor([0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    }
   ],
   "source": [
    "for (img,sensor,label) in valid_loader:\n",
    "    print(img.shape,sensor.shape,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'embedding_test_dict.pt')\n",
    "# print(model)\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/junsheng/ViLT/wandb/offline-run-20220811_120519-nzfb1xoz/files/epoch-best.bin\"))\n",
    "model.eval()\n",
    "device = config.device\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = dict()\n",
    "    batch[\"image\"] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9552, 0.2386, 0.8053, 0.8996, 0.8717, 0.2519, 0.1701, 0.5269, 0.7088,\n",
      "        0.9925, 0.1388])\n",
      "img.shape: torch.Size([1, 3, 352, 608])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/3499233738.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensor_feats': tensor([[[ 3.0723e-06, -1.7554e-07,  1.5229e-07, -2.1967e-08, -2.2484e-06,\n",
      "           3.2141e-06,  6.0833e-06, -5.4010e-09, -1.9050e-07, -7.8322e-07,\n",
      "          -1.5750e-07,  3.6265e-06, -3.8516e-08, -1.0081e-05, -1.4026e-08,\n",
      "          -1.8135e-07, -2.7653e-06,  9.1704e-09, -2.7459e-06, -5.0790e-09,\n",
      "           9.4938e-07,  4.1226e-09,  7.3300e-08, -1.0659e-08,  6.3456e-07,\n",
      "           3.1712e-06, -2.6293e-06, -1.0807e-09,  2.9015e-06,  4.4993e-06,\n",
      "          -9.2933e-06,  1.0645e-06, -4.3114e-07,  2.2734e-08, -9.7144e-07,\n",
      "           1.0887e-06,  6.2589e-08, -7.6105e-08, -2.9577e-09,  2.1593e-06,\n",
      "           7.3475e-08,  8.4675e-06,  3.4534e-08, -2.6103e-07, -4.6536e-06,\n",
      "           1.3890e-08,  3.9890e-08, -4.9402e-06, -2.0919e-06, -2.7671e-06,\n",
      "           1.0942e-07, -4.3214e-06, -3.1810e-08, -1.3133e-09, -2.3513e-06,\n",
      "          -1.7040e-09, -9.4722e-06,  2.6569e-06,  6.1225e-07, -5.3678e-08,\n",
      "           1.0648e-05, -2.6208e-07, -3.8011e-07,  1.9068e-08, -7.3565e-08,\n",
      "          -4.4702e-06,  4.7450e-10,  8.2008e-07, -1.0050e-08,  4.5204e-08,\n",
      "          -5.1802e-09, -4.5415e-07, -3.0031e-08,  7.7630e-07,  2.2411e-07,\n",
      "          -3.2618e-07,  2.4547e-09,  2.5815e-08, -1.8262e-09,  1.8196e-07,\n",
      "          -1.8963e-05,  3.3366e-10, -8.6666e-08, -2.1401e-06, -1.7300e-06,\n",
      "           1.6550e-08, -2.3359e-06,  1.8895e-07,  6.3381e-06, -3.2458e-07,\n",
      "           5.9980e-07,  8.5155e-10, -8.2974e-07,  3.7210e-06,  1.0432e-06,\n",
      "          -9.0340e-09, -2.6983e-08, -8.9112e-10, -1.1647e-08, -7.3879e-09,\n",
      "          -9.9330e-06,  4.5480e-07, -5.4927e-06,  4.2634e-06,  3.0792e-06,\n",
      "           4.8907e-06,  4.8906e-07,  1.9267e-05,  2.1950e-08, -1.8796e-08,\n",
      "           2.0869e-06,  2.9192e-06,  3.1749e-07, -7.7507e-08,  1.3225e-07,\n",
      "           5.8028e-06, -3.7069e-05, -3.0038e-07,  4.7274e-08, -3.7108e-07,\n",
      "          -5.0080e-06,  3.3471e-08, -4.8558e-06,  5.2316e-07,  3.9797e-06,\n",
      "           6.6093e-06,  1.0735e-06,  2.3619e-08,  1.2651e-07,  1.5612e-07,\n",
      "           1.2897e-05, -8.7070e-07, -2.0266e-07,  7.9312e-07,  1.9808e-05,\n",
      "           9.1687e-08, -1.8321e-07, -4.1519e-05, -8.2110e-06,  4.8526e-08,\n",
      "          -1.7259e-05,  3.3687e-06, -4.5652e-07,  3.0126e-06,  1.6767e-09,\n",
      "           4.7585e-08,  7.9376e-07, -5.0571e-09,  2.4012e-07, -4.3470e-06,\n",
      "           2.7637e-06,  2.7809e-07, -2.0349e-07,  1.6128e-08, -9.3054e-06,\n",
      "          -4.1585e-06, -1.0164e-06, -4.2375e-06,  9.7744e-06,  3.5280e-08,\n",
      "           4.2501e-06,  3.6184e-07, -7.5656e-08,  1.3486e-06, -9.1839e-06,\n",
      "           1.5882e-06, -5.5629e-08,  1.2909e-10, -6.2697e-06,  1.1519e-07,\n",
      "           1.3249e-07,  1.1594e-08,  1.0702e-07, -1.7704e-10,  1.3480e-06,\n",
      "           7.6663e-07,  3.0214e-06,  3.9514e-06,  1.8225e-07, -8.6592e-09,\n",
      "          -5.3212e-08,  1.3924e-06, -3.9184e-07, -5.9995e-06,  1.8678e-06,\n",
      "           2.4382e+00,  2.7470e-05,  1.7481e-07, -1.0066e-06,  1.1384e-06,\n",
      "           3.5771e-06,  1.6830e-08, -3.2149e-08,  1.3233e-06,  1.0347e-07,\n",
      "          -9.7003e-06, -3.4758e-06,  5.2135e-07,  9.9343e-09, -1.0689e-06,\n",
      "           1.1459e-07, -1.0104e-06, -1.9741e-06,  1.3659e-07,  2.0566e-09,\n",
      "           1.8341e-05,  2.3386e-09,  1.5413e-06, -1.3614e-05, -1.3377e-08,\n",
      "          -1.1260e-05,  6.5263e-06,  3.0834e-06,  2.0637e-09,  5.9669e-08,\n",
      "          -3.8255e-06, -2.6440e-07,  4.1761e-09, -1.6489e-09, -4.5722e-07,\n",
      "           1.5958e-09, -3.6105e-09,  8.5279e-07, -3.3948e-08,  1.1833e-05,\n",
      "          -3.8564e-08, -7.7325e-08, -1.2405e-07, -8.0368e-07,  3.5841e-07,\n",
      "          -1.1580e-06, -3.2381e-05,  5.1660e-08, -3.2447e-07, -1.0490e-08,\n",
      "          -4.8042e-06, -2.8876e-07,  2.0616e-06,  1.1622e-07,  1.9249e-07,\n",
      "          -3.8200e-08,  1.9088e-06,  3.3675e-07,  6.7106e-09, -3.7893e-08,\n",
      "           2.0501e-05,  4.8386e-07,  6.9481e-08, -3.0775e-07,  2.5659e-09,\n",
      "           1.1468e-07, -9.3127e-09,  7.0764e-08, -3.8759e-07, -1.8538e-07,\n",
      "          -2.3776e-06, -1.7561e-08,  1.2938e-08,  1.7631e-07, -1.6005e-08,\n",
      "           1.3190e-06, -1.8780e-06,  2.6531e-06,  3.2918e-07,  2.3177e-09,\n",
      "           9.0369e-09, -7.8687e-07,  5.2690e-06,  7.8105e-06, -4.3678e-08,\n",
      "          -6.3880e-09, -2.5165e-05,  2.9724e-08, -3.2173e-08, -3.1059e-06,\n",
      "          -6.6636e-08, -1.1658e-05, -9.0420e-10, -1.2132e-06,  1.5681e-06,\n",
      "           1.2708e-05,  6.3789e-08,  2.4808e-07,  6.8361e-08, -1.3197e-06,\n",
      "           1.6300e-08,  1.1385e-07, -6.5471e-06, -3.8585e-09,  6.2307e-07,\n",
      "           2.1949e-07, -3.5512e-06,  7.8617e-06,  2.3448e-08, -1.6660e-08,\n",
      "          -1.6433e-08,  1.7339e-08,  4.2444e-06,  3.3811e-07, -7.9330e-07,\n",
      "           1.5180e-07, -2.0710e-06,  6.5899e-07, -1.1119e-06,  1.2215e-06,\n",
      "           8.6579e-07, -2.0877e-08, -2.4562e-07,  1.7908e-06, -1.7872e-06,\n",
      "          -8.2790e-07, -6.9340e-07,  5.9892e-06, -4.0061e-09,  1.3201e-06,\n",
      "          -3.6311e-09, -1.3464e-06,  1.5923e-07, -4.1587e-09,  1.3169e-06,\n",
      "          -1.9885e-07,  1.8544e-07, -1.6652e-07, -1.1913e-09,  1.2543e-05,\n",
      "           3.0118e-06, -6.8633e-08, -2.3187e-09,  4.6512e-07,  7.7693e-07,\n",
      "           9.1277e-08, -1.0001e-06, -6.5130e-07, -2.7530e-06,  6.2069e-07,\n",
      "           2.1020e-06, -2.5445e-07, -4.6137e-08, -8.3698e-07,  6.8474e-08,\n",
      "          -1.7602e-06, -2.1282e-05, -9.8710e-07,  5.4287e-07, -1.6907e-08,\n",
      "          -4.9593e-09,  6.9803e-06, -2.1279e-08,  7.3531e-07,  1.0554e-08,\n",
      "          -5.0881e-08,  2.0157e-06,  2.3291e-06,  2.4314e-06, -1.7396e-08,\n",
      "          -6.6116e-09,  1.6800e-05, -2.3752e-06,  1.1090e-08,  9.4186e-07,\n",
      "          -4.6295e-09, -4.5012e-09, -1.1027e-08, -1.7150e-08, -5.3582e-07,\n",
      "           3.9801e-08,  6.3354e-09, -3.9512e-06,  1.4280e-05,  1.4428e-09,\n",
      "          -4.7703e-06,  1.8668e-06, -2.3177e-06,  4.7129e-07,  5.5295e-07,\n",
      "          -6.5444e-07,  1.9001e-07,  2.8564e-09,  4.2131e-09,  3.1700e-08,\n",
      "          -5.6144e-07, -3.3728e-07, -4.8253e-06, -1.2011e-07, -1.2578e-05,\n",
      "          -1.0286e-08,  5.6515e-06, -3.3863e-07, -1.4199e-07, -1.1463e-06,\n",
      "           1.4947e-05, -9.9756e-06,  4.2099e-08, -4.5970e-07, -1.9721e-06,\n",
      "          -1.4928e-08,  6.7723e-08, -3.3629e-08,  1.7256e-06, -4.4908e-06,\n",
      "           7.6818e-06, -9.2188e-04, -1.0623e-07, -3.6236e-07,  2.1087e-07,\n",
      "           2.2562e-07,  1.1825e-07,  2.1053e-09, -1.2763e-08, -6.2645e-06,\n",
      "          -8.6400e-06, -6.6074e-09,  2.3928e-06,  1.0341e-06, -9.1274e-08,\n",
      "           8.0273e-07,  5.8029e-08,  3.8576e-07, -2.4575e-07, -2.8638e-06,\n",
      "           1.9994e-07,  8.7311e-09,  6.0282e-07,  9.6328e-06,  6.2586e-07,\n",
      "          -9.2212e-08,  4.0201e-06,  5.0067e-08, -2.9323e-09, -7.3965e-09,\n",
      "          -1.3177e-06, -5.8452e-07, -1.6813e-06,  2.0465e-06,  1.2437e-05,\n",
      "           2.9860e-09,  5.9804e-08,  5.3445e-08, -1.7165e-06,  5.6213e-08,\n",
      "          -7.1813e-06,  5.0317e-06,  1.3395e-06, -4.1941e-07, -2.4110e-08,\n",
      "          -1.1952e-06, -3.2470e-08, -1.3269e-07, -8.6561e-08, -3.4704e-07,\n",
      "          -4.7496e-07,  3.0318e-06, -1.9495e-06,  1.6571e-07, -1.4059e-07,\n",
      "          -3.2401e-06, -1.5929e-06,  1.2848e-06, -5.1812e-06, -5.9173e-09,\n",
      "           8.0958e-09, -2.5112e-07,  6.3120e-08, -7.0843e-06,  6.2890e-06,\n",
      "           1.5063e-06, -1.3331e-08, -6.1629e-07, -1.0127e-06,  2.2348e-06,\n",
      "          -9.7075e-09,  5.5668e-06,  1.3291e-06, -1.3370e-05, -1.6887e-08,\n",
      "           5.2129e-07, -1.0116e-08, -3.6407e-07, -6.9511e-07,  1.4989e-06,\n",
      "          -1.1162e-08, -8.8722e-08,  3.1746e-06, -3.2039e-08,  7.3414e-08,\n",
      "           3.3959e-08,  7.3257e-06,  6.1773e-07, -2.0627e-07, -3.1065e-08,\n",
      "          -1.4231e-06,  8.8517e-08, -4.0479e-06, -1.8854e-05,  2.0245e-07,\n",
      "           6.6571e-08,  2.4725e-07, -4.9766e-09, -3.6539e-07,  5.5708e-09,\n",
      "          -6.8911e-08,  4.7601e-07, -1.4619e-09, -2.3620e-08, -6.8413e-06,\n",
      "           6.5422e-10, -2.3541e-07, -1.1248e-06, -2.4799e-06,  2.6442e-09,\n",
      "           4.7853e-08, -9.3691e-06,  5.2711e-06,  3.2681e-07, -4.6305e-09,\n",
      "          -1.5413e-08, -6.0654e-10,  1.9829e-06,  2.1303e-08, -2.4998e-09,\n",
      "           9.6417e-06, -2.9368e-06,  6.9716e-08, -1.6374e-06, -7.8645e-06,\n",
      "           4.2350e-07, -9.8586e-08, -4.0901e-07,  4.8835e-07, -6.1373e-09,\n",
      "           5.2185e-09,  1.1267e-06, -1.6042e-08,  8.3461e-08, -2.0667e-08,\n",
      "           3.1868e-06,  4.5913e-06,  5.6506e-07, -1.1695e-08,  3.1359e-06,\n",
      "           7.2843e-07,  5.3213e-07,  2.5098e-06,  9.0408e-08,  3.4283e-07,\n",
      "           5.9762e-07, -6.7579e-08, -7.1828e-07, -1.2225e-07, -1.9978e-05,\n",
      "           4.1750e-09,  6.7184e-07,  1.4025e-07, -1.2995e-08,  7.7824e-09,\n",
      "           1.9227e-06,  5.3590e-07,  1.8780e-07, -3.6603e-08, -6.5485e-07,\n",
      "           5.4650e-09, -2.8819e-07, -1.0265e-05, -1.2783e-05, -2.7736e-09,\n",
      "           4.3043e-08,  9.2450e-08,  1.1073e-06,  5.1688e-07,  1.0803e-08,\n",
      "          -7.5087e-07, -1.2425e-08, -1.7324e-07,  3.1914e-06,  3.0812e-08,\n",
      "           8.2454e-07,  1.4988e-06,  3.3312e-07,  1.9099e-06, -2.6439e-05,\n",
      "          -4.0890e-06,  3.2042e-06, -1.3003e-09, -1.7933e-08, -9.6058e-06,\n",
      "           4.1744e-06,  9.4759e-08,  7.0423e-09, -5.8366e-08,  4.0765e-06,\n",
      "           9.3381e-08,  1.4183e-08, -8.8225e-09,  2.0243e-08, -2.3166e-07,\n",
      "           2.9122e-06,  5.9431e-09, -6.5809e-07, -5.8936e-07, -3.4326e-08,\n",
      "           1.4523e-06,  1.7069e-08, -5.9587e-08, -1.2063e-06, -1.1997e-07,\n",
      "          -2.2054e-06, -1.6689e-06,  2.1246e-07,  6.2270e-08,  1.6191e-06,\n",
      "          -1.4131e-05,  1.2284e-05,  1.9956e-09, -1.1322e-09,  1.4154e-07,\n",
      "          -4.2383e-09, -7.4691e-06, -5.8829e-06, -4.7434e-06,  1.6903e-08,\n",
      "          -4.7152e-08,  3.3382e-09, -1.2117e-08, -1.7598e-08, -9.4448e-07,\n",
      "          -1.2268e-06, -4.0027e-08, -1.1813e-07, -6.1725e-06,  7.0416e-08,\n",
      "           6.7489e-08, -1.7169e-06, -2.6514e-07,  2.7427e-06,  2.2997e-09,\n",
      "           7.4122e-08, -6.9420e-10,  3.1030e-07,  2.8974e-08,  4.6630e-09,\n",
      "           1.9685e-07, -1.7612e-09,  1.2932e-07,  2.4889e-09, -6.4275e-08,\n",
      "           1.3948e-07, -1.6682e-07,  4.6279e-07,  1.2840e-07, -5.1384e-06,\n",
      "           1.0605e-07, -1.8766e-06,  8.7417e-10,  2.5751e-07, -3.5626e-06,\n",
      "           1.6775e-05,  3.0148e-06, -4.4158e-07, -9.3211e-08, -1.4789e-05,\n",
      "           1.2645e-07, -1.3182e-05, -1.0014e-06,  2.0630e-05,  4.9534e-07,\n",
      "           6.1195e-08,  5.1913e-06,  1.2311e-05, -5.5544e-07, -6.9333e-07,\n",
      "           3.7712e-06,  5.2931e-09, -2.3973e-08, -7.8219e-08, -8.3641e-09,\n",
      "           3.7597e-08,  1.3729e-07,  5.2355e-08, -3.9222e-06, -1.7822e-06,\n",
      "          -1.6950e-07,  1.1340e-07, -1.3477e-06, -4.8292e-06,  1.4061e-07,\n",
      "          -4.3786e-07, -4.1365e-08,  7.2878e-08, -8.9773e-08, -4.6045e-09,\n",
      "          -7.4258e-07, -1.5908e-05,  3.6720e-06,  8.8979e-07,  5.6846e-08,\n",
      "           1.7180e-09,  2.5191e-06, -4.5061e-08,  5.2641e-08,  5.4993e-08,\n",
      "           4.9846e-08,  2.1266e-06,  3.1196e-07,  9.0261e-08, -2.8499e-07,\n",
      "          -1.0393e-07, -1.7390e-06,  2.3264e-06, -3.0165e-06,  2.8547e-07,\n",
      "          -4.5362e-07, -6.7905e-06, -3.2309e-07, -2.3055e-08, -2.5481e-07,\n",
      "          -2.1910e-09,  1.7808e-06,  6.7258e-08, -5.6061e-07, -1.1196e-07,\n",
      "           1.0037e-07,  6.1459e-07, -1.1338e-08, -2.1081e-06,  2.4831e-08,\n",
      "           4.2150e-06, -6.9252e-09, -8.9839e-08, -7.0023e-08,  3.2632e-09,\n",
      "           2.1297e-06,  8.3259e-08, -2.1960e-06,  5.5825e-08,  2.2851e-08,\n",
      "          -5.2894e-09,  5.7358e-06, -2.1927e-07,  8.8418e-09, -2.2206e-06,\n",
      "          -1.5648e-07, -6.7006e-09,  1.3674e-07,  1.9016e-07,  1.8238e-07,\n",
      "          -6.0125e-06, -4.1107e-08, -9.2180e-06, -1.2954e-07, -2.9015e-06,\n",
      "           4.0419e-06,  4.3022e-06,  1.2596e-07,  5.3046e-08, -2.4119e-05,\n",
      "          -1.1273e-06, -1.0762e-08, -5.2263e-07,  2.3763e-08, -6.2570e-06,\n",
      "           7.2738e-07, -2.0995e-05,  2.9127e-06,  6.8859e-08,  4.8712e-07,\n",
      "          -8.0636e-07,  3.0683e-07, -3.7270e-06]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9464e-07,\n",
      "           3.1839e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1837e-07, -3.4246e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5569e-07,  ..., -7.9448e-07,\n",
      "           3.1838e-07, -3.4248e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1837e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4242e-06]]], device='cuda:0'), 'cls_feats': tensor([[ 4.8293e-02, -5.2256e-02,  5.2219e-02,  3.0762e-02,  4.9417e-02,\n",
      "          5.2200e-02,  5.1861e-02,  5.2183e-02, -5.2388e-02, -4.5775e-02,\n",
      "         -5.1938e-02, -5.2262e-02, -5.0589e-02,  6.3603e-07, -7.6977e-08,\n",
      "         -2.9580e-02, -5.1468e-02,  4.9361e-02, -5.2212e-02,  5.2229e-02,\n",
      "         -4.9964e-02, -5.2398e-02, -5.2135e-02,  5.1885e-02, -4.8644e-02,\n",
      "         -5.2148e-02,  5.2239e-02, -5.1981e-02, -5.1845e-02, -5.1998e-02,\n",
      "          5.2279e-02, -2.1983e-02, -3.2564e-02,  1.3394e-03, -5.1975e-02,\n",
      "         -5.1897e-02,  5.1391e-02,  5.2100e-02,  5.1646e-02, -9.3074e-09,\n",
      "         -2.0016e-03, -4.9806e-02,  5.1931e-02,  5.1544e-02, -5.2183e-02,\n",
      "          5.2316e-02,  5.2050e-02,  5.2140e-02,  4.8259e-02, -5.2103e-02,\n",
      "         -5.1266e-02,  5.2353e-02,  5.2351e-02,  5.1786e-02,  5.2070e-02,\n",
      "         -4.9512e-02, -5.1905e-02, -5.2301e-02, -5.2049e-02,  4.8510e-02,\n",
      "          5.2341e-02, -7.2710e-08, -5.1995e-02,  5.0491e-02,  5.2376e-02,\n",
      "         -3.2656e-03, -2.8657e-02,  5.0017e-02, -2.8981e-02, -5.1899e-02,\n",
      "         -5.2125e-02,  4.1856e-09, -5.2181e-02, -4.8646e-02,  3.1875e-02,\n",
      "          5.2200e-02,  5.1880e-02,  5.2001e-02, -5.1980e-02,  5.1841e-02,\n",
      "         -5.2222e-02, -5.2236e-02,  2.3707e-03, -5.2313e-02, -4.7796e-02,\n",
      "         -1.4595e-09, -5.2207e-02, -4.8142e-02,  5.1879e-02, -5.0231e-02,\n",
      "         -1.3707e-02, -2.8526e-02,  5.1086e-02, -5.0053e-02,  4.8681e-02,\n",
      "         -4.8542e-02, -5.2182e-02, -5.2000e-02,  4.9353e-02,  5.2397e-02,\n",
      "          3.3250e-02,  5.2388e-02,  4.4256e-07,  5.1754e-02,  5.2141e-02,\n",
      "          1.6336e-07, -6.0236e-08,  1.2014e-08, -5.2178e-02,  5.2182e-02,\n",
      "          6.0924e-03,  5.1834e-02,  5.2149e-02, -4.7869e-02, -1.0405e-03,\n",
      "         -5.0043e-02,  2.9561e-04,  1.8722e-02,  5.2272e-02, -5.7275e-07,\n",
      "         -5.0296e-02,  4.9206e-02, -1.5908e-05, -5.0136e-02, -5.2181e-02,\n",
      "         -1.3778e-11, -4.5699e-02, -5.2113e-02, -5.2180e-02,  5.1584e-02,\n",
      "         -1.6068e-07,  5.2232e-02,  6.4418e-07,  4.9400e-02,  1.0866e-08,\n",
      "          5.0808e-02, -4.9951e-02, -5.2054e-02, -2.0223e-02, -5.2238e-02,\n",
      "         -3.0222e-10,  1.4460e-03, -8.8237e-04,  5.0517e-02, -5.1610e-02,\n",
      "         -1.1418e-03, -5.1560e-02, -5.2283e-02,  4.9263e-02, -5.1519e-02,\n",
      "          5.1768e-02, -5.2358e-02, -5.2108e-02, -1.9421e-09,  5.2178e-02,\n",
      "          6.9547e-06,  4.9425e-02,  5.2291e-02,  4.9866e-02,  5.2127e-02,\n",
      "         -2.3692e-02,  5.2335e-02, -4.4781e-02,  4.8632e-02,  5.1290e-02,\n",
      "         -5.0250e-02, -5.2385e-02,  4.8205e-02, -5.1755e-02,  5.2011e-02,\n",
      "         -3.4157e-08,  4.4141e-02,  5.5017e-03,  5.0029e-02,  5.2040e-02,\n",
      "         -4.8223e-08, -5.2263e-02,  5.2103e-02, -1.6256e-09, -4.7759e-02,\n",
      "          1.8490e-02, -5.2320e-02, -5.1904e-02,  5.1979e-02, -5.2106e-02,\n",
      "          4.9900e-02, -4.3006e-02, -6.3382e-11,  5.0643e-02, -5.2261e-02,\n",
      "         -5.2040e-02, -5.0452e-02,  4.9928e-02, -5.2014e-02,  4.8349e-02,\n",
      "         -5.1923e-02, -4.9254e-02, -5.1860e-02, -5.1535e-02, -5.2150e-02,\n",
      "          5.1916e-02,  5.2176e-02, -5.2249e-02, -5.2123e-02, -6.9869e-08,\n",
      "          3.9179e-02,  5.1343e-02, -5.2015e-02, -5.1404e-02,  5.1646e-02,\n",
      "          4.9982e-02,  5.2042e-02, -3.8130e-02,  5.2232e-02, -4.4855e-05,\n",
      "          2.3473e-07,  5.1953e-02, -4.9864e-02,  5.2033e-02, -4.8720e-02,\n",
      "         -6.8429e-06, -2.8142e-02, -5.2311e-02,  4.9069e-02, -2.8382e-02,\n",
      "         -5.0378e-02, -5.2202e-02, -5.1985e-02, -5.2261e-02, -5.2314e-02,\n",
      "         -5.1035e-02,  6.3174e-09, -4.8980e-02,  5.2083e-02,  5.2059e-02,\n",
      "         -5.2187e-02,  2.6104e-03,  5.0973e-02, -5.0167e-02, -5.2392e-02,\n",
      "         -5.0592e-02, -5.0207e-02, -5.1267e-02, -5.1962e-02,  2.1662e-02,\n",
      "         -5.1827e-02,  2.3784e-03, -5.1352e-02,  2.7748e-02, -5.1835e-02,\n",
      "         -5.1839e-02, -5.2418e-02, -5.2465e-04, -4.8450e-02, -4.8954e-02,\n",
      "          5.2188e-02,  5.4739e-04,  5.3145e-05,  5.2402e-02, -5.1885e-02,\n",
      "          5.1902e-02,  5.2240e-02, -4.7578e-02, -1.1315e-07,  5.2236e-02,\n",
      "          4.7103e-02, -5.2180e-02, -4.9679e-02, -5.0414e-02, -5.0136e-02,\n",
      "         -4.7855e-09,  5.2061e-02, -4.4470e-02,  5.1743e-02, -5.2336e-02,\n",
      "          5.1401e-02, -4.9286e-02, -5.2242e-02, -5.2238e-02, -4.6985e-02,\n",
      "          5.2303e-02,  4.9971e-02, -5.0374e-02, -5.2189e-02, -5.7150e-06,\n",
      "          5.2311e-02,  5.3886e-03,  5.0436e-02, -2.1451e-02,  5.0688e-02,\n",
      "          5.0848e-02, -2.4602e-02,  5.2068e-02,  5.2070e-02,  1.8908e-06,\n",
      "          5.1888e-02,  5.1887e-02,  4.8151e-02,  5.1630e-02, -4.9044e-02,\n",
      "          5.2036e-02,  5.1992e-02, -5.1682e-02, -5.0201e-02,  5.2293e-02,\n",
      "         -5.2117e-02, -1.3156e-07,  8.0050e-05, -5.2130e-02, -5.1267e-07,\n",
      "         -4.1110e-02, -4.6066e-02,  3.3669e-07,  5.1395e-02, -5.2212e-02,\n",
      "          5.2380e-02, -5.2319e-02, -5.1926e-02,  5.1582e-02,  4.5316e-03,\n",
      "         -2.9152e-08, -5.2167e-02, -5.2315e-02,  5.2394e-02, -4.8791e-02,\n",
      "          5.2119e-02,  5.1524e-02,  5.1949e-02,  2.0806e-02,  4.9906e-02,\n",
      "          5.2172e-02,  4.9965e-02, -5.0935e-02, -3.9298e-08,  5.1938e-02,\n",
      "         -5.2076e-02, -6.6628e-05, -3.3068e-03,  4.9755e-02, -5.1389e-02,\n",
      "          5.1613e-02, -7.0744e-08,  5.2052e-02,  2.8562e-02,  5.2248e-02,\n",
      "          5.2346e-02, -4.8131e-02, -4.8527e-02, -2.8616e-02, -5.2362e-02,\n",
      "         -5.2201e-02, -5.1051e-02, -2.8832e-02, -5.1700e-02,  4.8987e-02,\n",
      "          5.2092e-02, -5.1571e-02, -4.7843e-02,  5.2114e-02,  1.9798e-02,\n",
      "          4.6523e-02, -5.1703e-02,  5.0139e-02, -5.2121e-02, -1.7916e-08,\n",
      "          5.2379e-02,  5.2009e-02,  5.2240e-02,  5.2183e-02,  5.1832e-02,\n",
      "         -4.6431e-03,  4.8422e-02,  5.2192e-02, -5.2379e-02, -5.2213e-02,\n",
      "         -5.0993e-02,  5.2171e-02,  5.2242e-02, -4.8512e-02, -4.7381e-02,\n",
      "          5.3400e-03, -5.2270e-02, -5.2203e-02,  5.2138e-02, -5.1466e-02,\n",
      "          4.8097e-02, -5.1852e-02, -6.8629e-07, -4.8726e-04, -3.7508e-03,\n",
      "          5.2180e-02, -4.8033e-02, -5.0417e-02, -5.2129e-02, -5.1752e-02,\n",
      "          1.8768e-03,  5.1687e-02,  5.0006e-02, -5.1901e-02, -5.2234e-02,\n",
      "         -2.5439e-02, -3.0126e-02, -5.2152e-02, -5.1990e-02,  5.2249e-02,\n",
      "         -5.1986e-02,  4.9817e-02, -4.7453e-02,  5.2305e-02,  5.0052e-02,\n",
      "          5.2003e-02, -5.2019e-02,  5.1583e-02, -2.7652e-07,  5.2328e-02,\n",
      "          5.0052e-02,  5.2216e-02, -5.2110e-02, -5.1988e-02,  5.2308e-02,\n",
      "         -2.3437e-09,  5.0584e-02,  5.2119e-02,  1.4644e-04,  3.1303e-02,\n",
      "          2.5496e-02, -5.0474e-02,  3.0375e-02,  5.1701e-02,  5.2103e-02,\n",
      "          4.5168e-02, -5.1963e-02, -5.1749e-02, -5.2219e-02, -4.8831e-02,\n",
      "         -5.2288e-02,  3.1125e-02, -5.1741e-02, -4.7361e-02, -7.4139e-05,\n",
      "         -5.2260e-02,  4.9979e-02,  5.2117e-02,  1.0642e-08,  5.1908e-02,\n",
      "         -5.2255e-02,  2.6658e-09,  5.1931e-02, -5.2248e-02, -5.0185e-02,\n",
      "          3.1796e-07, -5.2305e-02,  1.9581e-05, -5.2096e-02, -5.2009e-02,\n",
      "         -5.1665e-02, -1.6273e-04, -5.1799e-02, -5.2307e-02,  4.2231e-02,\n",
      "         -5.2206e-02,  5.2271e-02,  6.6896e-08,  5.2221e-02,  2.0972e-05,\n",
      "          4.8501e-02,  5.2065e-02, -5.1593e-02,  5.2291e-02,  5.1721e-02,\n",
      "          5.2163e-02, -9.0635e-08,  5.2187e-02, -5.1939e-02,  4.2572e-02,\n",
      "          4.3018e-02, -1.5656e-07, -5.1792e-02,  5.2206e-02,  4.8952e-02,\n",
      "         -5.0128e-02, -5.1476e-02,  5.1007e-02,  4.8059e-03, -5.2203e-02,\n",
      "          5.2249e-02, -5.0321e-02, -4.6010e-02,  5.2203e-02, -4.4030e-02,\n",
      "         -5.0342e-02, -1.2943e-03,  5.2375e-02,  5.1346e-02, -2.8085e-02,\n",
      "         -5.2291e-02,  5.0599e-02, -5.0739e-02, -5.1421e-02,  5.2353e-02,\n",
      "          5.2124e-02, -3.4358e-07,  6.1284e-03,  5.2248e-02, -4.7892e-02,\n",
      "          5.3209e-03, -5.2023e-02, -4.6726e-02, -2.4918e-02, -5.2324e-02,\n",
      "          4.9987e-02,  7.0507e-04, -4.2092e-02, -5.1357e-02,  9.3602e-05,\n",
      "          5.2045e-02, -5.3318e-03, -4.9152e-02, -4.2365e-02,  5.2318e-02,\n",
      "         -5.0228e-02, -4.5749e-02, -5.0081e-02,  5.2039e-02, -5.2110e-02,\n",
      "          4.9531e-02, -6.9207e-03, -5.0442e-02,  5.2331e-02,  5.2178e-02,\n",
      "          5.2229e-02, -1.1620e-04, -5.2206e-02,  5.2362e-02, -5.2042e-02,\n",
      "         -3.9598e-03,  3.9416e-02, -4.8634e-02,  5.2084e-02, -1.2510e-08,\n",
      "         -5.2130e-02, -4.8474e-02,  5.2343e-02, -2.3564e-05,  5.2131e-02,\n",
      "         -2.1476e-04, -4.7114e-02, -5.2217e-02, -4.9994e-02,  5.2226e-02,\n",
      "         -8.7305e-04, -5.0096e-02,  4.8431e-02,  5.1402e-02,  5.0629e-02,\n",
      "          5.2223e-02, -4.2565e-09, -5.9351e-07,  5.2218e-02,  5.2176e-02,\n",
      "         -4.8525e-02, -5.0036e-02, -5.1960e-02,  4.9773e-02, -5.1752e-02,\n",
      "          4.6928e-02, -5.1928e-02,  5.1965e-02,  5.0033e-02,  5.1947e-02,\n",
      "         -5.1782e-02,  5.2200e-02,  5.2092e-02,  5.2259e-02, -5.2116e-02,\n",
      "         -5.1136e-02, -1.5231e-06,  2.4446e-03, -5.1183e-02, -5.1490e-02,\n",
      "         -5.2229e-02,  5.2223e-02,  1.1466e-08,  5.1573e-02, -4.5974e-02,\n",
      "          5.0977e-02, -2.9654e-04, -5.1828e-02,  4.9975e-02,  5.2265e-02,\n",
      "          5.2344e-02, -4.8364e-02,  2.6059e-02,  5.2346e-02, -3.6188e-07,\n",
      "          5.1886e-02,  5.0166e-02, -5.0899e-02, -5.1939e-02,  5.2373e-02,\n",
      "          5.0692e-02, -5.1906e-02,  5.0471e-02, -5.0045e-02, -4.8198e-02,\n",
      "         -5.2276e-02,  7.7706e-09,  4.1875e-08, -5.2146e-02, -4.9901e-02,\n",
      "          5.2124e-02,  5.2189e-02,  5.2283e-02,  4.8083e-02,  5.1331e-02,\n",
      "         -5.1953e-02,  2.0977e-09, -4.5605e-06, -5.2422e-02,  5.2373e-02,\n",
      "         -5.2020e-02,  5.1051e-02, -5.2272e-02,  5.2006e-02,  5.2024e-02,\n",
      "          5.1938e-02, -1.1569e-03,  2.1187e-03,  2.9965e-05,  4.8384e-02,\n",
      "         -5.1949e-02, -1.8741e-06, -4.8489e-02,  4.1309e-04, -5.1896e-02,\n",
      "          3.2408e-02, -4.6227e-03,  5.2311e-02,  5.1971e-02,  5.2032e-02,\n",
      "          3.0988e-02, -3.5253e-09, -5.1977e-02,  7.1034e-08, -5.6551e-03,\n",
      "          4.9905e-02, -5.2278e-02, -5.1925e-02,  2.9422e-06, -5.2164e-02,\n",
      "          5.2353e-02,  4.8637e-02,  5.2082e-02,  5.2244e-02, -2.4478e-03,\n",
      "         -5.2095e-02,  5.2057e-02,  4.4435e-02, -5.2024e-02, -5.1672e-02,\n",
      "          4.6957e-02, -2.5448e-05, -5.1504e-02,  5.1468e-02, -5.1665e-02,\n",
      "          4.9679e-02,  3.5198e-05, -1.9774e-02,  9.7423e-04, -5.1812e-02,\n",
      "          5.2291e-02,  5.2069e-02,  5.1985e-02,  5.2109e-02,  7.0081e-05,\n",
      "         -5.1510e-02,  5.1316e-02,  4.8341e-02,  5.2096e-02, -5.2307e-02,\n",
      "         -5.2172e-02, -5.1050e-02, -5.2085e-02,  5.1903e-02, -5.1852e-02,\n",
      "          5.2138e-02,  4.9349e-02, -5.1054e-02,  3.2276e-06, -2.0101e-09,\n",
      "         -3.4775e-05, -5.1836e-02, -5.1855e-02,  5.2181e-02,  3.0696e-02,\n",
      "         -5.1507e-02,  5.1964e-02,  5.2263e-02,  3.1832e-02,  5.1762e-02,\n",
      "         -8.9082e-08,  5.1796e-02,  5.1762e-02, -2.5899e-02, -4.8290e-02,\n",
      "         -5.1927e-02, -5.1818e-02, -5.0630e-02,  2.0176e-07,  5.1671e-02,\n",
      "         -4.3526e-02, -5.1395e-02, -5.2074e-02,  5.1975e-02,  3.1673e-07,\n",
      "          4.8535e-02,  5.2262e-02,  5.2098e-02, -5.1456e-02,  5.2305e-02,\n",
      "          5.2335e-02,  4.9372e-02, -5.0130e-02, -5.0112e-02, -5.1701e-02,\n",
      "          4.9013e-02,  5.2036e-02,  5.1743e-02, -2.3519e-02, -5.1916e-02,\n",
      "          3.1657e-06, -5.2357e-02,  6.7319e-07, -4.8604e-02,  5.2175e-02,\n",
      "         -5.2047e-02,  5.2208e-02, -4.5414e-02, -4.9902e-02,  4.9918e-02,\n",
      "         -5.1049e-02, -4.8481e-02,  5.1738e-02, -4.8747e-02, -4.1193e-03,\n",
      "          5.1985e-02, -5.2070e-02,  2.9951e-03, -5.1998e-02,  4.5732e-03,\n",
      "          5.2158e-02,  4.7472e-02,  5.2239e-02,  5.2061e-02,  5.0840e-03,\n",
      "         -5.1196e-02, -5.1760e-02,  5.7938e-05, -4.9962e-02, -5.2346e-02,\n",
      "         -5.1997e-02, -5.2086e-02,  5.2150e-02,  5.2265e-02,  2.1374e-03,\n",
      "          1.9009e-09, -5.2129e-02,  5.2145e-02]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0723e-06, -1.7554e-07,  1.5229e-07, -2.1967e-08, -2.2484e-06,\n",
      "          3.2141e-06,  6.0833e-06, -5.4010e-09, -1.9050e-07, -7.8322e-07,\n",
      "         -1.5750e-07,  3.6265e-06, -3.8516e-08, -1.0081e-05, -1.4026e-08,\n",
      "         -1.8135e-07, -2.7653e-06,  9.1704e-09, -2.7459e-06, -5.0790e-09,\n",
      "          9.4938e-07,  4.1226e-09,  7.3300e-08, -1.0659e-08,  6.3456e-07,\n",
      "          3.1712e-06, -2.6293e-06, -1.0807e-09,  2.9015e-06,  4.4993e-06,\n",
      "         -9.2933e-06,  1.0645e-06, -4.3114e-07,  2.2734e-08, -9.7144e-07,\n",
      "          1.0887e-06,  6.2589e-08, -7.6105e-08, -2.9577e-09,  2.1593e-06,\n",
      "          7.3475e-08,  8.4675e-06,  3.4534e-08, -2.6103e-07, -4.6536e-06,\n",
      "          1.3890e-08,  3.9890e-08, -4.9402e-06, -2.0919e-06, -2.7671e-06,\n",
      "          1.0942e-07, -4.3214e-06, -3.1810e-08, -1.3133e-09, -2.3513e-06,\n",
      "         -1.7040e-09, -9.4722e-06,  2.6569e-06,  6.1225e-07, -5.3678e-08,\n",
      "          1.0648e-05, -2.6208e-07, -3.8011e-07,  1.9068e-08, -7.3565e-08,\n",
      "         -4.4702e-06,  4.7450e-10,  8.2008e-07, -1.0050e-08,  4.5204e-08,\n",
      "         -5.1802e-09, -4.5415e-07, -3.0031e-08,  7.7630e-07,  2.2411e-07,\n",
      "         -3.2618e-07,  2.4547e-09,  2.5815e-08, -1.8262e-09,  1.8196e-07,\n",
      "         -1.8963e-05,  3.3366e-10, -8.6666e-08, -2.1401e-06, -1.7300e-06,\n",
      "          1.6550e-08, -2.3359e-06,  1.8895e-07,  6.3381e-06, -3.2458e-07,\n",
      "          5.9980e-07,  8.5155e-10, -8.2974e-07,  3.7210e-06,  1.0432e-06,\n",
      "         -9.0340e-09, -2.6983e-08, -8.9112e-10, -1.1647e-08, -7.3879e-09,\n",
      "         -9.9330e-06,  4.5480e-07, -5.4927e-06,  4.2634e-06,  3.0792e-06,\n",
      "          4.8907e-06,  4.8906e-07,  1.9267e-05,  2.1950e-08, -1.8796e-08,\n",
      "          2.0869e-06,  2.9192e-06,  3.1749e-07, -7.7507e-08,  1.3225e-07,\n",
      "          5.8028e-06, -3.7069e-05, -3.0038e-07,  4.7274e-08, -3.7108e-07,\n",
      "         -5.0080e-06,  3.3471e-08, -4.8558e-06,  5.2316e-07,  3.9797e-06,\n",
      "          6.6093e-06,  1.0735e-06,  2.3619e-08,  1.2651e-07,  1.5612e-07,\n",
      "          1.2897e-05, -8.7070e-07, -2.0266e-07,  7.9312e-07,  1.9808e-05,\n",
      "          9.1687e-08, -1.8321e-07, -4.1519e-05, -8.2110e-06,  4.8526e-08,\n",
      "         -1.7259e-05,  3.3687e-06, -4.5652e-07,  3.0126e-06,  1.6767e-09,\n",
      "          4.7585e-08,  7.9376e-07, -5.0571e-09,  2.4012e-07, -4.3470e-06,\n",
      "          2.7637e-06,  2.7809e-07, -2.0349e-07,  1.6128e-08, -9.3054e-06,\n",
      "         -4.1585e-06, -1.0164e-06, -4.2375e-06,  9.7744e-06,  3.5280e-08,\n",
      "          4.2501e-06,  3.6184e-07, -7.5656e-08,  1.3486e-06, -9.1839e-06,\n",
      "          1.5882e-06, -5.5629e-08,  1.2909e-10, -6.2697e-06,  1.1519e-07,\n",
      "          1.3249e-07,  1.1594e-08,  1.0702e-07, -1.7704e-10,  1.3480e-06,\n",
      "          7.6663e-07,  3.0214e-06,  3.9514e-06,  1.8225e-07, -8.6592e-09,\n",
      "         -5.3212e-08,  1.3924e-06, -3.9184e-07, -5.9995e-06,  1.8678e-06,\n",
      "          2.4382e+00,  2.7470e-05,  1.7481e-07, -1.0066e-06,  1.1384e-06,\n",
      "          3.5771e-06,  1.6830e-08, -3.2149e-08,  1.3233e-06,  1.0347e-07,\n",
      "         -9.7003e-06, -3.4758e-06,  5.2135e-07,  9.9343e-09, -1.0689e-06,\n",
      "          1.1459e-07, -1.0104e-06, -1.9741e-06,  1.3659e-07,  2.0566e-09,\n",
      "          1.8341e-05,  2.3386e-09,  1.5413e-06, -1.3614e-05, -1.3377e-08,\n",
      "         -1.1260e-05,  6.5263e-06,  3.0834e-06,  2.0637e-09,  5.9669e-08,\n",
      "         -3.8255e-06, -2.6440e-07,  4.1761e-09, -1.6489e-09, -4.5722e-07,\n",
      "          1.5958e-09, -3.6105e-09,  8.5279e-07, -3.3948e-08,  1.1833e-05,\n",
      "         -3.8564e-08, -7.7325e-08, -1.2405e-07, -8.0368e-07,  3.5841e-07,\n",
      "         -1.1580e-06, -3.2381e-05,  5.1660e-08, -3.2447e-07, -1.0490e-08,\n",
      "         -4.8042e-06, -2.8876e-07,  2.0616e-06,  1.1622e-07,  1.9249e-07,\n",
      "         -3.8200e-08,  1.9088e-06,  3.3675e-07,  6.7106e-09, -3.7893e-08,\n",
      "          2.0501e-05,  4.8386e-07,  6.9481e-08, -3.0775e-07,  2.5659e-09,\n",
      "          1.1468e-07, -9.3127e-09,  7.0764e-08, -3.8759e-07, -1.8538e-07,\n",
      "         -2.3776e-06, -1.7561e-08,  1.2938e-08,  1.7631e-07, -1.6005e-08,\n",
      "          1.3190e-06, -1.8780e-06,  2.6531e-06,  3.2918e-07,  2.3177e-09,\n",
      "          9.0369e-09, -7.8687e-07,  5.2690e-06,  7.8105e-06, -4.3678e-08,\n",
      "         -6.3880e-09, -2.5165e-05,  2.9724e-08, -3.2173e-08, -3.1059e-06,\n",
      "         -6.6636e-08, -1.1658e-05, -9.0420e-10, -1.2132e-06,  1.5681e-06,\n",
      "          1.2708e-05,  6.3789e-08,  2.4808e-07,  6.8361e-08, -1.3197e-06,\n",
      "          1.6300e-08,  1.1385e-07, -6.5471e-06, -3.8585e-09,  6.2307e-07,\n",
      "          2.1949e-07, -3.5512e-06,  7.8617e-06,  2.3448e-08, -1.6660e-08,\n",
      "         -1.6433e-08,  1.7339e-08,  4.2444e-06,  3.3811e-07, -7.9330e-07,\n",
      "          1.5180e-07, -2.0710e-06,  6.5899e-07, -1.1119e-06,  1.2215e-06,\n",
      "          8.6579e-07, -2.0877e-08, -2.4562e-07,  1.7908e-06, -1.7872e-06,\n",
      "         -8.2790e-07, -6.9340e-07,  5.9892e-06, -4.0061e-09,  1.3201e-06,\n",
      "         -3.6311e-09, -1.3464e-06,  1.5923e-07, -4.1587e-09,  1.3169e-06,\n",
      "         -1.9885e-07,  1.8544e-07, -1.6652e-07, -1.1913e-09,  1.2543e-05,\n",
      "          3.0118e-06, -6.8633e-08, -2.3187e-09,  4.6512e-07,  7.7693e-07,\n",
      "          9.1277e-08, -1.0001e-06, -6.5130e-07, -2.7530e-06,  6.2069e-07,\n",
      "          2.1020e-06, -2.5445e-07, -4.6137e-08, -8.3698e-07,  6.8474e-08,\n",
      "         -1.7602e-06, -2.1282e-05, -9.8710e-07,  5.4287e-07, -1.6907e-08,\n",
      "         -4.9593e-09,  6.9803e-06, -2.1279e-08,  7.3531e-07,  1.0554e-08,\n",
      "         -5.0881e-08,  2.0157e-06,  2.3291e-06,  2.4314e-06, -1.7396e-08,\n",
      "         -6.6116e-09,  1.6800e-05, -2.3752e-06,  1.1090e-08,  9.4186e-07,\n",
      "         -4.6295e-09, -4.5012e-09, -1.1027e-08, -1.7150e-08, -5.3582e-07,\n",
      "          3.9801e-08,  6.3354e-09, -3.9512e-06,  1.4280e-05,  1.4428e-09,\n",
      "         -4.7703e-06,  1.8668e-06, -2.3177e-06,  4.7129e-07,  5.5295e-07,\n",
      "         -6.5444e-07,  1.9001e-07,  2.8564e-09,  4.2131e-09,  3.1700e-08,\n",
      "         -5.6144e-07, -3.3728e-07, -4.8253e-06, -1.2011e-07, -1.2578e-05,\n",
      "         -1.0286e-08,  5.6515e-06, -3.3863e-07, -1.4199e-07, -1.1463e-06,\n",
      "          1.4947e-05, -9.9756e-06,  4.2099e-08, -4.5970e-07, -1.9721e-06,\n",
      "         -1.4928e-08,  6.7723e-08, -3.3629e-08,  1.7256e-06, -4.4908e-06,\n",
      "          7.6818e-06, -9.2188e-04, -1.0623e-07, -3.6236e-07,  2.1087e-07,\n",
      "          2.2562e-07,  1.1825e-07,  2.1053e-09, -1.2763e-08, -6.2645e-06,\n",
      "         -8.6400e-06, -6.6074e-09,  2.3928e-06,  1.0341e-06, -9.1274e-08,\n",
      "          8.0273e-07,  5.8029e-08,  3.8576e-07, -2.4575e-07, -2.8638e-06,\n",
      "          1.9994e-07,  8.7311e-09,  6.0282e-07,  9.6328e-06,  6.2586e-07,\n",
      "         -9.2212e-08,  4.0201e-06,  5.0067e-08, -2.9323e-09, -7.3965e-09,\n",
      "         -1.3177e-06, -5.8452e-07, -1.6813e-06,  2.0465e-06,  1.2437e-05,\n",
      "          2.9860e-09,  5.9804e-08,  5.3445e-08, -1.7165e-06,  5.6213e-08,\n",
      "         -7.1813e-06,  5.0317e-06,  1.3395e-06, -4.1941e-07, -2.4110e-08,\n",
      "         -1.1952e-06, -3.2470e-08, -1.3269e-07, -8.6561e-08, -3.4704e-07,\n",
      "         -4.7496e-07,  3.0318e-06, -1.9495e-06,  1.6571e-07, -1.4059e-07,\n",
      "         -3.2401e-06, -1.5929e-06,  1.2848e-06, -5.1812e-06, -5.9173e-09,\n",
      "          8.0958e-09, -2.5112e-07,  6.3120e-08, -7.0843e-06,  6.2890e-06,\n",
      "          1.5063e-06, -1.3331e-08, -6.1629e-07, -1.0127e-06,  2.2348e-06,\n",
      "         -9.7075e-09,  5.5668e-06,  1.3291e-06, -1.3370e-05, -1.6887e-08,\n",
      "          5.2129e-07, -1.0116e-08, -3.6407e-07, -6.9511e-07,  1.4989e-06,\n",
      "         -1.1162e-08, -8.8722e-08,  3.1746e-06, -3.2039e-08,  7.3414e-08,\n",
      "          3.3959e-08,  7.3257e-06,  6.1773e-07, -2.0627e-07, -3.1065e-08,\n",
      "         -1.4231e-06,  8.8517e-08, -4.0479e-06, -1.8854e-05,  2.0245e-07,\n",
      "          6.6571e-08,  2.4725e-07, -4.9766e-09, -3.6539e-07,  5.5708e-09,\n",
      "         -6.8911e-08,  4.7601e-07, -1.4619e-09, -2.3620e-08, -6.8413e-06,\n",
      "          6.5422e-10, -2.3541e-07, -1.1248e-06, -2.4799e-06,  2.6442e-09,\n",
      "          4.7853e-08, -9.3691e-06,  5.2711e-06,  3.2681e-07, -4.6305e-09,\n",
      "         -1.5413e-08, -6.0654e-10,  1.9829e-06,  2.1303e-08, -2.4998e-09,\n",
      "          9.6417e-06, -2.9368e-06,  6.9716e-08, -1.6374e-06, -7.8645e-06,\n",
      "          4.2350e-07, -9.8586e-08, -4.0901e-07,  4.8835e-07, -6.1373e-09,\n",
      "          5.2185e-09,  1.1267e-06, -1.6042e-08,  8.3461e-08, -2.0667e-08,\n",
      "          3.1868e-06,  4.5913e-06,  5.6506e-07, -1.1695e-08,  3.1359e-06,\n",
      "          7.2843e-07,  5.3213e-07,  2.5098e-06,  9.0408e-08,  3.4283e-07,\n",
      "          5.9762e-07, -6.7579e-08, -7.1828e-07, -1.2225e-07, -1.9978e-05,\n",
      "          4.1750e-09,  6.7184e-07,  1.4025e-07, -1.2995e-08,  7.7824e-09,\n",
      "          1.9227e-06,  5.3590e-07,  1.8780e-07, -3.6603e-08, -6.5485e-07,\n",
      "          5.4650e-09, -2.8819e-07, -1.0265e-05, -1.2783e-05, -2.7736e-09,\n",
      "          4.3043e-08,  9.2450e-08,  1.1073e-06,  5.1688e-07,  1.0803e-08,\n",
      "         -7.5087e-07, -1.2425e-08, -1.7324e-07,  3.1914e-06,  3.0812e-08,\n",
      "          8.2454e-07,  1.4988e-06,  3.3312e-07,  1.9099e-06, -2.6439e-05,\n",
      "         -4.0890e-06,  3.2042e-06, -1.3003e-09, -1.7933e-08, -9.6058e-06,\n",
      "          4.1744e-06,  9.4759e-08,  7.0423e-09, -5.8366e-08,  4.0765e-06,\n",
      "          9.3381e-08,  1.4183e-08, -8.8225e-09,  2.0243e-08, -2.3166e-07,\n",
      "          2.9122e-06,  5.9431e-09, -6.5809e-07, -5.8936e-07, -3.4326e-08,\n",
      "          1.4523e-06,  1.7069e-08, -5.9587e-08, -1.2063e-06, -1.1997e-07,\n",
      "         -2.2054e-06, -1.6689e-06,  2.1246e-07,  6.2270e-08,  1.6191e-06,\n",
      "         -1.4131e-05,  1.2284e-05,  1.9956e-09, -1.1322e-09,  1.4154e-07,\n",
      "         -4.2383e-09, -7.4691e-06, -5.8829e-06, -4.7434e-06,  1.6903e-08,\n",
      "         -4.7152e-08,  3.3382e-09, -1.2117e-08, -1.7598e-08, -9.4448e-07,\n",
      "         -1.2268e-06, -4.0027e-08, -1.1813e-07, -6.1725e-06,  7.0416e-08,\n",
      "          6.7489e-08, -1.7169e-06, -2.6514e-07,  2.7427e-06,  2.2997e-09,\n",
      "          7.4122e-08, -6.9420e-10,  3.1030e-07,  2.8974e-08,  4.6630e-09,\n",
      "          1.9685e-07, -1.7612e-09,  1.2932e-07,  2.4889e-09, -6.4275e-08,\n",
      "          1.3948e-07, -1.6682e-07,  4.6279e-07,  1.2840e-07, -5.1384e-06,\n",
      "          1.0605e-07, -1.8766e-06,  8.7417e-10,  2.5751e-07, -3.5626e-06,\n",
      "          1.6775e-05,  3.0148e-06, -4.4158e-07, -9.3211e-08, -1.4789e-05,\n",
      "          1.2645e-07, -1.3182e-05, -1.0014e-06,  2.0630e-05,  4.9534e-07,\n",
      "          6.1195e-08,  5.1913e-06,  1.2311e-05, -5.5544e-07, -6.9333e-07,\n",
      "          3.7712e-06,  5.2931e-09, -2.3973e-08, -7.8219e-08, -8.3641e-09,\n",
      "          3.7597e-08,  1.3729e-07,  5.2355e-08, -3.9222e-06, -1.7822e-06,\n",
      "         -1.6950e-07,  1.1340e-07, -1.3477e-06, -4.8292e-06,  1.4061e-07,\n",
      "         -4.3786e-07, -4.1365e-08,  7.2878e-08, -8.9773e-08, -4.6045e-09,\n",
      "         -7.4258e-07, -1.5908e-05,  3.6720e-06,  8.8979e-07,  5.6846e-08,\n",
      "          1.7180e-09,  2.5191e-06, -4.5061e-08,  5.2641e-08,  5.4993e-08,\n",
      "          4.9846e-08,  2.1266e-06,  3.1196e-07,  9.0261e-08, -2.8499e-07,\n",
      "         -1.0393e-07, -1.7390e-06,  2.3264e-06, -3.0165e-06,  2.8547e-07,\n",
      "         -4.5362e-07, -6.7905e-06, -3.2309e-07, -2.3055e-08, -2.5481e-07,\n",
      "         -2.1910e-09,  1.7808e-06,  6.7258e-08, -5.6061e-07, -1.1196e-07,\n",
      "          1.0037e-07,  6.1459e-07, -1.1338e-08, -2.1081e-06,  2.4831e-08,\n",
      "          4.2150e-06, -6.9252e-09, -8.9839e-08, -7.0023e-08,  3.2632e-09,\n",
      "          2.1297e-06,  8.3259e-08, -2.1960e-06,  5.5825e-08,  2.2851e-08,\n",
      "         -5.2894e-09,  5.7358e-06, -2.1927e-07,  8.8418e-09, -2.2206e-06,\n",
      "         -1.5648e-07, -6.7006e-09,  1.3674e-07,  1.9016e-07,  1.8238e-07,\n",
      "         -6.0125e-06, -4.1107e-08, -9.2180e-06, -1.2954e-07, -2.9015e-06,\n",
      "          4.0419e-06,  4.3022e-06,  1.2596e-07,  5.3046e-08, -2.4119e-05,\n",
      "         -1.1273e-06, -1.0762e-08, -5.2263e-07,  2.3763e-08, -6.2570e-06,\n",
      "          7.2738e-07, -2.0995e-05,  2.9127e-06,  6.8859e-08,  4.8712e-07,\n",
      "         -8.0636e-07,  3.0683e-07, -3.7270e-06]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 3, 13],\n",
      "         [ 6, 16],\n",
      "         [ 2, 15],\n",
      "         [ 2,  7],\n",
      "         [ 7, 10],\n",
      "         [ 9,  1],\n",
      "         [ 3, 15],\n",
      "         [ 2, 16],\n",
      "         [ 1, 10],\n",
      "         [ 0, 13],\n",
      "         [ 6,  3],\n",
      "         [ 7,  8],\n",
      "         [ 3,  9],\n",
      "         [ 5, 13],\n",
      "         [ 1,  6],\n",
      "         [ 5, 11],\n",
      "         [ 6,  0],\n",
      "         [ 5,  2],\n",
      "         [ 2, 12],\n",
      "         [ 4,  6],\n",
      "         [ 8, 15],\n",
      "         [10,  0],\n",
      "         [ 3,  1],\n",
      "         [ 2,  4],\n",
      "         [ 7,  0],\n",
      "         [ 5, 17],\n",
      "         [ 6, 11],\n",
      "         [ 1,  7],\n",
      "         [ 3, 11],\n",
      "         [ 7,  9],\n",
      "         [ 8,  4],\n",
      "         [ 6, 17],\n",
      "         [10, 15],\n",
      "         [ 3, 12],\n",
      "         [10,  2],\n",
      "         [ 6,  7],\n",
      "         [ 1, 13],\n",
      "         [ 8,  0],\n",
      "         [ 8, 10],\n",
      "         [ 3,  6],\n",
      "         [ 4,  7],\n",
      "         [ 6,  8],\n",
      "         [ 9,  8],\n",
      "         [ 0,  9],\n",
      "         [ 9, 11],\n",
      "         [10, 11],\n",
      "         [ 0,  6],\n",
      "         [ 4, 10],\n",
      "         [ 8, 16],\n",
      "         [ 0, 12],\n",
      "         [ 7,  2],\n",
      "         [ 0, 10],\n",
      "         [ 4,  5],\n",
      "         [ 1,  2],\n",
      "         [ 1, 14],\n",
      "         [ 4, 15],\n",
      "         [10,  8],\n",
      "         [ 5,  0],\n",
      "         [10, 18],\n",
      "         [ 8, 12],\n",
      "         [ 1,  1],\n",
      "         [ 4, 17],\n",
      "         [ 9,  9],\n",
      "         [ 0, 11],\n",
      "         [ 1, 15],\n",
      "         [ 0,  8],\n",
      "         [ 5,  9],\n",
      "         [10,  9],\n",
      "         [ 4, 12],\n",
      "         [ 6, 10],\n",
      "         [ 4, 11],\n",
      "         [ 1,  8],\n",
      "         [ 1,  3],\n",
      "         [ 5,  7],\n",
      "         [ 8,  7],\n",
      "         [ 8,  5],\n",
      "         [ 4,  9],\n",
      "         [ 9, 16],\n",
      "         [ 8, 14],\n",
      "         [ 5, 10],\n",
      "         [ 0,  0],\n",
      "         [ 6, 13],\n",
      "         [ 9, 18],\n",
      "         [ 9, 13],\n",
      "         [10,  1],\n",
      "         [ 5,  5],\n",
      "         [ 3, 17],\n",
      "         [ 2, 17],\n",
      "         [ 1, 17],\n",
      "         [ 6,  5],\n",
      "         [ 0,  3],\n",
      "         [ 4,  2],\n",
      "         [ 0, 15],\n",
      "         [ 3,  7],\n",
      "         [ 7,  7],\n",
      "         [ 2,  5],\n",
      "         [ 1, 12],\n",
      "         [ 7, 14],\n",
      "         [ 6,  2],\n",
      "         [10,  4],\n",
      "         [ 1,  0],\n",
      "         [ 7,  4],\n",
      "         [ 4, 18],\n",
      "         [ 4,  4],\n",
      "         [ 6, 12],\n",
      "         [ 6,  6],\n",
      "         [10,  3],\n",
      "         [ 1,  5],\n",
      "         [ 7,  5],\n",
      "         [ 2,  1],\n",
      "         [ 2, 18],\n",
      "         [10, 10],\n",
      "         [10,  7],\n",
      "         [ 3,  4],\n",
      "         [ 6,  4],\n",
      "         [ 9,  0],\n",
      "         [ 5,  4],\n",
      "         [ 5, 15],\n",
      "         [ 8, 11],\n",
      "         [ 3,  5],\n",
      "         [10,  6],\n",
      "         [ 3, 10],\n",
      "         [ 6, 14],\n",
      "         [ 5,  8],\n",
      "         [ 8, 17],\n",
      "         [ 3, 16],\n",
      "         [ 8,  9],\n",
      "         [ 0,  4],\n",
      "         [ 9, 14],\n",
      "         [ 7, 18],\n",
      "         [ 1, 18],\n",
      "         [10, 13],\n",
      "         [ 3,  3],\n",
      "         [ 2,  6],\n",
      "         [ 7,  1],\n",
      "         [ 5, 14],\n",
      "         [ 0,  2],\n",
      "         [ 5, 16],\n",
      "         [ 3,  2],\n",
      "         [ 4,  8],\n",
      "         [ 8, 18],\n",
      "         [ 8,  3],\n",
      "         [ 9,  5],\n",
      "         [ 8,  1],\n",
      "         [ 0,  1],\n",
      "         [ 7, 17],\n",
      "         [ 2,  2],\n",
      "         [ 5,  1],\n",
      "         [ 1, 16],\n",
      "         [ 2,  9],\n",
      "         [ 2,  3],\n",
      "         [10, 14],\n",
      "         [ 3, 18],\n",
      "         [ 9,  7],\n",
      "         [ 9, 10],\n",
      "         [ 2,  8],\n",
      "         [ 6,  9],\n",
      "         [10, 12],\n",
      "         [ 3,  0],\n",
      "         [ 4,  0],\n",
      "         [10,  5],\n",
      "         [ 4, 16],\n",
      "         [ 7, 15],\n",
      "         [ 3, 14],\n",
      "         [ 9, 17],\n",
      "         [ 4, 14],\n",
      "         [ 7, 16],\n",
      "         [ 6, 15],\n",
      "         [ 1, 11],\n",
      "         [ 7, 13],\n",
      "         [ 2, 13],\n",
      "         [ 7,  6],\n",
      "         [ 6,  1],\n",
      "         [ 8, 13],\n",
      "         [ 5,  3],\n",
      "         [10, 16],\n",
      "         [ 2, 14],\n",
      "         [ 9,  4],\n",
      "         [ 0,  5],\n",
      "         [ 5,  6],\n",
      "         [ 9,  6],\n",
      "         [ 4,  1],\n",
      "         [ 5, 12],\n",
      "         [ 0, 14],\n",
      "         [ 1,  9],\n",
      "         [ 7, 12],\n",
      "         [ 1,  4],\n",
      "         [ 8,  6],\n",
      "         [ 8,  2],\n",
      "         [ 4,  3],\n",
      "         [10, 17],\n",
      "         [ 9,  2],\n",
      "         [ 3,  8],\n",
      "         [ 2, 11],\n",
      "         [ 6, 18],\n",
      "         [ 8,  8],\n",
      "         [ 0, 16],\n",
      "         [ 2, 10],\n",
      "         [ 7,  3],\n",
      "         [ 0,  7],\n",
      "         [ 9, 12],\n",
      "         [ 0, 18],\n",
      "         [ 4, 13],\n",
      "         [ 5, 18],\n",
      "         [ 2,  0],\n",
      "         [ 9, 15],\n",
      "         [ 7, 11],\n",
      "         [ 0, 17],\n",
      "         [ 9,  3]]]), (11, 19)), 'cls_output': tensor([[0.3515]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junsheng/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples=[\n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\", #0\n",
    "            \n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-07-18-04-22-30-preset-18.jpeg\", # 3\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "n = 1\n",
    "sensor = torch.rand(config.senser_input_num)\n",
    "# sensor = torch.ones(config.senser_input_num)\n",
    "print(sensor)\n",
    "sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "out = infer(examples[0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.3515]], device='cuda:0')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35151166\n"
     ]
    }
   ],
   "source": [
    "print(out[0].cpu().numpy()[0][0])\n",
    "#0.00031266143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test by valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择三组生长期不同的数据去验证训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.query(\"fold==0\").reset_index(drop=True)\n",
    "df_test.to_csv(\"test_by_valid.csv\",index=False)\n",
    "sensor_test_list = df_test.sensor.tolist()\n",
    "image_test_list = df_test.image_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.0731e-06, -1.7551e-07,  1.5700e-07, -2.3826e-08, -2.2488e-06,\n",
      "           3.2136e-06,  6.0759e-06, -5.3882e-09, -1.9196e-07, -7.7300e-07,\n",
      "          -1.5751e-07,  3.6218e-06, -3.8495e-08, -1.0082e-05, -1.3897e-08,\n",
      "          -1.8135e-07, -2.7734e-06,  9.1711e-09, -2.7478e-06, -6.8396e-09,\n",
      "           9.5117e-07,  3.9358e-09,  7.3520e-08, -1.0666e-08,  6.3484e-07,\n",
      "           3.1724e-06, -2.6400e-06, -1.0313e-09,  2.8941e-06,  4.4986e-06,\n",
      "          -9.2994e-06,  1.0640e-06, -4.3119e-07,  2.4046e-08, -9.7306e-07,\n",
      "           1.0860e-06,  7.6602e-08, -7.6102e-08, -2.9333e-09,  2.1590e-06,\n",
      "           7.3389e-08,  8.4670e-06,  3.4530e-08, -2.6061e-07, -4.6415e-06,\n",
      "           1.3894e-08, -6.2063e-08, -1.9665e-06, -2.1450e-06, -2.7528e-06,\n",
      "           1.0902e-07, -4.3158e-06, -3.1787e-08, -1.1309e-09, -2.3554e-06,\n",
      "          -1.7433e-09, -9.4328e-06,  2.6709e-06,  3.7688e-07, -5.3718e-08,\n",
      "           1.0649e-05, -2.6325e-07, -3.8030e-07,  1.8908e-08, -7.3576e-08,\n",
      "          -4.4252e-06,  5.2215e-10,  8.2042e-07, -1.0081e-08,  4.5138e-08,\n",
      "          -5.1646e-09, -4.5427e-07, -3.0546e-08,  7.7243e-07,  2.2411e-07,\n",
      "          -3.2992e-07,  2.4594e-09,  2.2522e-08, -1.8363e-09,  1.8076e-07,\n",
      "          -1.8960e-05,  3.4426e-10, -8.6695e-08, -2.1391e-06, -1.7408e-06,\n",
      "           1.6564e-08, -2.3244e-06,  1.8895e-07,  6.3497e-06, -3.2020e-07,\n",
      "           6.6921e-07,  8.3967e-10, -8.2967e-07,  3.7197e-06, -4.3116e-07,\n",
      "          -9.0634e-09, -2.6965e-08, -8.8893e-10, -1.1645e-08, -7.4030e-09,\n",
      "          -9.8766e-06,  4.5439e-07, -5.4883e-06,  4.2640e-06,  3.0774e-06,\n",
      "           4.9169e-06,  4.9211e-07,  1.9169e-05,  2.0705e-08, -1.8809e-08,\n",
      "           2.0791e-06,  2.9051e-06,  3.1628e-07, -7.7535e-08,  1.3467e-07,\n",
      "           5.7572e-06, -3.7101e-05, -3.0050e-07,  4.7542e-08, -3.7186e-07,\n",
      "          -4.9991e-06,  3.3441e-08, -4.8555e-06,  5.2284e-07,  3.9819e-06,\n",
      "           6.6161e-06,  1.1073e-06,  2.3592e-08,  1.0674e-07,  1.5660e-07,\n",
      "           1.2864e-05, -8.7023e-07, -2.0261e-07,  7.3630e-07,  1.9807e-05,\n",
      "           9.1666e-08, -1.8324e-07, -2.4104e-06, -8.2108e-06,  4.8516e-08,\n",
      "          -1.7242e-05,  3.3765e-06, -4.5645e-07,  3.0120e-06,  1.6659e-09,\n",
      "          -2.7519e-07,  7.9539e-07, -5.0687e-09,  2.4058e-07, -4.3708e-06,\n",
      "           2.7680e-06,  2.7811e-07, -2.0335e-07,  1.6129e-08, -1.0629e-05,\n",
      "          -4.1582e-06, -1.0090e-06, -4.2302e-06,  9.7757e-06,  3.5208e-08,\n",
      "           4.1933e-06,  3.6314e-07, -7.5769e-08,  1.3482e-06, -9.1384e-06,\n",
      "           1.5865e-06, -5.5608e-08,  1.5178e-10, -6.2557e-06,  1.1521e-07,\n",
      "           1.3250e-07,  1.1595e-08,  1.0706e-07, -1.8575e-10,  1.3402e-06,\n",
      "           7.6663e-07,  3.0210e-06,  1.6424e-06,  1.8190e-07, -8.7384e-09,\n",
      "          -5.3204e-08,  1.3948e-06, -3.9114e-07, -6.0375e-06,  1.8625e-06,\n",
      "           1.2639e+01,  2.7469e-05,  1.7479e-07, -1.0068e-06,  1.1419e-06,\n",
      "           3.5686e-06,  1.6829e-08, -3.2214e-08,  1.3307e-06,  9.8617e-08,\n",
      "          -9.7025e-06, -3.4883e-06,  5.2037e-07,  9.9384e-09,  1.9476e-07,\n",
      "           1.1483e-07, -1.0120e-06, -1.9443e-06,  1.3433e-07,  2.0598e-09,\n",
      "           1.8394e-05,  2.3505e-09,  1.5414e-06, -1.3613e-05, -1.3376e-08,\n",
      "          -1.1259e-05,  6.5301e-06,  3.0838e-06,  2.0913e-09,  5.9876e-08,\n",
      "          -3.8095e-06, -2.6421e-07,  3.3421e-09, -1.6452e-09, -4.5684e-07,\n",
      "           1.5680e-09, -3.6358e-09,  8.5328e-07, -3.3297e-08,  1.1831e-05,\n",
      "          -3.8713e-08, -7.6931e-08, -1.2447e-07, -8.2745e-07,  3.5845e-07,\n",
      "          -1.1630e-06, -3.2377e-05,  5.1642e-08, -3.2795e-07, -1.0645e-08,\n",
      "          -4.8180e-06, -2.9540e-07,  2.0671e-06,  1.1643e-07,  1.9250e-07,\n",
      "           2.8088e-08, -1.0073e-06,  3.3712e-07,  6.6968e-09, -3.8131e-08,\n",
      "           2.0502e-05,  4.7569e-07,  6.9069e-08, -3.0895e-07,  2.5827e-09,\n",
      "           1.1561e-07, -7.3428e-09,  7.0800e-08, -3.8727e-07, -1.8628e-07,\n",
      "          -2.3779e-06, -1.9754e-08,  1.2618e-08,  1.7253e-07, -1.4848e-08,\n",
      "           1.3224e-06, -1.8707e-06,  2.2959e-07,  4.2275e-07,  8.8572e-10,\n",
      "           9.0333e-09, -7.8824e-07,  5.2670e-06, -1.9822e-06, -4.2842e-08,\n",
      "          -6.3585e-09, -2.5143e-05,  2.9746e-08, -3.2169e-08, -3.1086e-06,\n",
      "          -6.6394e-08, -1.1715e-05, -9.0405e-10, -1.2117e-06,  1.5701e-06,\n",
      "           1.2708e-05,  6.3809e-08,  2.4725e-07,  4.3409e-08, -1.3163e-06,\n",
      "           1.6261e-08,  1.1395e-07, -6.6286e-06, -3.8524e-09,  6.1935e-07,\n",
      "           2.1917e-07, -3.5461e-06,  7.8639e-06,  2.3439e-08, -1.6630e-08,\n",
      "          -8.4101e-09,  1.7229e-08,  4.2214e-06,  3.4082e-07, -7.9125e-07,\n",
      "           1.5220e-07, -2.0634e-06,  6.5864e-07, -1.1109e-06,  1.2208e-06,\n",
      "           8.6633e-07, -1.9409e-08, -2.4519e-07,  1.7908e-06, -2.2746e-06,\n",
      "          -8.3017e-07, -7.6723e-07,  5.9936e-06, -4.0092e-09,  1.3182e-06,\n",
      "          -3.7049e-09, -1.4338e-06,  1.5914e-07, -4.1659e-09,  1.3179e-06,\n",
      "          -2.0006e-07,  1.8664e-07, -1.6595e-07, -1.2130e-09,  1.2602e-05,\n",
      "           3.0095e-06, -6.8562e-08, -2.2896e-09,  4.2360e-07,  7.7760e-07,\n",
      "           9.1269e-08, -1.0289e-06, -6.4849e-07, -2.7425e-06,  6.2416e-07,\n",
      "           2.1061e-06, -2.5375e-07, -4.6125e-08, -8.3468e-07,  5.2740e-08,\n",
      "          -1.7683e-06, -2.1282e-05, -9.4900e-07,  5.4186e-07, -1.6910e-08,\n",
      "          -4.9602e-09,  6.9899e-06, -2.1253e-08, -1.5927e-06,  1.0562e-08,\n",
      "          -4.2205e-07,  2.0155e-06,  2.3103e-06,  2.3998e-06, -1.8439e-08,\n",
      "          -6.5837e-09,  1.6803e-05, -2.3751e-06,  1.1104e-08,  9.3680e-07,\n",
      "          -4.6216e-09, -4.5140e-09, -9.3330e-09, -1.6987e-08, -5.3564e-07,\n",
      "           3.9501e-08,  6.3337e-09, -3.9531e-06,  1.4346e-05,  1.4624e-09,\n",
      "          -4.7685e-06,  1.8669e-06, -2.3149e-06,  4.7184e-07,  5.4996e-07,\n",
      "          -6.5571e-07,  1.8968e-07,  2.8536e-09,  4.2081e-09,  3.1673e-08,\n",
      "          -5.6114e-07, -3.3732e-07, -4.8259e-06, -1.1937e-07, -1.2564e-05,\n",
      "          -1.0291e-08,  5.6484e-06, -3.3859e-07, -1.4183e-07, -1.1479e-06,\n",
      "           1.4956e-05, -9.9787e-06,  4.2101e-08, -4.3874e-07, -1.9708e-06,\n",
      "          -1.4969e-08,  6.6910e-08, -3.3607e-08,  1.7325e-06, -4.4742e-06,\n",
      "           7.6750e-06, -2.8408e-04, -1.0527e-07, -3.6115e-07,  2.1140e-07,\n",
      "           2.2557e-07,  1.1810e-07,  2.1061e-09, -1.2680e-08, -6.2654e-06,\n",
      "          -8.9140e-06, -6.6753e-09,  2.3934e-06,  1.0343e-06, -9.1273e-08,\n",
      "           8.0178e-07,  5.7988e-08,  3.8291e-07,  2.9898e-07, -2.8716e-06,\n",
      "           2.0028e-07,  8.7726e-09,  2.2997e-06,  9.6309e-06,  6.2171e-07,\n",
      "          -9.2217e-08,  4.0201e-06,  5.0014e-08, -2.8651e-09, -7.3683e-09,\n",
      "          -1.3058e-06, -5.9892e-07, -7.7148e-07,  2.0493e-06,  1.2424e-05,\n",
      "           3.0595e-09,  6.0068e-08, -7.0723e-08, -1.7012e-06,  5.6167e-08,\n",
      "          -7.2496e-06,  2.3567e-06,  1.2073e-06, -5.7118e-07, -2.4134e-08,\n",
      "          -1.1934e-06, -3.2730e-08, -1.3273e-07,  4.5121e-08, -3.4823e-07,\n",
      "          -4.7487e-07,  3.0346e-06, -1.9514e-06,  1.6591e-07, -1.4067e-07,\n",
      "          -3.2425e-06, -1.8640e-06,  1.2860e-06, -5.1713e-06, -5.8073e-09,\n",
      "           8.1023e-09, -2.5076e-07,  6.4184e-08, -6.9696e-06,  6.2891e-06,\n",
      "           1.5059e-06, -1.3498e-08, -6.0809e-07, -1.0118e-06,  2.2347e-06,\n",
      "          -9.6993e-09,  5.5269e-06, -1.4860e-06, -1.3378e-05, -1.6847e-08,\n",
      "           5.1699e-07, -9.7207e-09, -3.6505e-07, -6.9659e-07,  1.5383e-06,\n",
      "          -1.1186e-08, -8.8916e-08,  3.1776e-06, -3.2056e-08,  7.5360e-08,\n",
      "           3.3948e-08,  7.3269e-06,  6.1935e-07, -2.0864e-07, -3.1214e-08,\n",
      "          -1.4246e-06,  8.8758e-08, -4.0477e-06, -1.8798e-05,  2.0243e-07,\n",
      "           6.6564e-08,  2.4743e-07, -4.9650e-09, -3.6522e-07,  9.6634e-09,\n",
      "          -6.8926e-08,  4.7752e-07, -1.4490e-09, -2.3616e-08, -6.8131e-06,\n",
      "           6.7498e-10, -2.3004e-07, -1.1257e-06, -2.4668e-06,  2.6438e-09,\n",
      "           4.7572e-08, -9.3765e-06,  5.2571e-06,  3.2994e-07, -4.6308e-09,\n",
      "          -1.5421e-08, -5.4971e-10,  1.9847e-06,  2.1274e-08, -2.4000e-09,\n",
      "           9.6023e-06, -4.3443e-07,  6.9685e-08, -1.6313e-06, -7.8867e-06,\n",
      "           4.2342e-07, -9.8610e-08, -3.9259e-07,  4.9382e-07, -6.1341e-09,\n",
      "           5.2204e-09,  1.1244e-06, -1.7313e-08,  8.3458e-08, -2.0668e-08,\n",
      "           3.2481e-06,  4.5927e-06,  5.6018e-07, -1.1699e-08,  3.1382e-06,\n",
      "           4.4186e-07,  1.8667e-08,  2.4762e-06,  2.7470e-07,  3.4327e-07,\n",
      "           5.9931e-07, -6.7373e-08, -1.8266e-06, -1.2227e-07, -1.9975e-05,\n",
      "           4.1785e-09,  3.1575e-08,  1.4538e-07, -1.3009e-08,  7.7919e-09,\n",
      "           1.9791e-06,  1.8472e-07,  1.8938e-07, -3.6621e-08, -4.6186e-07,\n",
      "           5.4753e-09, -2.8823e-07, -1.0282e-05, -1.2788e-05, -3.8027e-09,\n",
      "           4.3021e-08,  9.2473e-08,  1.1076e-06,  6.5873e-07,  1.0765e-08,\n",
      "          -2.7780e-07, -1.2427e-08, -1.7375e-07,  3.1917e-06,  3.0789e-08,\n",
      "           8.1597e-07,  1.5044e-06,  3.2296e-07,  1.9071e-06,  2.9520e-06,\n",
      "          -4.0872e-06,  3.2043e-06, -1.3391e-09, -1.7980e-08, -9.5702e-06,\n",
      "           4.1679e-06,  9.4759e-08,  7.0571e-09, -5.8282e-08,  4.0867e-06,\n",
      "           9.3149e-08,  1.4191e-08, -8.6042e-09,  2.0306e-08, -1.7310e-07,\n",
      "           1.8691e-06,  5.9487e-09, -6.5743e-07, -5.8847e-07, -3.4335e-08,\n",
      "           1.4586e-06,  1.7080e-08, -5.9673e-08, -1.2060e-06, -1.2557e-07,\n",
      "          -2.1979e-06, -1.6683e-06,  2.1141e-07,  6.2224e-08,  1.6096e-06,\n",
      "          -1.4156e-05,  1.2281e-05,  2.0004e-09, -1.1214e-09,  1.4128e-07,\n",
      "          -4.2370e-09, -7.5034e-06, -5.8733e-06, -4.7394e-06,  1.6883e-08,\n",
      "          -4.6447e-08,  3.3392e-09, -1.2011e-08, -1.7588e-08, -9.4779e-07,\n",
      "          -1.2280e-06,  6.1355e-09, -1.1840e-07, -6.1665e-06,  7.0297e-08,\n",
      "           6.7159e-08, -9.5825e-07, -2.6515e-07,  2.7544e-06,  2.3040e-09,\n",
      "           7.3039e-08, -7.0246e-10,  3.1016e-07,  2.9115e-08,  4.6559e-09,\n",
      "           1.9581e-07, -1.7755e-09,  1.3034e-07,  2.5012e-09, -6.1695e-08,\n",
      "           1.3941e-07, -1.6701e-07,  4.6332e-07,  1.2836e-07, -5.1817e-06,\n",
      "           1.0447e-07, -1.8767e-06,  8.7700e-10,  2.5772e-07, -3.5616e-06,\n",
      "           1.6789e-05,  3.0133e-06, -4.5417e-07, -9.3467e-08, -1.4756e-05,\n",
      "           1.2650e-07, -1.3181e-05, -1.1558e-06,  2.0676e-05,  4.9732e-07,\n",
      "           6.1125e-08,  5.1931e-06,  1.2303e-05, -5.5269e-07, -6.8725e-07,\n",
      "           3.7497e-06,  5.6268e-09, -2.4272e-08, -4.7970e-08, -8.3988e-09,\n",
      "           1.3658e-06,  2.5008e-07,  5.2300e-08, -3.9522e-06, -1.7959e-06,\n",
      "          -1.6908e-07,  1.1340e-07, -1.3514e-06, -4.8089e-06,  1.4055e-07,\n",
      "          -4.3708e-07, -4.1352e-08,  7.2974e-08, -8.9776e-08, -4.6084e-09,\n",
      "          -7.4264e-07, -1.5908e-05,  3.6736e-06,  7.5160e-06,  5.6613e-08,\n",
      "           1.7199e-09,  2.5234e-06, -4.5027e-08,  5.2434e-08,  5.4994e-08,\n",
      "          -6.9404e-08,  2.1247e-06,  3.1473e-07,  9.8683e-08, -2.8537e-07,\n",
      "          -1.0399e-07, -1.7436e-06,  1.5243e-07, -3.0210e-06,  2.8813e-07,\n",
      "          -4.5804e-07, -6.7805e-06, -3.2295e-07, -2.2355e-08, -2.5477e-07,\n",
      "          -2.1900e-09, -2.6180e-08,  7.6739e-08, -5.6661e-07, -1.0857e-07,\n",
      "           1.0018e-07,  6.1429e-07, -1.1309e-08, -2.1061e-06,  2.4824e-08,\n",
      "           4.2153e-06, -6.9241e-09, -8.9899e-08, -7.0194e-08,  3.3056e-09,\n",
      "           2.1231e-06,  1.8550e-07, -2.2188e-06,  5.5563e-08,  2.2852e-08,\n",
      "          -5.2482e-09,  5.7355e-06, -2.1822e-07,  8.8436e-09, -2.2390e-06,\n",
      "          -1.5666e-07, -6.9595e-09,  1.5325e-07,  1.5357e-07,  1.8194e-07,\n",
      "          -6.0071e-06, -4.1051e-08, -9.2102e-06, -1.2937e-07, -2.9012e-06,\n",
      "           4.0389e-06,  4.2987e-06,  1.2597e-07,  5.3300e-08, -2.4148e-05,\n",
      "          -3.9535e-07, -1.0746e-08, -4.9413e-07,  2.3723e-08, -6.2535e-06,\n",
      "           7.6138e-07, -2.0993e-05,  2.9322e-06,  6.8894e-08,  4.8714e-07,\n",
      "          -7.9850e-07,  3.1065e-07, -1.2993e-06]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1837e-07, -3.4237e-06],\n",
      "         [ 3.0765e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4243e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4256e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9451e-07,\n",
      "           3.1837e-07, -3.4249e-06]]], device='cuda:0'), 'cls_feats': tensor([[ 2.5288e-01, -2.7258e-01,  2.7240e-01,  1.6321e-01,  2.5849e-01,\n",
      "          2.7231e-01,  2.7063e-01,  2.7223e-01, -2.7324e-01, -2.4024e-01,\n",
      "         -2.7101e-01, -2.7261e-01, -2.6433e-01,  3.9661e-06, -4.3211e-07,\n",
      "         -1.5703e-01, -2.6868e-01,  2.5821e-01, -2.7237e-01,  2.7245e-01,\n",
      "         -2.6122e-01, -2.7329e-01, -2.7199e-01,  2.7075e-01, -2.5463e-01,\n",
      "         -2.7205e-01,  2.7250e-01, -2.7123e-01, -2.7055e-01, -2.7131e-01,\n",
      "          2.7270e-01, -1.1699e-01, -1.7259e-01,  6.9664e-03, -2.7120e-01,\n",
      "         -2.7081e-01,  2.6831e-01,  2.7181e-01,  2.6957e-01, -4.9638e-08,\n",
      "         -9.6788e-03, -2.6043e-01,  2.7098e-01,  2.6906e-01, -2.7222e-01,\n",
      "          2.7288e-01,  2.7157e-01,  2.7201e-01,  2.5271e-01, -2.7183e-01,\n",
      "         -2.6768e-01,  2.7307e-01,  2.7306e-01,  2.7026e-01,  2.7167e-01,\n",
      "         -2.5896e-01, -2.7085e-01, -2.7281e-01, -2.7156e-01,  2.5396e-01,\n",
      "          2.7301e-01, -3.4103e-07, -2.7129e-01,  2.6383e-01,  2.7318e-01,\n",
      "         -1.7017e-02, -1.5219e-01,  2.6148e-01, -1.5389e-01, -2.7082e-01,\n",
      "         -2.7194e-01,  2.0381e-08, -2.7221e-01, -2.5464e-01,  1.6901e-01,\n",
      "          2.7231e-01,  2.7073e-01,  2.7132e-01, -2.7122e-01,  2.7053e-01,\n",
      "         -2.7242e-01, -2.7249e-01,  1.2463e-02, -2.7287e-01, -2.5039e-01,\n",
      "          5.5605e-09, -2.7234e-01, -2.5212e-01,  2.7072e-01, -2.6254e-01,\n",
      "         -7.2850e-02, -1.5151e-01,  2.6679e-01, -2.6166e-01,  2.5482e-01,\n",
      "         -2.5412e-01, -2.7222e-01, -2.7132e-01,  2.5817e-01,  2.7328e-01,\n",
      "          1.7616e-01,  2.7324e-01,  2.6439e-06,  2.7010e-01,  2.7202e-01,\n",
      "          9.2776e-07, -2.9403e-07,  6.5829e-08, -2.7220e-01,  2.7222e-01,\n",
      "          3.2278e-02,  2.7050e-01,  2.7205e-01, -2.5076e-01, -5.4181e-03,\n",
      "         -2.6161e-01,  1.4109e-03,  9.9656e-02,  2.7266e-01, -2.8028e-06,\n",
      "         -2.6287e-01,  2.5744e-01, -7.8824e-05, -2.6207e-01, -2.7222e-01,\n",
      "          2.1414e-09, -2.3986e-01, -2.7188e-01, -2.7221e-01,  2.6926e-01,\n",
      "         -9.1637e-07,  2.7247e-01,  3.1514e-06,  2.5841e-01,  4.4186e-08,\n",
      "          2.6541e-01, -2.6115e-01, -2.7159e-01, -1.0765e-01, -2.7250e-01,\n",
      "         -7.0711e-10,  6.9301e-03, -4.2086e-03,  2.6397e-01, -2.6939e-01,\n",
      "         -5.8454e-03, -2.6914e-01, -2.7272e-01,  2.5772e-01, -2.6894e-01,\n",
      "          2.7017e-01, -2.7309e-01, -2.7185e-01, -9.7004e-09,  2.7220e-01,\n",
      "          3.4396e-05,  2.5853e-01,  2.7276e-01,  2.6073e-01,  2.7195e-01,\n",
      "         -1.2605e-01,  2.7297e-01, -2.3523e-01,  2.5457e-01,  2.6780e-01,\n",
      "         -2.6264e-01, -2.7322e-01,  2.5244e-01, -2.7010e-01,  2.7138e-01,\n",
      "         -1.7204e-07,  2.3199e-01,  2.8197e-02,  2.6154e-01,  2.7152e-01,\n",
      "         -2.4978e-07, -2.7262e-01,  2.7183e-01, -1.1341e-08, -2.5020e-01,\n",
      "          9.8418e-02, -2.7290e-01, -2.7085e-01,  2.7121e-01, -2.7184e-01,\n",
      "          2.6090e-01, -2.2624e-01, -1.8845e-10,  2.6459e-01, -2.7261e-01,\n",
      "         -2.7152e-01, -2.6364e-01,  2.6103e-01, -2.7139e-01,  2.5316e-01,\n",
      "         -2.7094e-01, -2.5768e-01, -2.7063e-01, -2.6902e-01, -2.7206e-01,\n",
      "          2.7090e-01,  2.7219e-01, -2.7255e-01, -2.7193e-01, -3.4919e-07,\n",
      "          2.0674e-01,  2.6806e-01, -2.7139e-01, -2.6837e-01,  2.6957e-01,\n",
      "          2.6131e-01,  2.7153e-01, -2.0135e-01,  2.7247e-01, -2.5522e-04,\n",
      "          2.3170e-06,  2.7109e-01, -2.6072e-01,  2.7148e-01, -2.5501e-01,\n",
      "         -3.4281e-05, -1.4949e-01, -2.7286e-01,  2.5676e-01, -1.5075e-01,\n",
      "         -2.6327e-01, -2.7232e-01, -2.7124e-01, -2.7261e-01, -2.7287e-01,\n",
      "         -2.6654e-01,  3.0954e-08, -2.5631e-01,  2.7173e-01,  2.7161e-01,\n",
      "         -2.7224e-01,  1.2901e-02,  2.6623e-01, -2.6222e-01, -2.7326e-01,\n",
      "         -2.6434e-01, -2.6243e-01, -2.6769e-01, -2.7113e-01,  1.1529e-01,\n",
      "         -2.7046e-01,  1.1504e-02, -2.6811e-01,  1.4742e-01, -2.7050e-01,\n",
      "         -2.7052e-01, -2.7338e-01, -2.5415e-03, -2.5366e-01, -2.5618e-01,\n",
      "          2.7225e-01,  2.6539e-03,  2.2757e-04,  2.7331e-01, -2.7075e-01,\n",
      "          2.7083e-01,  2.7251e-01, -2.4930e-01, -5.9975e-07,  2.7249e-01,\n",
      "          2.4692e-01, -2.7221e-01, -2.5980e-01, -2.6345e-01, -2.6207e-01,\n",
      "         -2.6716e-08,  2.7162e-01, -2.3366e-01,  2.7005e-01, -2.7298e-01,\n",
      "          2.6835e-01, -2.5784e-01, -2.7251e-01, -2.7250e-01, -2.4633e-01,\n",
      "          2.7282e-01,  2.6125e-01, -2.6325e-01, -2.7226e-01, -3.4773e-05,\n",
      "          2.7286e-01,  2.7723e-02,  2.6356e-01, -1.1417e-01,  2.6482e-01,\n",
      "          2.6561e-01, -1.3086e-01,  2.7165e-01,  2.7167e-01,  1.1686e-05,\n",
      "          2.7077e-01,  2.7076e-01,  2.5217e-01,  2.6949e-01, -2.5663e-01,\n",
      "          2.7150e-01,  2.7128e-01, -2.6974e-01, -2.6239e-01,  2.7277e-01,\n",
      "         -2.7190e-01, -7.1402e-07,  3.4358e-04, -2.7196e-01, -2.9052e-06,\n",
      "         -2.1660e-01, -2.4171e-01,  1.8973e-06,  2.6833e-01, -2.7237e-01,\n",
      "          2.7320e-01, -2.7290e-01, -2.7095e-01,  2.6925e-01,  2.3120e-02,\n",
      "         -1.6009e-07, -2.7215e-01, -2.7288e-01,  2.7327e-01, -2.5537e-01,\n",
      "          2.7191e-01,  2.6896e-01,  2.7107e-01,  1.1075e-01,  2.6093e-01,\n",
      "          2.7217e-01,  2.6122e-01, -2.6604e-01, -2.2019e-07,  2.7101e-01,\n",
      "         -2.7169e-01, -3.7636e-04, -1.6879e-02,  2.6017e-01, -2.6830e-01,\n",
      "          2.6940e-01, -3.3515e-07,  2.7158e-01,  1.5169e-01,  2.7255e-01,\n",
      "          2.7303e-01, -2.5207e-01, -2.5405e-01, -1.5198e-01, -2.7311e-01,\n",
      "         -2.7231e-01, -2.6662e-01, -1.5311e-01, -2.6983e-01,  2.5635e-01,\n",
      "          2.7177e-01, -2.6919e-01, -2.5063e-01,  2.7188e-01,  1.0539e-01,\n",
      "          2.4401e-01, -2.6985e-01,  2.6209e-01, -2.7192e-01, -8.7844e-08,\n",
      "          2.7319e-01,  2.7136e-01,  2.7251e-01,  2.7222e-01,  2.7049e-01,\n",
      "         -2.3555e-02,  2.5352e-01,  2.7227e-01, -2.7319e-01, -2.7237e-01,\n",
      "         -2.6633e-01,  2.7216e-01,  2.7252e-01, -2.5397e-01, -2.4831e-01,\n",
      "          2.7012e-02, -2.7266e-01, -2.7232e-01,  2.7200e-01, -2.6868e-01,\n",
      "          2.5190e-01, -2.7059e-01, -3.3556e-06, -2.3898e-03, -1.9815e-02,\n",
      "          2.7221e-01, -2.5158e-01, -2.6347e-01, -2.7196e-01, -2.7009e-01,\n",
      "          8.9908e-03,  2.6977e-01,  2.6142e-01, -2.7083e-01, -2.7248e-01,\n",
      "         -1.3527e-01, -1.5988e-01, -2.7207e-01, -2.7127e-01,  2.7255e-01,\n",
      "         -2.7125e-01,  2.6048e-01, -2.4867e-01,  2.7283e-01,  2.6165e-01,\n",
      "          2.7133e-01, -2.7141e-01,  2.6926e-01, -1.3382e-06,  2.7294e-01,\n",
      "          2.6165e-01,  2.7239e-01, -2.7186e-01, -2.7126e-01,  2.7284e-01,\n",
      "         -1.2233e-08,  2.6430e-01,  2.7191e-01,  8.9324e-04,  1.6602e-01,\n",
      "          1.3558e-01, -2.6375e-01,  1.6119e-01,  2.6984e-01,  2.7183e-01,\n",
      "          2.3718e-01, -2.7113e-01, -2.7007e-01, -2.7240e-01, -2.5557e-01,\n",
      "         -2.7274e-01,  1.6510e-01, -2.7004e-01, -2.4821e-01, -3.7476e-04,\n",
      "         -2.7261e-01,  2.6129e-01,  2.7190e-01,  5.3631e-08,  2.7086e-01,\n",
      "         -2.7258e-01,  1.5495e-08,  2.7098e-01, -2.7255e-01, -2.6231e-01,\n",
      "          1.9144e-06, -2.7283e-01,  1.2088e-04, -2.7179e-01, -2.7137e-01,\n",
      "         -2.6966e-01, -7.8888e-04, -2.7032e-01, -2.7284e-01,  2.2231e-01,\n",
      "         -2.7234e-01,  2.7266e-01,  3.9703e-07,  2.7241e-01,  1.0585e-04,\n",
      "          2.5392e-01,  2.7164e-01, -2.6931e-01,  2.7276e-01,  2.6994e-01,\n",
      "          2.7212e-01, -4.3408e-07,  2.7224e-01, -2.7102e-01,  2.2404e-01,\n",
      "          2.2630e-01, -7.3865e-07, -2.7029e-01,  2.7234e-01,  2.5617e-01,\n",
      "         -2.6203e-01, -2.6872e-01,  2.6640e-01,  2.5438e-02, -2.7233e-01,\n",
      "          2.7255e-01, -2.6299e-01, -2.4142e-01,  2.7232e-01, -2.3143e-01,\n",
      "         -2.6310e-01, -6.2119e-03,  2.7317e-01,  2.6808e-01, -1.4919e-01,\n",
      "         -2.7276e-01,  2.6437e-01, -2.6507e-01, -2.6845e-01,  2.7307e-01,\n",
      "          2.7193e-01, -2.0726e-06,  3.2470e-02,  2.7255e-01, -2.5087e-01,\n",
      "          2.8185e-02, -2.7143e-01, -2.4502e-01, -1.3252e-01, -2.7292e-01,\n",
      "          2.6133e-01,  4.2107e-03, -2.2160e-01, -2.6813e-01,  4.6128e-04,\n",
      "          2.7154e-01, -2.7430e-02, -2.5717e-01, -2.2299e-01,  2.7289e-01,\n",
      "         -2.6253e-01, -2.4011e-01, -2.6180e-01,  2.7151e-01, -2.7186e-01,\n",
      "          2.5906e-01, -3.5657e-02, -2.6359e-01,  2.7296e-01,  2.7220e-01,\n",
      "          2.7245e-01, -5.5374e-04, -2.7234e-01,  2.7311e-01, -2.7152e-01,\n",
      "         -1.9574e-02,  2.0795e-01, -2.5458e-01,  2.7174e-01, -6.3690e-08,\n",
      "         -2.7196e-01, -2.5378e-01,  2.7301e-01, -1.4670e-04,  2.7197e-01,\n",
      "         -1.2994e-03, -2.4697e-01, -2.7239e-01, -2.6136e-01,  2.7243e-01,\n",
      "         -4.4962e-03, -2.6187e-01,  2.5357e-01,  2.6836e-01,  2.6452e-01,\n",
      "          2.7242e-01, -1.1671e-08, -3.7872e-06,  2.7240e-01,  2.7219e-01,\n",
      "         -2.5404e-01, -2.6157e-01, -2.7112e-01,  2.6026e-01, -2.7009e-01,\n",
      "          2.4604e-01, -2.7096e-01,  2.7114e-01,  2.6156e-01,  2.7106e-01,\n",
      "         -2.7024e-01,  2.7231e-01,  2.7177e-01,  2.7260e-01, -2.7189e-01,\n",
      "         -2.6704e-01, -8.9918e-06,  1.2807e-02, -2.6727e-01, -2.6880e-01,\n",
      "         -2.7245e-01,  2.7242e-01,  6.1880e-08,  2.6921e-01, -2.4124e-01,\n",
      "          2.6625e-01, -1.4344e-03, -2.7047e-01,  2.6127e-01,  2.7263e-01,\n",
      "          2.7302e-01, -2.5323e-01,  1.3854e-01,  2.7303e-01, -2.1292e-06,\n",
      "          2.7075e-01,  2.6222e-01, -2.6586e-01, -2.7102e-01,  2.7316e-01,\n",
      "          2.6484e-01, -2.7086e-01,  2.6374e-01, -2.6162e-01, -2.5240e-01,\n",
      "         -2.7268e-01,  4.4685e-08,  2.2199e-07, -2.7204e-01, -2.6090e-01,\n",
      "          2.7193e-01,  2.7226e-01,  2.7272e-01,  2.5182e-01,  2.6801e-01,\n",
      "         -2.7108e-01,  1.3630e-08, -2.2754e-05, -2.7341e-01,  2.7316e-01,\n",
      "         -2.7142e-01,  2.6661e-01, -2.7266e-01,  2.7135e-01,  2.7144e-01,\n",
      "          2.7101e-01, -6.0468e-03,  1.0412e-02,  1.5132e-04,  2.5333e-01,\n",
      "         -2.7107e-01, -9.2181e-06, -2.5386e-01,  2.0181e-03, -2.7081e-01,\n",
      "          1.7178e-01, -2.4247e-02,  2.7286e-01,  2.7117e-01,  2.7148e-01,\n",
      "          1.6439e-01, -1.3830e-08, -2.7120e-01,  3.3716e-07, -2.9958e-02,\n",
      "          2.6092e-01, -2.7269e-01, -2.7095e-01,  1.6705e-05, -2.7213e-01,\n",
      "          2.7306e-01,  2.5460e-01,  2.7173e-01,  2.7253e-01, -1.1875e-02,\n",
      "         -2.7179e-01,  2.7160e-01,  2.3348e-01, -2.7144e-01, -2.6970e-01,\n",
      "          2.4618e-01, -1.2409e-04, -2.6886e-01,  2.6868e-01, -2.6966e-01,\n",
      "          2.5979e-01,  1.9486e-04, -1.0526e-01,  4.9793e-03, -2.7039e-01,\n",
      "          2.7276e-01,  2.7166e-01,  2.7125e-01,  2.7186e-01,  4.2877e-04,\n",
      "         -2.6889e-01,  2.6793e-01,  2.5312e-01,  2.7179e-01, -2.7284e-01,\n",
      "         -2.7217e-01, -2.6661e-01, -2.7174e-01,  2.7084e-01, -2.7059e-01,\n",
      "          2.7200e-01,  2.5815e-01, -2.6663e-01,  1.5503e-05, -9.2091e-09,\n",
      "         -2.1341e-04, -2.7051e-01, -2.7060e-01,  2.7222e-01,  1.6286e-01,\n",
      "         -2.6888e-01,  2.7114e-01,  2.7262e-01,  1.6878e-01,  2.7014e-01,\n",
      "         -4.5412e-07,  2.7031e-01,  2.7014e-01, -1.3769e-01, -2.5286e-01,\n",
      "         -2.7096e-01, -2.7042e-01, -2.6453e-01,  9.4697e-07,  2.6969e-01,\n",
      "         -2.2888e-01, -2.6832e-01, -2.7168e-01,  2.7119e-01,  1.5430e-06,\n",
      "          2.5409e-01,  2.7262e-01,  2.7180e-01, -2.6863e-01,  2.7283e-01,\n",
      "          2.7297e-01,  2.5826e-01, -2.6204e-01, -2.6195e-01, -2.6984e-01,\n",
      "          2.5648e-01,  2.7150e-01,  2.7005e-01, -1.2513e-01, -2.7090e-01,\n",
      "          1.8566e-05, -2.7309e-01,  4.2231e-06, -2.5443e-01,  2.7218e-01,\n",
      "         -2.7155e-01,  2.7235e-01, -2.3842e-01, -2.6091e-01,  2.6099e-01,\n",
      "         -2.6661e-01, -2.5382e-01,  2.7002e-01, -2.5515e-01, -2.1556e-02,\n",
      "          2.7125e-01, -2.7167e-01,  1.5780e-02, -2.7131e-01,  2.4199e-02,\n",
      "          2.7210e-01,  2.4877e-01,  2.7250e-01,  2.7162e-01,  2.6918e-02,\n",
      "         -2.6734e-01, -2.7013e-01,  2.7372e-04, -2.6121e-01, -2.7303e-01,\n",
      "         -2.7131e-01, -2.7175e-01,  2.7206e-01,  2.7263e-01,  1.1218e-02,\n",
      "          9.9201e-09, -2.7196e-01,  2.7204e-01]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0731e-06, -1.7551e-07,  1.5700e-07, -2.3826e-08, -2.2488e-06,\n",
      "          3.2136e-06,  6.0759e-06, -5.3882e-09, -1.9196e-07, -7.7300e-07,\n",
      "         -1.5751e-07,  3.6218e-06, -3.8495e-08, -1.0082e-05, -1.3897e-08,\n",
      "         -1.8135e-07, -2.7734e-06,  9.1711e-09, -2.7478e-06, -6.8396e-09,\n",
      "          9.5117e-07,  3.9358e-09,  7.3520e-08, -1.0666e-08,  6.3484e-07,\n",
      "          3.1724e-06, -2.6400e-06, -1.0313e-09,  2.8941e-06,  4.4986e-06,\n",
      "         -9.2994e-06,  1.0640e-06, -4.3119e-07,  2.4046e-08, -9.7306e-07,\n",
      "          1.0860e-06,  7.6602e-08, -7.6102e-08, -2.9333e-09,  2.1590e-06,\n",
      "          7.3389e-08,  8.4670e-06,  3.4530e-08, -2.6061e-07, -4.6415e-06,\n",
      "          1.3894e-08, -6.2063e-08, -1.9665e-06, -2.1450e-06, -2.7528e-06,\n",
      "          1.0902e-07, -4.3158e-06, -3.1787e-08, -1.1309e-09, -2.3554e-06,\n",
      "         -1.7433e-09, -9.4328e-06,  2.6709e-06,  3.7688e-07, -5.3718e-08,\n",
      "          1.0649e-05, -2.6325e-07, -3.8030e-07,  1.8908e-08, -7.3576e-08,\n",
      "         -4.4252e-06,  5.2215e-10,  8.2042e-07, -1.0081e-08,  4.5138e-08,\n",
      "         -5.1646e-09, -4.5427e-07, -3.0546e-08,  7.7243e-07,  2.2411e-07,\n",
      "         -3.2992e-07,  2.4594e-09,  2.2522e-08, -1.8363e-09,  1.8076e-07,\n",
      "         -1.8960e-05,  3.4426e-10, -8.6695e-08, -2.1391e-06, -1.7408e-06,\n",
      "          1.6564e-08, -2.3244e-06,  1.8895e-07,  6.3497e-06, -3.2020e-07,\n",
      "          6.6921e-07,  8.3967e-10, -8.2967e-07,  3.7197e-06, -4.3116e-07,\n",
      "         -9.0634e-09, -2.6965e-08, -8.8893e-10, -1.1645e-08, -7.4030e-09,\n",
      "         -9.8766e-06,  4.5439e-07, -5.4883e-06,  4.2640e-06,  3.0774e-06,\n",
      "          4.9169e-06,  4.9211e-07,  1.9169e-05,  2.0705e-08, -1.8809e-08,\n",
      "          2.0791e-06,  2.9051e-06,  3.1628e-07, -7.7535e-08,  1.3467e-07,\n",
      "          5.7572e-06, -3.7101e-05, -3.0050e-07,  4.7542e-08, -3.7186e-07,\n",
      "         -4.9991e-06,  3.3441e-08, -4.8555e-06,  5.2284e-07,  3.9819e-06,\n",
      "          6.6161e-06,  1.1073e-06,  2.3592e-08,  1.0674e-07,  1.5660e-07,\n",
      "          1.2864e-05, -8.7023e-07, -2.0261e-07,  7.3630e-07,  1.9807e-05,\n",
      "          9.1666e-08, -1.8324e-07, -2.4104e-06, -8.2108e-06,  4.8516e-08,\n",
      "         -1.7242e-05,  3.3765e-06, -4.5645e-07,  3.0120e-06,  1.6659e-09,\n",
      "         -2.7519e-07,  7.9539e-07, -5.0687e-09,  2.4058e-07, -4.3708e-06,\n",
      "          2.7680e-06,  2.7811e-07, -2.0335e-07,  1.6129e-08, -1.0629e-05,\n",
      "         -4.1582e-06, -1.0090e-06, -4.2302e-06,  9.7757e-06,  3.5208e-08,\n",
      "          4.1933e-06,  3.6314e-07, -7.5769e-08,  1.3482e-06, -9.1384e-06,\n",
      "          1.5865e-06, -5.5608e-08,  1.5178e-10, -6.2557e-06,  1.1521e-07,\n",
      "          1.3250e-07,  1.1595e-08,  1.0706e-07, -1.8575e-10,  1.3402e-06,\n",
      "          7.6663e-07,  3.0210e-06,  1.6424e-06,  1.8190e-07, -8.7384e-09,\n",
      "         -5.3204e-08,  1.3948e-06, -3.9114e-07, -6.0375e-06,  1.8625e-06,\n",
      "          1.2639e+01,  2.7469e-05,  1.7479e-07, -1.0068e-06,  1.1419e-06,\n",
      "          3.5686e-06,  1.6829e-08, -3.2214e-08,  1.3307e-06,  9.8617e-08,\n",
      "         -9.7025e-06, -3.4883e-06,  5.2037e-07,  9.9384e-09,  1.9476e-07,\n",
      "          1.1483e-07, -1.0120e-06, -1.9443e-06,  1.3433e-07,  2.0598e-09,\n",
      "          1.8394e-05,  2.3505e-09,  1.5414e-06, -1.3613e-05, -1.3376e-08,\n",
      "         -1.1259e-05,  6.5301e-06,  3.0838e-06,  2.0913e-09,  5.9876e-08,\n",
      "         -3.8095e-06, -2.6421e-07,  3.3421e-09, -1.6452e-09, -4.5684e-07,\n",
      "          1.5680e-09, -3.6358e-09,  8.5328e-07, -3.3297e-08,  1.1831e-05,\n",
      "         -3.8713e-08, -7.6931e-08, -1.2447e-07, -8.2745e-07,  3.5845e-07,\n",
      "         -1.1630e-06, -3.2377e-05,  5.1642e-08, -3.2795e-07, -1.0645e-08,\n",
      "         -4.8180e-06, -2.9540e-07,  2.0671e-06,  1.1643e-07,  1.9250e-07,\n",
      "          2.8088e-08, -1.0073e-06,  3.3712e-07,  6.6968e-09, -3.8131e-08,\n",
      "          2.0502e-05,  4.7569e-07,  6.9069e-08, -3.0895e-07,  2.5827e-09,\n",
      "          1.1561e-07, -7.3428e-09,  7.0800e-08, -3.8727e-07, -1.8628e-07,\n",
      "         -2.3779e-06, -1.9754e-08,  1.2618e-08,  1.7253e-07, -1.4848e-08,\n",
      "          1.3224e-06, -1.8707e-06,  2.2959e-07,  4.2275e-07,  8.8572e-10,\n",
      "          9.0333e-09, -7.8824e-07,  5.2670e-06, -1.9822e-06, -4.2842e-08,\n",
      "         -6.3585e-09, -2.5143e-05,  2.9746e-08, -3.2169e-08, -3.1086e-06,\n",
      "         -6.6394e-08, -1.1715e-05, -9.0405e-10, -1.2117e-06,  1.5701e-06,\n",
      "          1.2708e-05,  6.3809e-08,  2.4725e-07,  4.3409e-08, -1.3163e-06,\n",
      "          1.6261e-08,  1.1395e-07, -6.6286e-06, -3.8524e-09,  6.1935e-07,\n",
      "          2.1917e-07, -3.5461e-06,  7.8639e-06,  2.3439e-08, -1.6630e-08,\n",
      "         -8.4101e-09,  1.7229e-08,  4.2214e-06,  3.4082e-07, -7.9125e-07,\n",
      "          1.5220e-07, -2.0634e-06,  6.5864e-07, -1.1109e-06,  1.2208e-06,\n",
      "          8.6633e-07, -1.9409e-08, -2.4519e-07,  1.7908e-06, -2.2746e-06,\n",
      "         -8.3017e-07, -7.6723e-07,  5.9936e-06, -4.0092e-09,  1.3182e-06,\n",
      "         -3.7049e-09, -1.4338e-06,  1.5914e-07, -4.1659e-09,  1.3179e-06,\n",
      "         -2.0006e-07,  1.8664e-07, -1.6595e-07, -1.2130e-09,  1.2602e-05,\n",
      "          3.0095e-06, -6.8562e-08, -2.2896e-09,  4.2360e-07,  7.7760e-07,\n",
      "          9.1269e-08, -1.0289e-06, -6.4849e-07, -2.7425e-06,  6.2416e-07,\n",
      "          2.1061e-06, -2.5375e-07, -4.6125e-08, -8.3468e-07,  5.2740e-08,\n",
      "         -1.7683e-06, -2.1282e-05, -9.4900e-07,  5.4186e-07, -1.6910e-08,\n",
      "         -4.9602e-09,  6.9899e-06, -2.1253e-08, -1.5927e-06,  1.0562e-08,\n",
      "         -4.2205e-07,  2.0155e-06,  2.3103e-06,  2.3998e-06, -1.8439e-08,\n",
      "         -6.5837e-09,  1.6803e-05, -2.3751e-06,  1.1104e-08,  9.3680e-07,\n",
      "         -4.6216e-09, -4.5140e-09, -9.3330e-09, -1.6987e-08, -5.3564e-07,\n",
      "          3.9501e-08,  6.3337e-09, -3.9531e-06,  1.4346e-05,  1.4624e-09,\n",
      "         -4.7685e-06,  1.8669e-06, -2.3149e-06,  4.7184e-07,  5.4996e-07,\n",
      "         -6.5571e-07,  1.8968e-07,  2.8536e-09,  4.2081e-09,  3.1673e-08,\n",
      "         -5.6114e-07, -3.3732e-07, -4.8259e-06, -1.1937e-07, -1.2564e-05,\n",
      "         -1.0291e-08,  5.6484e-06, -3.3859e-07, -1.4183e-07, -1.1479e-06,\n",
      "          1.4956e-05, -9.9787e-06,  4.2101e-08, -4.3874e-07, -1.9708e-06,\n",
      "         -1.4969e-08,  6.6910e-08, -3.3607e-08,  1.7325e-06, -4.4742e-06,\n",
      "          7.6750e-06, -2.8408e-04, -1.0527e-07, -3.6115e-07,  2.1140e-07,\n",
      "          2.2557e-07,  1.1810e-07,  2.1061e-09, -1.2680e-08, -6.2654e-06,\n",
      "         -8.9140e-06, -6.6753e-09,  2.3934e-06,  1.0343e-06, -9.1273e-08,\n",
      "          8.0178e-07,  5.7988e-08,  3.8291e-07,  2.9898e-07, -2.8716e-06,\n",
      "          2.0028e-07,  8.7726e-09,  2.2997e-06,  9.6309e-06,  6.2171e-07,\n",
      "         -9.2217e-08,  4.0201e-06,  5.0014e-08, -2.8651e-09, -7.3683e-09,\n",
      "         -1.3058e-06, -5.9892e-07, -7.7148e-07,  2.0493e-06,  1.2424e-05,\n",
      "          3.0595e-09,  6.0068e-08, -7.0723e-08, -1.7012e-06,  5.6167e-08,\n",
      "         -7.2496e-06,  2.3567e-06,  1.2073e-06, -5.7118e-07, -2.4134e-08,\n",
      "         -1.1934e-06, -3.2730e-08, -1.3273e-07,  4.5121e-08, -3.4823e-07,\n",
      "         -4.7487e-07,  3.0346e-06, -1.9514e-06,  1.6591e-07, -1.4067e-07,\n",
      "         -3.2425e-06, -1.8640e-06,  1.2860e-06, -5.1713e-06, -5.8073e-09,\n",
      "          8.1023e-09, -2.5076e-07,  6.4184e-08, -6.9696e-06,  6.2891e-06,\n",
      "          1.5059e-06, -1.3498e-08, -6.0809e-07, -1.0118e-06,  2.2347e-06,\n",
      "         -9.6993e-09,  5.5269e-06, -1.4860e-06, -1.3378e-05, -1.6847e-08,\n",
      "          5.1699e-07, -9.7207e-09, -3.6505e-07, -6.9659e-07,  1.5383e-06,\n",
      "         -1.1186e-08, -8.8916e-08,  3.1776e-06, -3.2056e-08,  7.5360e-08,\n",
      "          3.3948e-08,  7.3269e-06,  6.1935e-07, -2.0864e-07, -3.1214e-08,\n",
      "         -1.4246e-06,  8.8758e-08, -4.0477e-06, -1.8798e-05,  2.0243e-07,\n",
      "          6.6564e-08,  2.4743e-07, -4.9650e-09, -3.6522e-07,  9.6634e-09,\n",
      "         -6.8926e-08,  4.7752e-07, -1.4490e-09, -2.3616e-08, -6.8131e-06,\n",
      "          6.7498e-10, -2.3004e-07, -1.1257e-06, -2.4668e-06,  2.6438e-09,\n",
      "          4.7572e-08, -9.3765e-06,  5.2571e-06,  3.2994e-07, -4.6308e-09,\n",
      "         -1.5421e-08, -5.4971e-10,  1.9847e-06,  2.1274e-08, -2.4000e-09,\n",
      "          9.6023e-06, -4.3443e-07,  6.9685e-08, -1.6313e-06, -7.8867e-06,\n",
      "          4.2342e-07, -9.8610e-08, -3.9259e-07,  4.9382e-07, -6.1341e-09,\n",
      "          5.2204e-09,  1.1244e-06, -1.7313e-08,  8.3458e-08, -2.0668e-08,\n",
      "          3.2481e-06,  4.5927e-06,  5.6018e-07, -1.1699e-08,  3.1382e-06,\n",
      "          4.4186e-07,  1.8667e-08,  2.4762e-06,  2.7470e-07,  3.4327e-07,\n",
      "          5.9931e-07, -6.7373e-08, -1.8266e-06, -1.2227e-07, -1.9975e-05,\n",
      "          4.1785e-09,  3.1575e-08,  1.4538e-07, -1.3009e-08,  7.7919e-09,\n",
      "          1.9791e-06,  1.8472e-07,  1.8938e-07, -3.6621e-08, -4.6186e-07,\n",
      "          5.4753e-09, -2.8823e-07, -1.0282e-05, -1.2788e-05, -3.8027e-09,\n",
      "          4.3021e-08,  9.2473e-08,  1.1076e-06,  6.5873e-07,  1.0765e-08,\n",
      "         -2.7780e-07, -1.2427e-08, -1.7375e-07,  3.1917e-06,  3.0789e-08,\n",
      "          8.1597e-07,  1.5044e-06,  3.2296e-07,  1.9071e-06,  2.9520e-06,\n",
      "         -4.0872e-06,  3.2043e-06, -1.3391e-09, -1.7980e-08, -9.5702e-06,\n",
      "          4.1679e-06,  9.4759e-08,  7.0571e-09, -5.8282e-08,  4.0867e-06,\n",
      "          9.3149e-08,  1.4191e-08, -8.6042e-09,  2.0306e-08, -1.7310e-07,\n",
      "          1.8691e-06,  5.9487e-09, -6.5743e-07, -5.8847e-07, -3.4335e-08,\n",
      "          1.4586e-06,  1.7080e-08, -5.9673e-08, -1.2060e-06, -1.2557e-07,\n",
      "         -2.1979e-06, -1.6683e-06,  2.1141e-07,  6.2224e-08,  1.6096e-06,\n",
      "         -1.4156e-05,  1.2281e-05,  2.0004e-09, -1.1214e-09,  1.4128e-07,\n",
      "         -4.2370e-09, -7.5034e-06, -5.8733e-06, -4.7394e-06,  1.6883e-08,\n",
      "         -4.6447e-08,  3.3392e-09, -1.2011e-08, -1.7588e-08, -9.4779e-07,\n",
      "         -1.2280e-06,  6.1355e-09, -1.1840e-07, -6.1665e-06,  7.0297e-08,\n",
      "          6.7159e-08, -9.5825e-07, -2.6515e-07,  2.7544e-06,  2.3040e-09,\n",
      "          7.3039e-08, -7.0246e-10,  3.1016e-07,  2.9115e-08,  4.6559e-09,\n",
      "          1.9581e-07, -1.7755e-09,  1.3034e-07,  2.5012e-09, -6.1695e-08,\n",
      "          1.3941e-07, -1.6701e-07,  4.6332e-07,  1.2836e-07, -5.1817e-06,\n",
      "          1.0447e-07, -1.8767e-06,  8.7700e-10,  2.5772e-07, -3.5616e-06,\n",
      "          1.6789e-05,  3.0133e-06, -4.5417e-07, -9.3467e-08, -1.4756e-05,\n",
      "          1.2650e-07, -1.3181e-05, -1.1558e-06,  2.0676e-05,  4.9732e-07,\n",
      "          6.1125e-08,  5.1931e-06,  1.2303e-05, -5.5269e-07, -6.8725e-07,\n",
      "          3.7497e-06,  5.6268e-09, -2.4272e-08, -4.7970e-08, -8.3988e-09,\n",
      "          1.3658e-06,  2.5008e-07,  5.2300e-08, -3.9522e-06, -1.7959e-06,\n",
      "         -1.6908e-07,  1.1340e-07, -1.3514e-06, -4.8089e-06,  1.4055e-07,\n",
      "         -4.3708e-07, -4.1352e-08,  7.2974e-08, -8.9776e-08, -4.6084e-09,\n",
      "         -7.4264e-07, -1.5908e-05,  3.6736e-06,  7.5160e-06,  5.6613e-08,\n",
      "          1.7199e-09,  2.5234e-06, -4.5027e-08,  5.2434e-08,  5.4994e-08,\n",
      "         -6.9404e-08,  2.1247e-06,  3.1473e-07,  9.8683e-08, -2.8537e-07,\n",
      "         -1.0399e-07, -1.7436e-06,  1.5243e-07, -3.0210e-06,  2.8813e-07,\n",
      "         -4.5804e-07, -6.7805e-06, -3.2295e-07, -2.2355e-08, -2.5477e-07,\n",
      "         -2.1900e-09, -2.6180e-08,  7.6739e-08, -5.6661e-07, -1.0857e-07,\n",
      "          1.0018e-07,  6.1429e-07, -1.1309e-08, -2.1061e-06,  2.4824e-08,\n",
      "          4.2153e-06, -6.9241e-09, -8.9899e-08, -7.0194e-08,  3.3056e-09,\n",
      "          2.1231e-06,  1.8550e-07, -2.2188e-06,  5.5563e-08,  2.2852e-08,\n",
      "         -5.2482e-09,  5.7355e-06, -2.1822e-07,  8.8436e-09, -2.2390e-06,\n",
      "         -1.5666e-07, -6.9595e-09,  1.5325e-07,  1.5357e-07,  1.8194e-07,\n",
      "         -6.0071e-06, -4.1051e-08, -9.2102e-06, -1.2937e-07, -2.9012e-06,\n",
      "          4.0389e-06,  4.2987e-06,  1.2597e-07,  5.3300e-08, -2.4148e-05,\n",
      "         -3.9535e-07, -1.0746e-08, -4.9413e-07,  2.3723e-08, -6.2535e-06,\n",
      "          7.6138e-07, -2.0993e-05,  2.9322e-06,  6.8894e-08,  4.8714e-07,\n",
      "         -7.9850e-07,  3.1065e-07, -1.2993e-06]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 4,  1],\n",
      "         [10,  8],\n",
      "         [10, 12],\n",
      "         [10, 14],\n",
      "         [ 0, 16],\n",
      "         [ 7,  0],\n",
      "         [ 7,  1],\n",
      "         [ 4,  0],\n",
      "         [ 1,  3],\n",
      "         [ 3, 15],\n",
      "         [ 0,  8],\n",
      "         [ 8,  3],\n",
      "         [ 6,  2],\n",
      "         [ 9,  0],\n",
      "         [ 2, 11],\n",
      "         [ 3,  6],\n",
      "         [ 7,  3],\n",
      "         [ 1,  4],\n",
      "         [10, 16],\n",
      "         [ 6, 12],\n",
      "         [ 2,  0],\n",
      "         [ 7,  9],\n",
      "         [ 8,  2],\n",
      "         [ 2,  8],\n",
      "         [ 7,  8],\n",
      "         [ 6,  5],\n",
      "         [ 8, 10],\n",
      "         [ 1, 18],\n",
      "         [ 6, 14],\n",
      "         [ 9, 11],\n",
      "         [ 8,  0],\n",
      "         [ 3,  1],\n",
      "         [ 6,  4],\n",
      "         [ 7, 12],\n",
      "         [ 5, 13],\n",
      "         [ 5, 17],\n",
      "         [ 3, 18],\n",
      "         [ 8, 17],\n",
      "         [ 9,  4],\n",
      "         [ 0,  0],\n",
      "         [ 3,  3],\n",
      "         [10, 17],\n",
      "         [ 3, 13],\n",
      "         [ 9,  2],\n",
      "         [ 2,  2],\n",
      "         [ 9, 18],\n",
      "         [10,  4],\n",
      "         [ 3,  5],\n",
      "         [ 2, 17],\n",
      "         [ 2, 18],\n",
      "         [ 4, 18],\n",
      "         [ 4,  2],\n",
      "         [ 4, 11],\n",
      "         [ 7, 15],\n",
      "         [ 5,  5],\n",
      "         [ 1,  9],\n",
      "         [ 6,  3],\n",
      "         [ 5, 16],\n",
      "         [ 6,  0],\n",
      "         [ 2, 13],\n",
      "         [ 3,  9],\n",
      "         [ 2,  4],\n",
      "         [ 1, 14],\n",
      "         [ 6, 15],\n",
      "         [ 6, 11],\n",
      "         [ 7, 10],\n",
      "         [ 5,  7],\n",
      "         [ 0,  1],\n",
      "         [ 5,  6],\n",
      "         [ 0, 10],\n",
      "         [ 1, 17],\n",
      "         [ 0,  7],\n",
      "         [ 9, 14],\n",
      "         [10,  7],\n",
      "         [ 2,  1],\n",
      "         [ 2, 16],\n",
      "         [ 5, 14],\n",
      "         [ 9,  5],\n",
      "         [ 8, 11],\n",
      "         [ 9, 12],\n",
      "         [ 4, 12],\n",
      "         [ 1,  8],\n",
      "         [ 6,  8],\n",
      "         [10, 13],\n",
      "         [ 7,  2],\n",
      "         [ 9,  3],\n",
      "         [ 5, 12],\n",
      "         [ 8,  5],\n",
      "         [ 2, 12],\n",
      "         [ 4, 10],\n",
      "         [ 1,  1],\n",
      "         [10, 11],\n",
      "         [ 5, 10],\n",
      "         [ 9, 15],\n",
      "         [ 5,  2],\n",
      "         [ 8,  8],\n",
      "         [ 0, 11],\n",
      "         [ 7,  5],\n",
      "         [ 6, 13],\n",
      "         [ 8,  6],\n",
      "         [ 4, 17],\n",
      "         [ 3, 16],\n",
      "         [ 5,  8],\n",
      "         [10,  9],\n",
      "         [ 6,  7],\n",
      "         [ 2, 14],\n",
      "         [10,  6],\n",
      "         [ 9,  7],\n",
      "         [ 8, 18],\n",
      "         [ 2,  6],\n",
      "         [ 2,  3],\n",
      "         [ 5,  0],\n",
      "         [ 0, 18],\n",
      "         [ 1,  5],\n",
      "         [ 0, 14],\n",
      "         [ 8,  9],\n",
      "         [10,  2],\n",
      "         [ 2,  5],\n",
      "         [ 8, 12],\n",
      "         [ 9, 17],\n",
      "         [ 6, 18],\n",
      "         [ 8, 13],\n",
      "         [10,  1],\n",
      "         [ 0, 13],\n",
      "         [ 8, 16],\n",
      "         [ 0,  2],\n",
      "         [10,  5],\n",
      "         [ 8,  7],\n",
      "         [ 1, 11],\n",
      "         [ 8,  4],\n",
      "         [ 4, 13],\n",
      "         [ 1, 16],\n",
      "         [ 1, 12],\n",
      "         [ 4,  7],\n",
      "         [ 4,  3],\n",
      "         [ 1, 13],\n",
      "         [ 4, 14],\n",
      "         [10, 15],\n",
      "         [ 9,  1],\n",
      "         [ 2,  7],\n",
      "         [ 4,  8],\n",
      "         [ 5,  3],\n",
      "         [ 7,  4],\n",
      "         [ 1,  7],\n",
      "         [ 3, 10],\n",
      "         [ 6,  9],\n",
      "         [ 9, 16],\n",
      "         [ 9,  9],\n",
      "         [ 4, 16],\n",
      "         [ 0,  5],\n",
      "         [ 3,  7],\n",
      "         [ 5, 15],\n",
      "         [ 2, 10],\n",
      "         [ 1,  0],\n",
      "         [10, 18],\n",
      "         [ 1,  2],\n",
      "         [ 7, 18],\n",
      "         [ 7, 11],\n",
      "         [ 4,  4],\n",
      "         [ 9, 13],\n",
      "         [ 6,  6],\n",
      "         [ 6, 16],\n",
      "         [ 2, 15],\n",
      "         [ 3, 14],\n",
      "         [ 3,  4],\n",
      "         [ 4,  9],\n",
      "         [ 5,  1],\n",
      "         [ 7, 16],\n",
      "         [ 5,  4],\n",
      "         [ 3,  2],\n",
      "         [ 1,  6],\n",
      "         [ 7, 14],\n",
      "         [ 0, 12],\n",
      "         [ 9,  8],\n",
      "         [ 3,  8],\n",
      "         [ 9,  6],\n",
      "         [ 0, 15],\n",
      "         [ 3, 17],\n",
      "         [ 7,  7],\n",
      "         [ 7, 13],\n",
      "         [ 0,  6],\n",
      "         [ 9, 10],\n",
      "         [ 8, 14],\n",
      "         [ 3, 11],\n",
      "         [ 1, 10],\n",
      "         [ 0, 17],\n",
      "         [ 4, 15],\n",
      "         [ 1, 15],\n",
      "         [10,  3],\n",
      "         [ 0,  3],\n",
      "         [ 8,  1],\n",
      "         [ 4,  5],\n",
      "         [ 2,  9],\n",
      "         [ 6, 10],\n",
      "         [ 3,  0],\n",
      "         [ 6,  1],\n",
      "         [ 0,  4],\n",
      "         [ 8, 15],\n",
      "         [ 3, 12],\n",
      "         [10,  0],\n",
      "         [ 4,  6],\n",
      "         [ 5, 11],\n",
      "         [ 7, 17],\n",
      "         [ 7,  6],\n",
      "         [ 0,  9],\n",
      "         [ 5,  9],\n",
      "         [ 6, 17],\n",
      "         [10, 10],\n",
      "         [ 5, 18]]]), (11, 19)), 'cls_output': tensor([[0.0285]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/164085027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 64\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.0827e-06, -1.7530e-07,  1.5452e-07, -2.2674e-08, -2.2496e-06,\n",
      "           3.2150e-06,  6.0465e-06, -5.3801e-09, -1.8847e-07, -7.7690e-07,\n",
      "          -1.5788e-07,  3.6213e-06, -3.8558e-08, -1.0063e-05, -1.4680e-08,\n",
      "          -1.8126e-07, -2.7730e-06,  9.1713e-09, -2.7553e-06, -5.3852e-09,\n",
      "           9.5103e-07,  3.8168e-09,  7.2278e-08, -1.0654e-08,  6.3138e-07,\n",
      "           3.1689e-06, -2.6612e-06, -1.0689e-09,  2.8926e-06,  4.4980e-06,\n",
      "          -9.2651e-06,  1.0652e-06, -4.1821e-07,  2.2096e-08, -9.7153e-07,\n",
      "           1.0851e-06,  7.0076e-08, -7.6146e-08, -2.9167e-09,  2.1692e-06,\n",
      "           7.3493e-08,  8.4667e-06,  3.4533e-08, -2.6099e-07, -4.6151e-06,\n",
      "           1.3888e-08,  3.3025e-07, -5.3568e-06, -2.0778e-06, -2.7470e-06,\n",
      "           1.0874e-07, -4.3026e-06, -3.1775e-08, -1.3380e-09, -2.3559e-06,\n",
      "          -1.7084e-09, -9.4255e-06,  2.6611e-06,  7.8697e-07, -5.3748e-08,\n",
      "           1.0649e-05, -2.6270e-07, -3.8041e-07,  1.9315e-08, -7.3593e-08,\n",
      "          -4.4816e-06,  5.6049e-10,  8.2062e-07, -1.0063e-08,  4.5128e-08,\n",
      "          -5.1747e-09, -4.5403e-07, -2.9219e-08,  7.7394e-07,  2.2411e-07,\n",
      "          -3.3084e-07,  2.4516e-09,  3.5821e-10, -1.8521e-09,  1.8354e-07,\n",
      "          -1.8954e-05,  2.7999e-10, -8.5999e-08, -2.1377e-06, -1.6716e-06,\n",
      "           1.6572e-08, -2.3315e-06,  1.8895e-07,  6.3496e-06, -3.1833e-07,\n",
      "           6.2023e-07,  8.4298e-10, -8.2967e-07,  3.7212e-06,  1.1039e-06,\n",
      "          -9.0300e-09, -2.6949e-08, -8.5791e-10, -1.1635e-08, -7.3959e-09,\n",
      "          -9.7975e-06,  4.5500e-07, -5.4868e-06,  4.2644e-06,  3.0767e-06,\n",
      "           4.9342e-06,  4.9311e-07,  1.9163e-05,  2.2566e-08, -1.8834e-08,\n",
      "           2.0761e-06,  2.9012e-06,  3.2295e-07, -7.7596e-08,  1.3192e-07,\n",
      "           5.7596e-06, -3.6992e-05, -3.0007e-07,  4.7662e-08, -3.7218e-07,\n",
      "          -5.0134e-06,  3.3447e-08, -4.8486e-06,  5.2302e-07,  3.9796e-06,\n",
      "           6.6157e-06,  1.0902e-06,  2.3589e-08,  1.0390e-07,  1.5728e-07,\n",
      "           1.2887e-05, -8.6768e-07, -2.0269e-07,  7.0103e-07,  1.9808e-05,\n",
      "           9.1876e-08, -1.8358e-07, -3.2414e-05, -8.2112e-06,  4.8515e-08,\n",
      "          -1.7336e-05,  3.3777e-06, -4.5661e-07,  3.0050e-06,  1.6503e-09,\n",
      "          -7.2160e-09,  7.9344e-07, -5.0495e-09,  2.4069e-07, -4.3610e-06,\n",
      "           2.7735e-06,  2.7815e-07, -2.0350e-07,  1.6112e-08, -2.0510e-06,\n",
      "          -4.1556e-06, -1.0095e-06, -4.1982e-06,  9.7768e-06,  3.5272e-08,\n",
      "           4.1931e-06,  3.6369e-07, -7.5657e-08,  1.3493e-06, -9.1426e-06,\n",
      "           1.5492e-06, -5.5606e-08,  2.8326e-10, -6.2274e-06,  1.1553e-07,\n",
      "           1.3244e-07,  1.1595e-08,  1.0701e-07, -1.7686e-10,  1.3434e-06,\n",
      "           7.6663e-07,  3.0214e-06,  3.1273e-06,  1.8162e-07, -8.7588e-09,\n",
      "          -5.3203e-08,  1.3994e-06, -3.9140e-07, -5.9820e-06,  1.8577e-06,\n",
      "          -2.9392e+00,  2.7478e-05,  1.7482e-07, -1.0052e-06,  1.1428e-06,\n",
      "           3.5725e-06,  1.6826e-08, -3.2411e-08,  1.3470e-06,  9.9076e-08,\n",
      "          -9.7098e-06, -3.4484e-06,  5.2059e-07,  9.9433e-09, -1.4315e-06,\n",
      "           1.1571e-07, -1.0162e-06, -1.9451e-06,  1.3364e-07,  2.0557e-09,\n",
      "           1.8360e-05,  2.3615e-09,  1.5429e-06, -1.3612e-05, -1.3375e-08,\n",
      "          -1.1263e-05,  6.5371e-06,  3.0841e-06,  2.1411e-09,  6.0650e-08,\n",
      "          -3.8018e-06, -2.6458e-07,  3.9600e-09, -1.6493e-09, -4.5734e-07,\n",
      "           1.6022e-09, -3.6367e-09,  8.5293e-07, -3.3017e-08,  1.1829e-05,\n",
      "          -3.8856e-08, -7.8512e-08, -1.2441e-07, -8.3563e-07,  3.5845e-07,\n",
      "          -1.1632e-06, -3.2378e-05,  5.1766e-08, -3.2791e-07, -1.0741e-08,\n",
      "          -4.8005e-06, -3.0898e-07,  2.0715e-06,  1.1634e-07,  1.9251e-07,\n",
      "          -6.8956e-08,  1.6680e-06,  3.3701e-07,  6.6928e-09, -3.8224e-08,\n",
      "           2.0501e-05,  4.8596e-07,  6.9212e-08, -3.0852e-07,  2.6557e-09,\n",
      "           1.1923e-07, -8.0101e-09,  7.0794e-08, -3.8855e-07, -1.8652e-07,\n",
      "          -2.3786e-06, -1.5048e-08,  1.0523e-08,  1.7478e-07, -1.3772e-08,\n",
      "           1.3210e-06, -1.8784e-06,  2.4997e-06,  3.5084e-07,  3.1842e-10,\n",
      "           9.0063e-09, -7.8309e-07,  5.2683e-06,  1.2784e-05, -4.2034e-08,\n",
      "          -6.3294e-09, -2.5147e-05,  2.9737e-08, -3.2164e-08, -3.1089e-06,\n",
      "          -6.6904e-08, -1.1640e-05, -8.9892e-10, -1.2145e-06,  1.5709e-06,\n",
      "           1.2708e-05,  6.3839e-08,  2.4791e-07,  7.2218e-08, -1.3138e-06,\n",
      "           1.6254e-08,  1.1444e-07, -6.5649e-06, -3.8067e-09,  6.3245e-07,\n",
      "           2.2508e-07, -3.5040e-06,  7.8615e-06,  2.3449e-08, -1.6639e-08,\n",
      "          -1.5493e-08,  1.7303e-08,  4.1829e-06,  3.4123e-07, -7.8461e-07,\n",
      "           1.5215e-07, -2.0604e-06,  6.5877e-07, -1.1104e-06,  1.2191e-06,\n",
      "           8.6716e-07, -1.7774e-08, -2.4495e-07,  1.7908e-06, -1.5804e-06,\n",
      "          -8.3130e-07, -3.4135e-07,  5.9923e-06, -4.0097e-09,  1.3151e-06,\n",
      "          -3.6523e-09, -1.2073e-06,  1.5975e-07, -4.1334e-09,  1.3149e-06,\n",
      "          -1.9978e-07,  1.8486e-07, -1.6621e-07, -1.2257e-09,  1.2581e-05,\n",
      "           3.0089e-06, -6.8363e-08, -2.2873e-09,  4.8389e-07,  7.7758e-07,\n",
      "           9.1260e-08, -1.0340e-06, -6.4681e-07, -2.7343e-06,  6.2495e-07,\n",
      "           2.1049e-06, -2.5364e-07, -4.6147e-08, -8.3495e-07,  2.7527e-08,\n",
      "          -1.7728e-06, -2.1282e-05, -9.3445e-07,  5.4204e-07, -1.6911e-08,\n",
      "          -4.9603e-09,  6.9281e-06, -2.1246e-08,  2.5717e-06,  1.0584e-08,\n",
      "          -9.4637e-08,  2.0147e-06,  2.3032e-06,  2.3860e-06, -2.1552e-08,\n",
      "          -6.6082e-09,  1.6799e-05, -2.3743e-06,  1.1095e-08,  9.3728e-07,\n",
      "          -4.6303e-09, -4.5186e-09, -6.7962e-09, -1.7106e-08, -5.3569e-07,\n",
      "           3.9490e-08,  6.3466e-09, -3.9532e-06,  1.4323e-05,  1.4552e-09,\n",
      "          -4.7709e-06,  1.8672e-06, -2.3148e-06,  4.7150e-07,  5.4807e-07,\n",
      "          -6.5502e-07,  1.8981e-07,  2.8571e-09,  4.2063e-09,  3.1712e-08,\n",
      "          -5.6101e-07, -3.3603e-07, -4.8259e-06, -1.1903e-07, -1.2574e-05,\n",
      "          -1.0291e-08,  5.6589e-06, -3.3860e-07, -1.4214e-07, -1.1464e-06,\n",
      "           1.4969e-05, -9.9780e-06,  4.2101e-08, -4.2471e-07, -1.9702e-06,\n",
      "          -1.4949e-08,  6.7702e-08, -3.3593e-08,  1.7504e-06, -4.4923e-06,\n",
      "           7.6709e-06, -3.6925e-04, -1.0527e-07, -3.6139e-07,  2.1184e-07,\n",
      "           2.2556e-07,  1.1836e-07,  2.1052e-09, -1.2651e-08, -6.2661e-06,\n",
      "          -9.0347e-06, -6.6332e-09,  2.4092e-06,  1.0344e-06, -9.1272e-08,\n",
      "           7.9553e-07,  5.8131e-08,  3.6408e-07, -6.0187e-07, -2.8670e-06,\n",
      "           2.0006e-07,  8.7373e-09, -2.4830e-06,  9.6273e-06,  6.2010e-07,\n",
      "          -9.2130e-08,  4.0188e-06,  4.9999e-08, -2.8333e-09, -7.3779e-09,\n",
      "          -1.2982e-06, -5.7213e-07, -2.5262e-06,  2.0549e-06,  1.2422e-05,\n",
      "           3.0170e-09,  6.0589e-08,  1.7087e-07, -1.6573e-06,  5.5917e-08,\n",
      "          -7.3781e-06,  6.5990e-06,  1.4642e-06, -4.5863e-07, -2.4139e-08,\n",
      "          -1.1933e-06, -3.2544e-08, -1.3277e-07, -1.0180e-07, -3.4549e-07,\n",
      "          -4.7493e-07,  3.0314e-06, -1.9502e-06,  1.6607e-07, -1.4059e-07,\n",
      "          -3.2411e-06, -1.2351e-06,  1.3018e-06, -5.1746e-06, -6.0567e-09,\n",
      "           8.0995e-09, -2.5051e-07,  6.3932e-08, -7.1051e-06,  6.2887e-06,\n",
      "           1.5063e-06, -1.3608e-08, -5.9592e-07, -1.0121e-06,  2.2344e-06,\n",
      "          -9.7173e-09,  5.5005e-06,  8.0114e-07, -1.3357e-05, -1.6829e-08,\n",
      "           5.2020e-07, -1.0246e-08, -3.6627e-07, -6.9186e-07,  1.5120e-06,\n",
      "          -1.1171e-08, -8.9179e-08,  3.1674e-06, -3.2056e-08,  7.5366e-08,\n",
      "           3.3973e-08,  7.3093e-06,  6.1867e-07, -2.1678e-07, -3.1126e-08,\n",
      "          -1.4283e-06,  8.8902e-08, -4.0483e-06, -1.8835e-05,  2.0241e-07,\n",
      "           6.6565e-08,  2.4728e-07, -4.9105e-09, -3.6491e-07,  4.1599e-09,\n",
      "          -6.8935e-08,  4.7498e-07, -1.4405e-09, -2.3634e-08, -6.7978e-06,\n",
      "           6.5777e-10, -2.2546e-07, -1.1316e-06, -2.4625e-06,  2.6516e-09,\n",
      "           4.7425e-08, -9.3632e-06,  5.2651e-06,  3.2771e-07, -4.6329e-09,\n",
      "          -1.5464e-08, -6.1605e-10,  1.9835e-06,  2.1300e-08, -2.4945e-09,\n",
      "           9.7230e-06, -5.8208e-06,  6.9670e-08, -1.6276e-06, -7.9235e-06,\n",
      "           4.2358e-07, -9.8639e-08, -4.0145e-07,  4.9376e-07, -6.1349e-09,\n",
      "           5.2211e-09,  1.1231e-06, -1.9232e-08,  8.3668e-08, -2.0639e-08,\n",
      "           3.2266e-06,  4.5991e-06,  5.6453e-07, -1.1690e-08,  3.1363e-06,\n",
      "           7.9319e-07,  6.3972e-07,  2.4431e-06,  1.3691e-07,  3.4397e-07,\n",
      "           5.9716e-07, -6.7640e-08,  4.2014e-07, -1.2256e-07, -1.9971e-05,\n",
      "           4.1802e-09,  7.2193e-07,  1.4406e-07, -1.3008e-08,  7.8154e-09,\n",
      "           1.9151e-06,  7.1253e-07,  1.8491e-07, -3.6584e-08, -7.2075e-07,\n",
      "           5.4406e-09, -2.8820e-07, -1.0357e-05, -1.2787e-05, -2.7474e-09,\n",
      "           4.3042e-08,  9.2539e-08,  1.1078e-06,  4.8793e-07,  1.0743e-08,\n",
      "          -4.2308e-07, -1.2427e-08, -1.7520e-07,  3.1959e-06,  3.0790e-08,\n",
      "           8.1962e-07,  1.5018e-06,  3.3025e-07,  1.9033e-06, -2.8258e-05,\n",
      "          -4.0832e-06,  3.2066e-06, -1.3506e-09, -1.7938e-08, -9.5366e-06,\n",
      "           4.1767e-06,  9.4760e-08,  7.0537e-09, -5.8291e-08,  4.0933e-06,\n",
      "           9.3245e-08,  1.4259e-08, -8.7811e-09,  2.0307e-08, -2.2486e-07,\n",
      "           3.6306e-06,  5.9481e-09, -6.3747e-07, -5.8849e-07, -3.4337e-08,\n",
      "           1.4729e-06,  1.7147e-08, -5.9682e-08, -1.2062e-06, -1.2547e-07,\n",
      "          -2.1901e-06, -1.6813e-06,  2.1180e-07,  6.2201e-08,  1.6180e-06,\n",
      "          -1.4157e-05,  1.2282e-05,  1.9979e-09, -1.0967e-09,  1.4225e-07,\n",
      "          -4.2317e-09, -7.5325e-06, -5.8693e-06, -4.7341e-06,  1.6854e-08,\n",
      "          -4.6850e-08,  3.3433e-09, -1.1984e-08, -1.7735e-08, -9.4798e-07,\n",
      "          -1.2299e-06, -2.9764e-08, -1.1842e-07, -6.1587e-06,  7.0409e-08,\n",
      "           6.7254e-08, -2.3478e-06, -2.6517e-07,  2.7608e-06,  2.3073e-09,\n",
      "           7.2569e-08, -7.0159e-10,  3.1021e-07,  2.9071e-08,  4.6543e-09,\n",
      "           1.9451e-07, -1.7354e-09,  1.3059e-07,  2.5021e-09, -6.0248e-08,\n",
      "           1.3928e-07, -1.6684e-07,  4.6277e-07,  1.2854e-07, -5.1473e-06,\n",
      "           1.0356e-07, -1.8853e-06,  8.8526e-10,  2.5783e-07, -3.5606e-06,\n",
      "           1.6781e-05,  3.0114e-06, -4.5087e-07, -9.2964e-08, -1.4801e-05,\n",
      "           1.2632e-07, -1.3177e-05, -5.8259e-07,  2.0617e-05,  4.9409e-07,\n",
      "           6.1154e-08,  5.1906e-06,  1.2295e-05, -5.4655e-07, -6.9090e-07,\n",
      "           3.7675e-06,  5.6017e-09, -2.4236e-08, -6.7151e-08, -8.4430e-09,\n",
      "          -7.9482e-07,  8.9041e-08,  5.2378e-08, -3.9485e-06, -1.8214e-06,\n",
      "          -1.7094e-07,  1.1340e-07, -1.3492e-06, -4.8415e-06,  1.4066e-07,\n",
      "          -4.3767e-07, -4.1373e-08,  7.2860e-08, -8.9656e-08, -4.6104e-09,\n",
      "          -7.4272e-07, -1.5907e-05,  3.6720e-06,  1.5925e-06,  5.7099e-08,\n",
      "           1.7195e-09,  2.5247e-06, -4.4720e-08,  5.2508e-08,  5.4985e-08,\n",
      "           7.8396e-08,  2.1274e-06,  3.1347e-07,  8.5338e-08, -2.8651e-07,\n",
      "          -1.0325e-07, -1.7460e-06,  3.5058e-06, -3.0202e-06,  2.8637e-07,\n",
      "          -4.5763e-07, -6.7738e-06, -3.2277e-07, -2.2951e-08, -2.5471e-07,\n",
      "          -2.1949e-09,  1.7677e-06,  6.9713e-08, -5.6668e-07, -1.1175e-07,\n",
      "           1.0015e-07,  6.1517e-07, -1.1322e-08, -2.1065e-06,  2.4826e-08,\n",
      "           4.2148e-06, -6.9230e-09, -8.9478e-08, -7.0129e-08,  3.3094e-09,\n",
      "           2.1308e-06, -1.3334e-08, -2.2185e-06,  5.5710e-08,  2.2817e-08,\n",
      "          -5.1039e-09,  5.7348e-06, -2.1602e-07,  8.8378e-09, -2.1920e-06,\n",
      "          -1.5670e-07, -7.1959e-09,  1.7429e-07,  3.3435e-07,  1.8197e-07,\n",
      "          -6.0268e-06, -4.1070e-08, -9.2149e-06, -1.2947e-07, -2.9009e-06,\n",
      "           4.0414e-06,  4.2962e-06,  1.2600e-07,  5.2677e-08, -2.4173e-05,\n",
      "          -1.6576e-06, -1.0721e-08, -5.0578e-07,  2.3554e-08, -6.2507e-06,\n",
      "           7.5529e-07, -2.0994e-05,  2.9447e-06,  6.8881e-08,  4.8714e-07,\n",
      "          -7.9799e-07,  3.2075e-07, -4.6073e-06]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1839e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9462e-07,\n",
      "           3.1838e-07, -3.4247e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5569e-07,  ..., -7.9456e-07,\n",
      "           3.1840e-07, -3.4248e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5567e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4248e-06]]], device='cuda:0'), 'cls_feats': tensor([[-6.2369e-02,  6.7438e-02, -6.7392e-02, -3.9794e-02, -6.3808e-02,\n",
      "         -6.7367e-02, -6.6934e-02, -6.7346e-02,  6.7608e-02,  5.9141e-02,\n",
      "          6.7032e-02,  6.7446e-02,  6.5307e-02, -1.1194e-06,  1.1023e-07,\n",
      "          3.8261e-02,  6.6431e-02, -6.3736e-02,  6.7383e-02, -6.7404e-02,\n",
      "          6.4508e-02,  6.7620e-02,  6.7284e-02, -6.6964e-02,  6.2817e-02,\n",
      "          6.7301e-02, -6.7417e-02,  6.7087e-02,  6.6913e-02,  6.7109e-02,\n",
      "         -6.7469e-02,  2.8371e-02,  4.2128e-02, -1.6269e-03,  6.7079e-02,\n",
      "          6.6979e-02, -6.6333e-02, -6.7239e-02, -6.6659e-02,  1.1936e-08,\n",
      "          2.0455e-03,  6.4305e-02, -6.7023e-02, -6.6528e-02,  6.7345e-02,\n",
      "         -6.7515e-02, -6.7175e-02, -6.7290e-02, -6.2325e-02,  6.7243e-02,\n",
      "          6.6173e-02, -6.7563e-02, -6.7561e-02, -6.6837e-02, -6.7201e-02,\n",
      "          6.3929e-02,  6.6990e-02,  6.7497e-02,  6.7174e-02, -6.2646e-02,\n",
      "         -6.7547e-02,  6.8752e-08,  6.7104e-02, -6.5181e-02, -6.7592e-02,\n",
      "          3.9844e-03,  3.7063e-02, -6.4575e-02,  3.7483e-02,  6.6982e-02,\n",
      "          6.7271e-02, -4.6332e-09,  6.7342e-02,  6.2820e-02, -4.1235e-02,\n",
      "         -6.7368e-02, -6.6958e-02, -6.7113e-02,  6.7086e-02, -6.6908e-02,\n",
      "          6.7395e-02,  6.7414e-02, -2.9495e-03,  6.7512e-02,  6.1731e-02,\n",
      "         -2.2837e-09,  6.7376e-02,  6.2175e-02, -6.6956e-02,  6.4848e-02,\n",
      "          1.7535e-02,  3.6893e-02, -6.5943e-02,  6.4621e-02, -6.2865e-02,\n",
      "          6.2686e-02,  6.7344e-02,  6.7112e-02, -6.3725e-02, -6.7618e-02,\n",
      "         -4.3014e-02, -6.7607e-02, -7.1798e-07, -6.6797e-02, -6.7291e-02,\n",
      "         -2.3954e-07,  6.3011e-08, -1.6364e-08,  6.7339e-02, -6.7345e-02,\n",
      "         -7.7168e-03, -6.6899e-02, -6.7301e-02,  6.1825e-02,  1.2671e-03,\n",
      "          6.4608e-02, -2.9232e-04, -2.4108e-02, -6.7459e-02,  6.0292e-07,\n",
      "          6.4932e-02, -6.3537e-02,  1.7254e-05,  6.4727e-02,  6.7343e-02,\n",
      "         -7.2030e-10,  5.9043e-02,  6.7256e-02,  6.7341e-02, -6.6580e-02,\n",
      "          2.3766e-07, -6.7408e-02, -6.7751e-07, -6.3785e-02, -9.5706e-09,\n",
      "         -6.5586e-02,  6.4491e-02,  6.7181e-02,  2.6073e-02,  6.7416e-02,\n",
      "         -7.2806e-11, -1.4449e-03,  8.7101e-04, -6.5215e-02,  6.6613e-02,\n",
      "          1.3377e-03,  6.6549e-02,  6.7473e-02, -6.3610e-02,  6.6497e-02,\n",
      "         -6.6815e-02,  6.7569e-02,  6.7249e-02,  2.1300e-09, -6.7339e-02,\n",
      "         -7.5100e-06, -6.3818e-02, -6.7483e-02, -6.4382e-02, -6.7273e-02,\n",
      "          3.0603e-02, -6.7539e-02,  5.7865e-02, -6.2802e-02, -6.6204e-02,\n",
      "          6.4873e-02,  6.7604e-02, -6.2256e-02,  6.6797e-02, -6.7126e-02,\n",
      "          3.8525e-08, -5.7044e-02, -6.4657e-03, -6.4590e-02, -6.7163e-02,\n",
      "          5.8586e-08,  6.7447e-02, -6.7242e-02,  3.4424e-09,  6.1683e-02,\n",
      "         -2.3804e-02,  6.7520e-02,  6.6989e-02, -6.7084e-02,  6.7247e-02,\n",
      "         -6.4425e-02,  5.5586e-02, -7.8754e-13, -6.5376e-02,  6.7446e-02,\n",
      "          6.7162e-02,  6.5131e-02, -6.4461e-02,  6.7130e-02, -6.2440e-02,\n",
      "          6.7013e-02,  6.3599e-02,  6.6932e-02,  6.6517e-02,  6.7303e-02,\n",
      "         -6.7004e-02, -6.7336e-02,  6.7429e-02,  6.7268e-02,  7.7366e-08,\n",
      "         -5.0665e-02, -6.6271e-02,  6.7131e-02,  6.6349e-02, -6.6659e-02,\n",
      "         -6.4530e-02, -6.7165e-02,  4.9314e-02, -6.7408e-02,  6.6034e-05,\n",
      "         -8.6171e-07, -6.7051e-02,  6.4380e-02, -6.7154e-02,  6.2915e-02,\n",
      "          7.6205e-06,  3.6394e-02,  6.7509e-02, -6.3362e-02,  3.6703e-02,\n",
      "          6.5037e-02,  6.7369e-02,  6.7092e-02,  6.7445e-02,  6.7513e-02,\n",
      "          6.5877e-02, -6.9070e-09,  6.3248e-02, -6.7217e-02, -6.7186e-02,\n",
      "          6.7350e-02, -2.8146e-03, -6.5798e-02,  6.4767e-02,  6.7612e-02,\n",
      "          6.5310e-02,  6.4819e-02,  6.6173e-02,  6.7063e-02, -2.7954e-02,\n",
      "          6.6891e-02, -2.4323e-03,  6.6283e-02, -3.5880e-02,  6.6900e-02,\n",
      "          6.6905e-02,  6.7645e-02,  5.3848e-04,  6.2569e-02,  6.3214e-02,\n",
      "         -6.7352e-02, -5.6301e-04, -3.8800e-05, -6.7625e-02,  6.6964e-02,\n",
      "         -6.6986e-02, -6.7418e-02,  6.1452e-02,  1.4334e-07, -6.7413e-02,\n",
      "         -6.0843e-02,  6.7342e-02,  6.4143e-02,  6.5083e-02,  6.4727e-02,\n",
      "          6.6866e-09, -6.7189e-02,  5.7467e-02, -6.6783e-02,  6.7541e-02,\n",
      "         -6.6345e-02,  6.3640e-02,  6.7420e-02,  6.7416e-02,  6.0693e-02,\n",
      "         -6.7499e-02, -6.4516e-02,  6.5031e-02,  6.7353e-02,  9.6023e-06,\n",
      "         -6.7508e-02, -6.3883e-03, -6.5111e-02,  2.7676e-02, -6.5434e-02,\n",
      "         -6.5638e-02,  3.1789e-02, -6.7198e-02, -6.7201e-02, -3.2726e-06,\n",
      "         -6.6969e-02, -6.6967e-02, -6.2186e-02, -6.6638e-02,  6.3329e-02,\n",
      "         -6.7157e-02, -6.7101e-02,  6.6704e-02,  6.4811e-02, -6.7486e-02,\n",
      "          6.7260e-02,  1.7546e-07, -5.8865e-05,  6.7277e-02,  7.4857e-07,\n",
      "          5.3150e-02,  5.9514e-02, -4.8562e-07, -6.6338e-02,  6.7382e-02,\n",
      "         -6.7597e-02,  6.7519e-02,  6.7017e-02, -6.6577e-02, -5.2690e-03,\n",
      "          3.9099e-08,  6.7325e-02,  6.7514e-02, -6.7615e-02,  6.3006e-02,\n",
      "         -6.7263e-02, -6.6503e-02, -6.7047e-02, -2.6836e-02, -6.4434e-02,\n",
      "         -6.7331e-02, -6.4508e-02,  6.5750e-02,  5.6114e-08, -6.7032e-02,\n",
      "          6.7208e-02,  9.6642e-05,  3.8482e-03, -6.4239e-02,  6.6331e-02,\n",
      "         -6.6616e-02,  6.8670e-08, -6.7178e-02, -3.6938e-02, -6.7429e-02,\n",
      "         -6.7554e-02,  6.2161e-02,  6.2668e-02,  3.7008e-02,  6.7574e-02,\n",
      "          6.7368e-02,  6.5898e-02,  3.7291e-02,  6.6727e-02, -6.3257e-02,\n",
      "         -6.7228e-02,  6.6562e-02,  6.1792e-02, -6.7257e-02, -2.5518e-02,\n",
      "         -6.0100e-02,  6.6731e-02, -6.4731e-02,  6.7266e-02,  1.8924e-08,\n",
      "         -6.7595e-02, -6.7123e-02, -6.7418e-02, -6.7345e-02, -6.6897e-02,\n",
      "          5.3282e-03, -6.2533e-02, -6.7356e-02,  6.7596e-02,  6.7384e-02,\n",
      "          6.5824e-02, -6.7330e-02, -6.7421e-02,  6.2648e-02,  6.1200e-02,\n",
      "         -6.0875e-03,  6.7457e-02,  6.7371e-02, -6.7288e-02,  6.6429e-02,\n",
      "         -6.2117e-02,  6.6923e-02,  7.2089e-07,  5.1562e-04,  4.7186e-03,\n",
      "         -6.7341e-02,  6.2035e-02,  6.5086e-02,  6.7276e-02,  6.6795e-02,\n",
      "         -1.8734e-03, -6.6711e-02, -6.4561e-02,  6.6985e-02,  6.7410e-02,\n",
      "          3.2877e-02,  3.8970e-02,  6.7305e-02,  6.7098e-02, -6.7430e-02,\n",
      "          6.7094e-02, -6.4319e-02,  6.1292e-02, -6.7501e-02, -6.4619e-02,\n",
      "         -6.7115e-02,  6.7135e-02, -6.6579e-02,  2.8509e-07, -6.7531e-02,\n",
      "         -6.4620e-02, -6.7388e-02,  6.7252e-02,  6.7096e-02, -6.7506e-02,\n",
      "          2.8633e-09, -6.5300e-02, -6.7263e-02, -2.4722e-04, -4.0494e-02,\n",
      "         -3.2954e-02,  6.5160e-02, -3.9293e-02, -6.6729e-02, -6.7242e-02,\n",
      "         -5.8362e-02,  6.7063e-02,  6.6790e-02,  6.7391e-02,  6.3057e-02,\n",
      "          6.7479e-02, -4.0264e-02,  6.6780e-02,  6.1174e-02,  8.4328e-05,\n",
      "          6.7444e-02, -6.4527e-02, -6.7261e-02, -1.2016e-08, -6.6994e-02,\n",
      "          6.7437e-02, -4.0952e-09, -6.7023e-02,  6.7428e-02,  6.4790e-02,\n",
      "         -5.2360e-07,  6.7501e-02, -3.3817e-05,  6.7234e-02,  6.7123e-02,\n",
      "          6.6683e-02,  1.6733e-04,  6.6854e-02,  6.7504e-02, -5.4591e-02,\n",
      "          6.7375e-02, -6.7458e-02, -1.0713e-07, -6.7394e-02, -2.3773e-05,\n",
      "         -6.2634e-02, -6.7195e-02,  6.6591e-02, -6.7483e-02, -6.6754e-02,\n",
      "         -6.7320e-02,  9.0393e-08, -6.7350e-02,  6.7033e-02, -5.5030e-02,\n",
      "         -5.5602e-02,  1.5041e-07,  6.6845e-02, -6.7374e-02, -6.3212e-02,\n",
      "          6.4717e-02,  6.6441e-02, -6.5841e-02, -6.0728e-03,  6.7371e-02,\n",
      "         -6.7430e-02,  6.4964e-02,  5.9442e-02, -6.7371e-02,  5.6901e-02,\n",
      "          6.4991e-02,  1.2980e-03, -6.7591e-02, -6.6276e-02,  3.6318e-02,\n",
      "          6.7483e-02, -6.5320e-02,  6.5499e-02,  6.6371e-02, -6.7563e-02,\n",
      "         -6.7270e-02,  5.6841e-07, -7.7628e-03, -6.7428e-02,  6.1854e-02,\n",
      "         -6.7351e-03,  6.7140e-02,  6.0360e-02,  3.2200e-02,  6.7526e-02,\n",
      "         -6.4536e-02, -1.1429e-03,  5.4412e-02,  6.6289e-02, -1.0021e-04,\n",
      "         -6.7169e-02,  6.3203e-03,  6.3468e-02,  5.4763e-02, -6.7518e-02,\n",
      "          6.4844e-02,  5.9107e-02,  6.4657e-02, -6.7161e-02,  6.7252e-02,\n",
      "         -6.3953e-02,  8.2347e-03,  6.5119e-02, -6.7534e-02, -6.7339e-02,\n",
      "         -6.7405e-02,  1.1444e-04,  6.7375e-02, -6.7574e-02,  6.7164e-02,\n",
      "          4.2723e-03, -5.0971e-02,  6.2805e-02, -6.7219e-02,  1.4702e-08,\n",
      "          6.7277e-02,  6.2599e-02, -6.7550e-02,  4.1347e-05, -6.7279e-02,\n",
      "          3.5698e-04,  6.0857e-02,  6.7389e-02,  6.4545e-02, -6.7400e-02,\n",
      "          1.0368e-03,  6.4676e-02, -6.2545e-02, -6.6346e-02, -6.5358e-02,\n",
      "         -6.7396e-02,  1.5532e-09,  1.0901e-06, -6.7390e-02, -6.7336e-02,\n",
      "          6.2665e-02,  6.4600e-02,  6.7060e-02, -6.4262e-02,  6.6794e-02,\n",
      "         -6.0619e-02,  6.7019e-02, -6.7066e-02, -6.4595e-02, -6.7044e-02,\n",
      "          6.6832e-02, -6.7367e-02, -6.7229e-02, -6.7443e-02,  6.7259e-02,\n",
      "          6.6007e-02,  2.4126e-06, -3.0179e-03,  6.6067e-02,  6.6460e-02,\n",
      "          6.7404e-02, -6.7397e-02, -1.5102e-08, -6.6566e-02,  5.9396e-02,\n",
      "         -6.5803e-02,  3.0325e-04,  6.6892e-02, -6.4522e-02, -6.7451e-02,\n",
      "         -6.7552e-02,  6.2458e-02, -3.3686e-02, -6.7554e-02,  5.6933e-07,\n",
      "         -6.6965e-02, -6.4766e-02,  6.5704e-02,  6.7033e-02, -6.7588e-02,\n",
      "         -6.5439e-02,  6.6992e-02, -6.5156e-02,  6.4610e-02,  6.2246e-02,\n",
      "          6.7464e-02, -1.0333e-08, -5.2990e-08,  6.7298e-02,  6.4427e-02,\n",
      "         -6.7270e-02, -6.7353e-02, -6.7473e-02, -6.2098e-02, -6.6256e-02,\n",
      "          6.7051e-02, -3.9792e-09,  5.0297e-06,  6.7651e-02, -6.7588e-02,\n",
      "          6.7137e-02, -6.5897e-02,  6.7459e-02, -6.7119e-02, -6.7142e-02,\n",
      "         -6.7032e-02,  1.4208e-03, -2.2531e-03, -3.4002e-05, -6.2485e-02,\n",
      "          6.7047e-02,  1.9950e-06,  6.2619e-02, -4.3295e-04,  6.6979e-02,\n",
      "         -4.1925e-02,  5.7241e-03, -6.7509e-02, -6.7074e-02, -6.7152e-02,\n",
      "         -4.0087e-02,  1.9418e-09,  6.7081e-02, -6.9224e-08,  7.1605e-03,\n",
      "         -6.4432e-02,  6.7467e-02,  6.7016e-02, -4.3124e-06,  6.7321e-02,\n",
      "         -6.7562e-02, -6.2809e-02, -6.7216e-02, -6.7424e-02,  2.5220e-03,\n",
      "          6.7233e-02, -6.7184e-02, -5.7422e-02,  6.7142e-02,  6.6692e-02,\n",
      "         -6.0657e-02,  2.6549e-05,  6.6478e-02, -6.6431e-02,  6.6683e-02,\n",
      "         -6.4142e-02, -4.8963e-05,  2.5485e-02, -1.1370e-03,  6.6871e-02,\n",
      "         -6.7483e-02, -6.7199e-02, -6.7092e-02, -6.7251e-02, -1.1900e-04,\n",
      "          6.6485e-02, -6.6237e-02, -6.2429e-02, -6.7234e-02,  6.7504e-02,\n",
      "          6.7331e-02,  6.5897e-02,  6.7221e-02, -6.6987e-02,  6.6922e-02,\n",
      "         -6.7288e-02, -6.3721e-02,  6.5902e-02, -3.2436e-06,  1.7997e-09,\n",
      "          5.9391e-05,  6.6901e-02,  6.6926e-02, -6.7343e-02, -3.9708e-02,\n",
      "          6.6481e-02, -6.7066e-02, -6.7447e-02, -4.1179e-02, -6.6806e-02,\n",
      "          9.8530e-08, -6.6850e-02, -6.6807e-02,  3.3475e-02,  6.2364e-02,\n",
      "          6.7018e-02,  6.6879e-02,  6.5360e-02, -1.9073e-07, -6.6690e-02,\n",
      "          5.6255e-02,  6.6337e-02,  6.7206e-02, -6.7079e-02, -3.2966e-07,\n",
      "         -6.2678e-02, -6.7446e-02, -6.7237e-02,  6.6416e-02, -6.7501e-02,\n",
      "         -6.7539e-02, -6.3749e-02,  6.4719e-02,  6.4697e-02,  6.6729e-02,\n",
      "         -6.3290e-02, -6.7158e-02, -6.6782e-02,  3.0375e-02,  6.7004e-02,\n",
      "         -4.9525e-06,  6.7568e-02, -1.1982e-06,  6.2767e-02, -6.7334e-02,\n",
      "          6.7172e-02, -6.7377e-02,  5.8678e-02,  6.4428e-02, -6.4449e-02,\n",
      "          6.5896e-02,  6.2609e-02, -6.6776e-02,  6.2950e-02,  5.0737e-03,\n",
      "         -6.7092e-02,  6.7201e-02, -3.7448e-03,  6.7109e-02, -5.7745e-03,\n",
      "         -6.7313e-02, -6.1316e-02, -6.7416e-02, -6.7189e-02, -6.4288e-03,\n",
      "          6.6084e-02,  6.6805e-02, -5.5808e-05,  6.4505e-02,  6.7553e-02,\n",
      "          6.7108e-02,  6.7221e-02, -6.7303e-02, -6.7451e-02, -2.6494e-03,\n",
      "         -2.3323e-09,  6.7276e-02, -6.7297e-02]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0827e-06, -1.7530e-07,  1.5452e-07, -2.2674e-08, -2.2496e-06,\n",
      "          3.2150e-06,  6.0465e-06, -5.3801e-09, -1.8847e-07, -7.7690e-07,\n",
      "         -1.5788e-07,  3.6213e-06, -3.8558e-08, -1.0063e-05, -1.4680e-08,\n",
      "         -1.8126e-07, -2.7730e-06,  9.1713e-09, -2.7553e-06, -5.3852e-09,\n",
      "          9.5103e-07,  3.8168e-09,  7.2278e-08, -1.0654e-08,  6.3138e-07,\n",
      "          3.1689e-06, -2.6612e-06, -1.0689e-09,  2.8926e-06,  4.4980e-06,\n",
      "         -9.2651e-06,  1.0652e-06, -4.1821e-07,  2.2096e-08, -9.7153e-07,\n",
      "          1.0851e-06,  7.0076e-08, -7.6146e-08, -2.9167e-09,  2.1692e-06,\n",
      "          7.3493e-08,  8.4667e-06,  3.4533e-08, -2.6099e-07, -4.6151e-06,\n",
      "          1.3888e-08,  3.3025e-07, -5.3568e-06, -2.0778e-06, -2.7470e-06,\n",
      "          1.0874e-07, -4.3026e-06, -3.1775e-08, -1.3380e-09, -2.3559e-06,\n",
      "         -1.7084e-09, -9.4255e-06,  2.6611e-06,  7.8697e-07, -5.3748e-08,\n",
      "          1.0649e-05, -2.6270e-07, -3.8041e-07,  1.9315e-08, -7.3593e-08,\n",
      "         -4.4816e-06,  5.6049e-10,  8.2062e-07, -1.0063e-08,  4.5128e-08,\n",
      "         -5.1747e-09, -4.5403e-07, -2.9219e-08,  7.7394e-07,  2.2411e-07,\n",
      "         -3.3084e-07,  2.4516e-09,  3.5821e-10, -1.8521e-09,  1.8354e-07,\n",
      "         -1.8954e-05,  2.7999e-10, -8.5999e-08, -2.1377e-06, -1.6716e-06,\n",
      "          1.6572e-08, -2.3315e-06,  1.8895e-07,  6.3496e-06, -3.1833e-07,\n",
      "          6.2023e-07,  8.4298e-10, -8.2967e-07,  3.7212e-06,  1.1039e-06,\n",
      "         -9.0300e-09, -2.6949e-08, -8.5791e-10, -1.1635e-08, -7.3959e-09,\n",
      "         -9.7975e-06,  4.5500e-07, -5.4868e-06,  4.2644e-06,  3.0767e-06,\n",
      "          4.9342e-06,  4.9311e-07,  1.9163e-05,  2.2566e-08, -1.8834e-08,\n",
      "          2.0761e-06,  2.9012e-06,  3.2295e-07, -7.7596e-08,  1.3192e-07,\n",
      "          5.7596e-06, -3.6992e-05, -3.0007e-07,  4.7662e-08, -3.7218e-07,\n",
      "         -5.0134e-06,  3.3447e-08, -4.8486e-06,  5.2302e-07,  3.9796e-06,\n",
      "          6.6157e-06,  1.0902e-06,  2.3589e-08,  1.0390e-07,  1.5728e-07,\n",
      "          1.2887e-05, -8.6768e-07, -2.0269e-07,  7.0103e-07,  1.9808e-05,\n",
      "          9.1876e-08, -1.8358e-07, -3.2414e-05, -8.2112e-06,  4.8515e-08,\n",
      "         -1.7336e-05,  3.3777e-06, -4.5661e-07,  3.0050e-06,  1.6503e-09,\n",
      "         -7.2160e-09,  7.9344e-07, -5.0495e-09,  2.4069e-07, -4.3610e-06,\n",
      "          2.7735e-06,  2.7815e-07, -2.0350e-07,  1.6112e-08, -2.0510e-06,\n",
      "         -4.1556e-06, -1.0095e-06, -4.1982e-06,  9.7768e-06,  3.5272e-08,\n",
      "          4.1931e-06,  3.6369e-07, -7.5657e-08,  1.3493e-06, -9.1426e-06,\n",
      "          1.5492e-06, -5.5606e-08,  2.8326e-10, -6.2274e-06,  1.1553e-07,\n",
      "          1.3244e-07,  1.1595e-08,  1.0701e-07, -1.7686e-10,  1.3434e-06,\n",
      "          7.6663e-07,  3.0214e-06,  3.1273e-06,  1.8162e-07, -8.7588e-09,\n",
      "         -5.3203e-08,  1.3994e-06, -3.9140e-07, -5.9820e-06,  1.8577e-06,\n",
      "         -2.9392e+00,  2.7478e-05,  1.7482e-07, -1.0052e-06,  1.1428e-06,\n",
      "          3.5725e-06,  1.6826e-08, -3.2411e-08,  1.3470e-06,  9.9076e-08,\n",
      "         -9.7098e-06, -3.4484e-06,  5.2059e-07,  9.9433e-09, -1.4315e-06,\n",
      "          1.1571e-07, -1.0162e-06, -1.9451e-06,  1.3364e-07,  2.0557e-09,\n",
      "          1.8360e-05,  2.3615e-09,  1.5429e-06, -1.3612e-05, -1.3375e-08,\n",
      "         -1.1263e-05,  6.5371e-06,  3.0841e-06,  2.1411e-09,  6.0650e-08,\n",
      "         -3.8018e-06, -2.6458e-07,  3.9600e-09, -1.6493e-09, -4.5734e-07,\n",
      "          1.6022e-09, -3.6367e-09,  8.5293e-07, -3.3017e-08,  1.1829e-05,\n",
      "         -3.8856e-08, -7.8512e-08, -1.2441e-07, -8.3563e-07,  3.5845e-07,\n",
      "         -1.1632e-06, -3.2378e-05,  5.1766e-08, -3.2791e-07, -1.0741e-08,\n",
      "         -4.8005e-06, -3.0898e-07,  2.0715e-06,  1.1634e-07,  1.9251e-07,\n",
      "         -6.8956e-08,  1.6680e-06,  3.3701e-07,  6.6928e-09, -3.8224e-08,\n",
      "          2.0501e-05,  4.8596e-07,  6.9212e-08, -3.0852e-07,  2.6557e-09,\n",
      "          1.1923e-07, -8.0101e-09,  7.0794e-08, -3.8855e-07, -1.8652e-07,\n",
      "         -2.3786e-06, -1.5048e-08,  1.0523e-08,  1.7478e-07, -1.3772e-08,\n",
      "          1.3210e-06, -1.8784e-06,  2.4997e-06,  3.5084e-07,  3.1842e-10,\n",
      "          9.0063e-09, -7.8309e-07,  5.2683e-06,  1.2784e-05, -4.2034e-08,\n",
      "         -6.3294e-09, -2.5147e-05,  2.9737e-08, -3.2164e-08, -3.1089e-06,\n",
      "         -6.6904e-08, -1.1640e-05, -8.9892e-10, -1.2145e-06,  1.5709e-06,\n",
      "          1.2708e-05,  6.3839e-08,  2.4791e-07,  7.2218e-08, -1.3138e-06,\n",
      "          1.6254e-08,  1.1444e-07, -6.5649e-06, -3.8067e-09,  6.3245e-07,\n",
      "          2.2508e-07, -3.5040e-06,  7.8615e-06,  2.3449e-08, -1.6639e-08,\n",
      "         -1.5493e-08,  1.7303e-08,  4.1829e-06,  3.4123e-07, -7.8461e-07,\n",
      "          1.5215e-07, -2.0604e-06,  6.5877e-07, -1.1104e-06,  1.2191e-06,\n",
      "          8.6716e-07, -1.7774e-08, -2.4495e-07,  1.7908e-06, -1.5804e-06,\n",
      "         -8.3130e-07, -3.4135e-07,  5.9923e-06, -4.0097e-09,  1.3151e-06,\n",
      "         -3.6523e-09, -1.2073e-06,  1.5975e-07, -4.1334e-09,  1.3149e-06,\n",
      "         -1.9978e-07,  1.8486e-07, -1.6621e-07, -1.2257e-09,  1.2581e-05,\n",
      "          3.0089e-06, -6.8363e-08, -2.2873e-09,  4.8389e-07,  7.7758e-07,\n",
      "          9.1260e-08, -1.0340e-06, -6.4681e-07, -2.7343e-06,  6.2495e-07,\n",
      "          2.1049e-06, -2.5364e-07, -4.6147e-08, -8.3495e-07,  2.7527e-08,\n",
      "         -1.7728e-06, -2.1282e-05, -9.3445e-07,  5.4204e-07, -1.6911e-08,\n",
      "         -4.9603e-09,  6.9281e-06, -2.1246e-08,  2.5717e-06,  1.0584e-08,\n",
      "         -9.4637e-08,  2.0147e-06,  2.3032e-06,  2.3860e-06, -2.1552e-08,\n",
      "         -6.6082e-09,  1.6799e-05, -2.3743e-06,  1.1095e-08,  9.3728e-07,\n",
      "         -4.6303e-09, -4.5186e-09, -6.7962e-09, -1.7106e-08, -5.3569e-07,\n",
      "          3.9490e-08,  6.3466e-09, -3.9532e-06,  1.4323e-05,  1.4552e-09,\n",
      "         -4.7709e-06,  1.8672e-06, -2.3148e-06,  4.7150e-07,  5.4807e-07,\n",
      "         -6.5502e-07,  1.8981e-07,  2.8571e-09,  4.2063e-09,  3.1712e-08,\n",
      "         -5.6101e-07, -3.3603e-07, -4.8259e-06, -1.1903e-07, -1.2574e-05,\n",
      "         -1.0291e-08,  5.6589e-06, -3.3860e-07, -1.4214e-07, -1.1464e-06,\n",
      "          1.4969e-05, -9.9780e-06,  4.2101e-08, -4.2471e-07, -1.9702e-06,\n",
      "         -1.4949e-08,  6.7702e-08, -3.3593e-08,  1.7504e-06, -4.4923e-06,\n",
      "          7.6709e-06, -3.6925e-04, -1.0527e-07, -3.6139e-07,  2.1184e-07,\n",
      "          2.2556e-07,  1.1836e-07,  2.1052e-09, -1.2651e-08, -6.2661e-06,\n",
      "         -9.0347e-06, -6.6332e-09,  2.4092e-06,  1.0344e-06, -9.1272e-08,\n",
      "          7.9553e-07,  5.8131e-08,  3.6408e-07, -6.0187e-07, -2.8670e-06,\n",
      "          2.0006e-07,  8.7373e-09, -2.4830e-06,  9.6273e-06,  6.2010e-07,\n",
      "         -9.2130e-08,  4.0188e-06,  4.9999e-08, -2.8333e-09, -7.3779e-09,\n",
      "         -1.2982e-06, -5.7213e-07, -2.5262e-06,  2.0549e-06,  1.2422e-05,\n",
      "          3.0170e-09,  6.0589e-08,  1.7087e-07, -1.6573e-06,  5.5917e-08,\n",
      "         -7.3781e-06,  6.5990e-06,  1.4642e-06, -4.5863e-07, -2.4139e-08,\n",
      "         -1.1933e-06, -3.2544e-08, -1.3277e-07, -1.0180e-07, -3.4549e-07,\n",
      "         -4.7493e-07,  3.0314e-06, -1.9502e-06,  1.6607e-07, -1.4059e-07,\n",
      "         -3.2411e-06, -1.2351e-06,  1.3018e-06, -5.1746e-06, -6.0567e-09,\n",
      "          8.0995e-09, -2.5051e-07,  6.3932e-08, -7.1051e-06,  6.2887e-06,\n",
      "          1.5063e-06, -1.3608e-08, -5.9592e-07, -1.0121e-06,  2.2344e-06,\n",
      "         -9.7173e-09,  5.5005e-06,  8.0114e-07, -1.3357e-05, -1.6829e-08,\n",
      "          5.2020e-07, -1.0246e-08, -3.6627e-07, -6.9186e-07,  1.5120e-06,\n",
      "         -1.1171e-08, -8.9179e-08,  3.1674e-06, -3.2056e-08,  7.5366e-08,\n",
      "          3.3973e-08,  7.3093e-06,  6.1867e-07, -2.1678e-07, -3.1126e-08,\n",
      "         -1.4283e-06,  8.8902e-08, -4.0483e-06, -1.8835e-05,  2.0241e-07,\n",
      "          6.6565e-08,  2.4728e-07, -4.9105e-09, -3.6491e-07,  4.1599e-09,\n",
      "         -6.8935e-08,  4.7498e-07, -1.4405e-09, -2.3634e-08, -6.7978e-06,\n",
      "          6.5777e-10, -2.2546e-07, -1.1316e-06, -2.4625e-06,  2.6516e-09,\n",
      "          4.7425e-08, -9.3632e-06,  5.2651e-06,  3.2771e-07, -4.6329e-09,\n",
      "         -1.5464e-08, -6.1605e-10,  1.9835e-06,  2.1300e-08, -2.4945e-09,\n",
      "          9.7230e-06, -5.8208e-06,  6.9670e-08, -1.6276e-06, -7.9235e-06,\n",
      "          4.2358e-07, -9.8639e-08, -4.0145e-07,  4.9376e-07, -6.1349e-09,\n",
      "          5.2211e-09,  1.1231e-06, -1.9232e-08,  8.3668e-08, -2.0639e-08,\n",
      "          3.2266e-06,  4.5991e-06,  5.6453e-07, -1.1690e-08,  3.1363e-06,\n",
      "          7.9319e-07,  6.3972e-07,  2.4431e-06,  1.3691e-07,  3.4397e-07,\n",
      "          5.9716e-07, -6.7640e-08,  4.2014e-07, -1.2256e-07, -1.9971e-05,\n",
      "          4.1802e-09,  7.2193e-07,  1.4406e-07, -1.3008e-08,  7.8154e-09,\n",
      "          1.9151e-06,  7.1253e-07,  1.8491e-07, -3.6584e-08, -7.2075e-07,\n",
      "          5.4406e-09, -2.8820e-07, -1.0357e-05, -1.2787e-05, -2.7474e-09,\n",
      "          4.3042e-08,  9.2539e-08,  1.1078e-06,  4.8793e-07,  1.0743e-08,\n",
      "         -4.2308e-07, -1.2427e-08, -1.7520e-07,  3.1959e-06,  3.0790e-08,\n",
      "          8.1962e-07,  1.5018e-06,  3.3025e-07,  1.9033e-06, -2.8258e-05,\n",
      "         -4.0832e-06,  3.2066e-06, -1.3506e-09, -1.7938e-08, -9.5366e-06,\n",
      "          4.1767e-06,  9.4760e-08,  7.0537e-09, -5.8291e-08,  4.0933e-06,\n",
      "          9.3245e-08,  1.4259e-08, -8.7811e-09,  2.0307e-08, -2.2486e-07,\n",
      "          3.6306e-06,  5.9481e-09, -6.3747e-07, -5.8849e-07, -3.4337e-08,\n",
      "          1.4729e-06,  1.7147e-08, -5.9682e-08, -1.2062e-06, -1.2547e-07,\n",
      "         -2.1901e-06, -1.6813e-06,  2.1180e-07,  6.2201e-08,  1.6180e-06,\n",
      "         -1.4157e-05,  1.2282e-05,  1.9979e-09, -1.0967e-09,  1.4225e-07,\n",
      "         -4.2317e-09, -7.5325e-06, -5.8693e-06, -4.7341e-06,  1.6854e-08,\n",
      "         -4.6850e-08,  3.3433e-09, -1.1984e-08, -1.7735e-08, -9.4798e-07,\n",
      "         -1.2299e-06, -2.9764e-08, -1.1842e-07, -6.1587e-06,  7.0409e-08,\n",
      "          6.7254e-08, -2.3478e-06, -2.6517e-07,  2.7608e-06,  2.3073e-09,\n",
      "          7.2569e-08, -7.0159e-10,  3.1021e-07,  2.9071e-08,  4.6543e-09,\n",
      "          1.9451e-07, -1.7354e-09,  1.3059e-07,  2.5021e-09, -6.0248e-08,\n",
      "          1.3928e-07, -1.6684e-07,  4.6277e-07,  1.2854e-07, -5.1473e-06,\n",
      "          1.0356e-07, -1.8853e-06,  8.8526e-10,  2.5783e-07, -3.5606e-06,\n",
      "          1.6781e-05,  3.0114e-06, -4.5087e-07, -9.2964e-08, -1.4801e-05,\n",
      "          1.2632e-07, -1.3177e-05, -5.8259e-07,  2.0617e-05,  4.9409e-07,\n",
      "          6.1154e-08,  5.1906e-06,  1.2295e-05, -5.4655e-07, -6.9090e-07,\n",
      "          3.7675e-06,  5.6017e-09, -2.4236e-08, -6.7151e-08, -8.4430e-09,\n",
      "         -7.9482e-07,  8.9041e-08,  5.2378e-08, -3.9485e-06, -1.8214e-06,\n",
      "         -1.7094e-07,  1.1340e-07, -1.3492e-06, -4.8415e-06,  1.4066e-07,\n",
      "         -4.3767e-07, -4.1373e-08,  7.2860e-08, -8.9656e-08, -4.6104e-09,\n",
      "         -7.4272e-07, -1.5907e-05,  3.6720e-06,  1.5925e-06,  5.7099e-08,\n",
      "          1.7195e-09,  2.5247e-06, -4.4720e-08,  5.2508e-08,  5.4985e-08,\n",
      "          7.8396e-08,  2.1274e-06,  3.1347e-07,  8.5338e-08, -2.8651e-07,\n",
      "         -1.0325e-07, -1.7460e-06,  3.5058e-06, -3.0202e-06,  2.8637e-07,\n",
      "         -4.5763e-07, -6.7738e-06, -3.2277e-07, -2.2951e-08, -2.5471e-07,\n",
      "         -2.1949e-09,  1.7677e-06,  6.9713e-08, -5.6668e-07, -1.1175e-07,\n",
      "          1.0015e-07,  6.1517e-07, -1.1322e-08, -2.1065e-06,  2.4826e-08,\n",
      "          4.2148e-06, -6.9230e-09, -8.9478e-08, -7.0129e-08,  3.3094e-09,\n",
      "          2.1308e-06, -1.3334e-08, -2.2185e-06,  5.5710e-08,  2.2817e-08,\n",
      "         -5.1039e-09,  5.7348e-06, -2.1602e-07,  8.8378e-09, -2.1920e-06,\n",
      "         -1.5670e-07, -7.1959e-09,  1.7429e-07,  3.3435e-07,  1.8197e-07,\n",
      "         -6.0268e-06, -4.1070e-08, -9.2149e-06, -1.2947e-07, -2.9009e-06,\n",
      "          4.0414e-06,  4.2962e-06,  1.2600e-07,  5.2677e-08, -2.4173e-05,\n",
      "         -1.6576e-06, -1.0721e-08, -5.0578e-07,  2.3554e-08, -6.2507e-06,\n",
      "          7.5529e-07, -2.0994e-05,  2.9447e-06,  6.8881e-08,  4.8714e-07,\n",
      "         -7.9799e-07,  3.2075e-07, -4.6073e-06]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 0, 14],\n",
      "         [ 2, 12],\n",
      "         [10, 13],\n",
      "         [ 3, 18],\n",
      "         [ 9,  1],\n",
      "         [ 5,  2],\n",
      "         [10,  1],\n",
      "         [ 4, 12],\n",
      "         [ 7, 14],\n",
      "         [ 9,  5],\n",
      "         [ 4, 11],\n",
      "         [ 9, 12],\n",
      "         [ 9,  6],\n",
      "         [10, 18],\n",
      "         [ 8, 18],\n",
      "         [ 2,  5],\n",
      "         [10,  2],\n",
      "         [ 9,  8],\n",
      "         [ 2, 10],\n",
      "         [ 6, 16],\n",
      "         [ 9,  4],\n",
      "         [10, 10],\n",
      "         [ 7, 13],\n",
      "         [ 0, 16],\n",
      "         [ 0, 18],\n",
      "         [ 3, 12],\n",
      "         [ 3,  8],\n",
      "         [10,  9],\n",
      "         [ 6, 10],\n",
      "         [ 3, 13],\n",
      "         [ 8, 10],\n",
      "         [ 4,  7],\n",
      "         [ 0, 12],\n",
      "         [ 6,  4],\n",
      "         [ 8,  1],\n",
      "         [ 3,  7],\n",
      "         [ 2,  4],\n",
      "         [ 2,  7],\n",
      "         [ 5, 13],\n",
      "         [ 8, 15],\n",
      "         [ 4,  9],\n",
      "         [ 8, 17],\n",
      "         [ 6, 17],\n",
      "         [ 8, 12],\n",
      "         [ 1, 15],\n",
      "         [10, 12],\n",
      "         [10,  6],\n",
      "         [ 2,  9],\n",
      "         [ 8, 11],\n",
      "         [ 8, 13],\n",
      "         [ 4,  0],\n",
      "         [ 0, 15],\n",
      "         [ 8,  0],\n",
      "         [ 8,  4],\n",
      "         [ 0,  4],\n",
      "         [ 3, 17],\n",
      "         [ 8,  2],\n",
      "         [ 3,  6],\n",
      "         [ 9, 11],\n",
      "         [ 8,  3],\n",
      "         [ 4,  6],\n",
      "         [ 4,  8],\n",
      "         [10,  3],\n",
      "         [ 7, 11],\n",
      "         [10,  8],\n",
      "         [ 4,  2],\n",
      "         [ 6, 13],\n",
      "         [ 5, 11],\n",
      "         [ 0,  9],\n",
      "         [ 0,  7],\n",
      "         [ 1, 14],\n",
      "         [ 5, 12],\n",
      "         [ 5,  5],\n",
      "         [ 1, 13],\n",
      "         [ 8,  6],\n",
      "         [ 0, 11],\n",
      "         [ 9,  0],\n",
      "         [ 0, 13],\n",
      "         [ 1, 18],\n",
      "         [ 3,  9],\n",
      "         [ 2, 16],\n",
      "         [10, 16],\n",
      "         [ 1,  8],\n",
      "         [ 6, 18],\n",
      "         [ 1,  0],\n",
      "         [ 0,  8],\n",
      "         [ 5, 14],\n",
      "         [ 5,  4],\n",
      "         [ 6,  9],\n",
      "         [ 2, 17],\n",
      "         [ 9, 13],\n",
      "         [ 7,  7],\n",
      "         [ 7, 10],\n",
      "         [ 9, 18],\n",
      "         [ 2, 13],\n",
      "         [ 7,  1],\n",
      "         [10,  4],\n",
      "         [ 1,  7],\n",
      "         [ 7,  4],\n",
      "         [ 5, 17],\n",
      "         [ 0,  0],\n",
      "         [ 1, 17],\n",
      "         [ 3,  0],\n",
      "         [ 4,  3],\n",
      "         [ 4, 16],\n",
      "         [ 9, 15],\n",
      "         [ 7,  6],\n",
      "         [ 6,  3],\n",
      "         [ 1, 10],\n",
      "         [ 8,  5],\n",
      "         [ 5,  9],\n",
      "         [ 5, 16],\n",
      "         [ 9, 16],\n",
      "         [ 4, 10],\n",
      "         [ 3, 16],\n",
      "         [ 9, 17],\n",
      "         [ 5,  1],\n",
      "         [ 6, 15],\n",
      "         [ 9,  7],\n",
      "         [ 6, 12],\n",
      "         [ 6,  2],\n",
      "         [ 4, 17],\n",
      "         [ 1, 11],\n",
      "         [ 6,  5],\n",
      "         [ 3, 11],\n",
      "         [10, 11],\n",
      "         [ 1,  9],\n",
      "         [ 2,  8],\n",
      "         [ 7,  8],\n",
      "         [10,  0],\n",
      "         [ 3,  2],\n",
      "         [ 5, 10],\n",
      "         [ 0,  3],\n",
      "         [ 3, 10],\n",
      "         [ 4, 14],\n",
      "         [ 8,  9],\n",
      "         [ 2,  6],\n",
      "         [ 1, 12],\n",
      "         [ 1,  6],\n",
      "         [ 6,  8],\n",
      "         [ 7, 16],\n",
      "         [ 0,  6],\n",
      "         [ 7,  2],\n",
      "         [ 9, 10],\n",
      "         [ 8, 16],\n",
      "         [ 2,  1],\n",
      "         [ 1,  3],\n",
      "         [ 3,  1],\n",
      "         [ 3,  3],\n",
      "         [ 5, 18],\n",
      "         [ 4, 18],\n",
      "         [ 7, 18],\n",
      "         [ 8, 14],\n",
      "         [ 2, 15],\n",
      "         [ 5,  7],\n",
      "         [ 2,  0],\n",
      "         [ 9,  2],\n",
      "         [ 4,  1],\n",
      "         [ 6,  1],\n",
      "         [ 6, 11],\n",
      "         [ 9,  3],\n",
      "         [ 7, 12],\n",
      "         [ 2,  2],\n",
      "         [ 5,  8],\n",
      "         [ 5,  6],\n",
      "         [ 4, 13],\n",
      "         [ 1,  1],\n",
      "         [ 6,  6],\n",
      "         [10,  5],\n",
      "         [ 7, 17],\n",
      "         [10, 17],\n",
      "         [ 0,  5],\n",
      "         [ 1,  4],\n",
      "         [ 7,  9],\n",
      "         [ 6,  7],\n",
      "         [ 2,  3],\n",
      "         [ 2, 14],\n",
      "         [ 0, 17],\n",
      "         [ 2, 18],\n",
      "         [ 2, 11],\n",
      "         [ 0, 10],\n",
      "         [ 6, 14],\n",
      "         [ 4,  5],\n",
      "         [10, 15],\n",
      "         [ 0,  1],\n",
      "         [ 1,  2],\n",
      "         [ 8,  8],\n",
      "         [ 5, 15],\n",
      "         [ 3, 14],\n",
      "         [ 7,  0],\n",
      "         [ 6,  0],\n",
      "         [ 4,  4],\n",
      "         [ 8,  7],\n",
      "         [ 3,  5],\n",
      "         [ 3, 15],\n",
      "         [ 0,  2],\n",
      "         [ 1,  5],\n",
      "         [ 5,  3],\n",
      "         [ 1, 16],\n",
      "         [ 7,  3],\n",
      "         [ 7, 15],\n",
      "         [10,  7],\n",
      "         [10, 14],\n",
      "         [ 9, 14],\n",
      "         [ 4, 15],\n",
      "         [ 9,  9],\n",
      "         [ 5,  0],\n",
      "         [ 3,  4],\n",
      "         [ 7,  5]]]), (11, 19)), 'cls_output': tensor([[0.7252]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/164085027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 876\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.0705e-06, -1.7521e-07,  1.5456e-07, -2.1202e-08, -2.2476e-06,\n",
      "           3.2173e-06,  6.0811e-06, -5.3703e-09, -1.9807e-07, -7.9415e-07,\n",
      "          -1.5692e-07,  3.6335e-06, -3.8573e-08, -1.0093e-05, -1.3580e-08,\n",
      "          -1.8139e-07, -2.7724e-06,  9.1703e-09, -2.7452e-06, -5.7367e-09,\n",
      "           9.5353e-07,  3.9170e-09,  8.3201e-08, -1.0664e-08,  6.3288e-07,\n",
      "           3.1788e-06, -2.6123e-06, -1.0732e-09,  2.9125e-06,  4.4989e-06,\n",
      "          -9.3056e-06,  1.0643e-06, -4.5918e-07,  2.2646e-08, -9.7590e-07,\n",
      "           1.0921e-06,  5.4508e-08, -7.6128e-08, -2.9861e-09,  2.1609e-06,\n",
      "           7.3610e-08,  8.4742e-06,  3.4496e-08, -2.6085e-07, -4.6524e-06,\n",
      "           1.3929e-08, -5.0008e-07, -2.3619e-06, -2.0132e-06, -2.7828e-06,\n",
      "           1.0898e-07, -4.3159e-06, -3.1896e-08, -1.0233e-09, -2.3536e-06,\n",
      "          -1.7547e-09, -9.5498e-06,  2.6606e-06,  2.5922e-07, -5.3656e-08,\n",
      "           1.0648e-05, -2.6302e-07, -3.8063e-07,  1.8673e-08, -7.3548e-08,\n",
      "          -4.4497e-06,  5.8488e-10,  8.2012e-07, -1.0048e-08,  4.5198e-08,\n",
      "          -5.1587e-09, -4.5402e-07, -3.1521e-08,  7.7532e-07,  2.2411e-07,\n",
      "          -3.2163e-07,  2.4312e-09,  4.7526e-08, -1.8406e-09,  1.8258e-07,\n",
      "          -1.8963e-05,  3.5911e-10, -8.7442e-08, -2.1156e-06, -1.6889e-06,\n",
      "           1.6607e-08, -2.3158e-06,  1.8894e-07,  6.2720e-06, -3.0614e-07,\n",
      "           6.4272e-07,  8.4808e-10, -8.2978e-07,  3.7205e-06, -4.5556e-07,\n",
      "          -9.0431e-09, -2.6982e-08, -9.0076e-10, -1.1646e-08, -7.4008e-09,\n",
      "          -1.0095e-05,  4.5463e-07, -5.4923e-06,  4.2628e-06,  3.0881e-06,\n",
      "           4.9144e-06,  4.8263e-07,  1.9304e-05,  2.0120e-08, -1.8777e-08,\n",
      "           2.1247e-06,  2.9010e-06,  3.2299e-07, -7.7630e-08,  1.3161e-07,\n",
      "           5.7831e-06, -3.7161e-05, -2.9974e-07,  4.7228e-08, -3.7540e-07,\n",
      "          -5.0153e-06,  3.3485e-08, -4.8561e-06,  5.2390e-07,  3.9803e-06,\n",
      "           6.6331e-06,  1.0615e-06,  2.3658e-08,  1.9803e-07,  1.5534e-07,\n",
      "           1.2872e-05, -8.7163e-07, -2.0274e-07,  8.1007e-07,  1.9812e-05,\n",
      "           9.1543e-08, -1.8288e-07, -4.4274e-06, -8.2111e-06,  4.8516e-08,\n",
      "          -1.7234e-05,  3.3590e-06, -4.5906e-07,  3.0226e-06,  1.6611e-09,\n",
      "          -1.8436e-07,  7.9059e-07, -5.0428e-09,  2.3985e-07, -4.3570e-06,\n",
      "           2.7547e-06,  2.7828e-07, -2.0589e-07,  1.6140e-08, -8.7913e-06,\n",
      "          -4.1581e-06, -1.0303e-06, -4.2579e-06,  9.7744e-06,  3.5265e-08,\n",
      "           4.3416e-06,  3.5801e-07, -7.5727e-08,  1.3483e-06, -9.2882e-06,\n",
      "           1.5881e-06, -5.5673e-08,  1.8862e-10, -6.2908e-06,  1.1524e-07,\n",
      "           1.3282e-07,  1.1588e-08,  1.0572e-07, -1.8299e-10,  1.3464e-06,\n",
      "           7.6655e-07,  3.0214e-06,  4.7245e-07,  1.8281e-07, -8.3301e-09,\n",
      "          -5.3229e-08,  1.4018e-06, -3.9204e-07, -5.9981e-06,  1.8483e-06,\n",
      "          -1.2476e+01,  2.7466e-05,  1.7483e-07, -1.0044e-06,  1.1345e-06,\n",
      "           3.5873e-06,  1.6828e-08, -3.2386e-08,  1.3231e-06,  1.1067e-07,\n",
      "          -9.7024e-06, -3.4435e-06,  5.2040e-07,  9.8994e-09,  6.7235e-07,\n",
      "           1.1420e-07, -1.0131e-06, -1.9564e-06,  1.3763e-07,  2.0571e-09,\n",
      "           1.8330e-05,  2.3529e-09,  1.5280e-06, -1.3599e-05, -1.3387e-08,\n",
      "          -1.1265e-05,  6.5373e-06,  3.0832e-06,  2.0227e-09,  5.8327e-08,\n",
      "          -3.7830e-06, -2.6543e-07,  3.6740e-09, -1.6549e-09, -4.5770e-07,\n",
      "           1.5224e-09, -3.6159e-09,  8.5259e-07, -3.2185e-08,  1.1833e-05,\n",
      "          -3.8601e-08, -7.7616e-08, -1.2406e-07, -7.7444e-07,  3.5853e-07,\n",
      "          -1.1620e-06, -3.2386e-05,  5.1563e-08, -3.2455e-07, -1.0634e-08,\n",
      "          -4.7977e-06, -2.8738e-07,  2.0620e-06,  1.1597e-07,  1.9269e-07,\n",
      "           2.6304e-08, -1.5591e-06,  3.3739e-07,  6.7261e-09, -3.7009e-08,\n",
      "           2.0499e-05,  4.8093e-07,  6.9865e-08, -3.0791e-07,  2.4737e-09,\n",
      "           1.1568e-07, -1.1909e-08,  7.0830e-08, -3.8724e-07, -1.8492e-07,\n",
      "          -2.3770e-06, -4.9863e-08,  1.4903e-08,  1.7465e-07, -1.5729e-08,\n",
      "           1.3197e-06, -1.8862e-06, -3.4125e-07,  3.8081e-07,  4.1185e-09,\n",
      "           9.0387e-09, -7.8791e-07,  5.2688e-06,  1.0472e-06, -4.5152e-08,\n",
      "          -6.3991e-09, -2.5148e-05,  2.9618e-08, -3.2158e-08, -3.1126e-06,\n",
      "          -6.7059e-08, -1.1617e-05, -9.1126e-10, -1.2096e-06,  1.5687e-06,\n",
      "           1.2704e-05,  6.3805e-08,  2.4865e-07,  4.2421e-08, -1.3085e-06,\n",
      "           1.6238e-08,  1.1387e-07, -6.5243e-06, -3.7927e-09,  6.3239e-07,\n",
      "           2.1772e-07, -3.6238e-06,  7.8510e-06,  2.3451e-08, -1.6644e-08,\n",
      "          -6.7653e-09,  1.7376e-08,  4.2325e-06,  3.4382e-07, -7.9322e-07,\n",
      "           1.5175e-07, -2.0806e-06,  6.5979e-07, -1.1127e-06,  1.2323e-06,\n",
      "           8.6577e-07, -2.1816e-08, -2.4667e-07,  1.7908e-06, -2.7271e-06,\n",
      "          -8.2854e-07, -1.1972e-06,  5.9904e-06, -4.0014e-09,  1.3214e-06,\n",
      "          -3.6247e-09, -1.3826e-06,  1.5857e-07, -4.1422e-09,  1.3100e-06,\n",
      "          -1.9948e-07,  1.7546e-07, -1.6663e-07, -1.1876e-09,  1.2469e-05,\n",
      "           2.9894e-06, -6.9129e-08, -2.3283e-09,  4.1247e-07,  7.7483e-07,\n",
      "           9.1203e-08, -9.8638e-07, -6.5233e-07, -2.7579e-06,  6.2525e-07,\n",
      "           2.1035e-06, -2.5365e-07, -4.6151e-08, -8.3807e-07,  2.7435e-07,\n",
      "          -1.7637e-06, -2.1282e-05, -1.0119e-06,  5.4634e-07, -1.6888e-08,\n",
      "          -4.9618e-09,  7.0129e-06, -2.1212e-08, -3.2276e-08,  1.0739e-08,\n",
      "          -3.8581e-07,  2.0151e-06,  2.3192e-06,  2.5386e-06, -6.3311e-09,\n",
      "          -6.5924e-09,  1.6800e-05, -2.3732e-06,  1.1052e-08,  9.3492e-07,\n",
      "          -4.6413e-09, -4.5131e-09, -1.3413e-08, -1.7090e-08, -5.3563e-07,\n",
      "           3.9143e-08,  6.3290e-09, -3.9526e-06,  1.4251e-05,  1.4609e-09,\n",
      "          -4.7690e-06,  1.8617e-06, -2.3164e-06,  4.7677e-07,  5.5473e-07,\n",
      "          -6.5424e-07,  1.9043e-07,  2.8705e-09,  4.1889e-09,  3.1675e-08,\n",
      "          -5.6701e-07, -3.3418e-07, -4.8245e-06, -1.2208e-07, -1.2579e-05,\n",
      "          -1.0292e-08,  5.6189e-06, -3.3867e-07, -1.4220e-07, -1.1529e-06,\n",
      "           1.4955e-05, -9.9755e-06,  4.2098e-08, -4.7091e-07, -1.9709e-06,\n",
      "          -1.4910e-08,  6.7365e-08, -3.3627e-08,  1.7504e-06, -4.4959e-06,\n",
      "           7.6812e-06,  1.3357e-04, -1.0540e-07, -3.6329e-07,  2.1173e-07,\n",
      "           2.2562e-07,  1.1760e-07,  2.1037e-09, -1.2845e-08, -6.2611e-06,\n",
      "          -8.4922e-06, -6.6068e-09,  2.3698e-06,  1.0364e-06, -9.1273e-08,\n",
      "           7.9910e-07,  5.8195e-08,  3.8610e-07, -2.8259e-07, -2.8695e-06,\n",
      "           2.0003e-07,  8.7001e-09,  2.7057e-06,  9.6254e-06,  6.2056e-07,\n",
      "          -9.2256e-08,  4.0103e-06,  5.0245e-08, -3.0545e-09, -7.4120e-09,\n",
      "          -1.3020e-06, -5.8061e-07, -5.0467e-07,  2.0459e-06,  1.2453e-05,\n",
      "           2.8797e-09,  5.9096e-08, -1.3650e-07, -1.7231e-06,  5.6155e-08,\n",
      "          -7.2823e-06,  2.2833e-06,  1.0018e-06, -4.8581e-07, -2.4255e-08,\n",
      "          -1.1936e-06, -3.0576e-08, -1.3276e-07,  1.8906e-08, -3.4256e-07,\n",
      "          -4.7502e-07,  3.0308e-06, -1.9504e-06,  1.6687e-07, -1.4067e-07,\n",
      "          -3.2389e-06, -1.9622e-06,  1.2840e-06, -5.1929e-06, -5.7583e-09,\n",
      "           8.1122e-09, -2.4988e-07,  6.2490e-08, -7.1051e-06,  6.2890e-06,\n",
      "           1.5064e-06, -1.2966e-08, -6.0107e-07, -1.0128e-06,  2.2346e-06,\n",
      "          -9.7218e-09,  5.6699e-06, -2.6947e-06, -1.3278e-05, -1.7069e-08,\n",
      "           5.2235e-07, -9.7624e-09, -3.6445e-07, -7.0137e-07,  1.5170e-06,\n",
      "          -1.1140e-08, -8.9419e-08,  3.2134e-06, -3.2035e-08,  7.0367e-08,\n",
      "           3.3939e-08,  7.3384e-06,  6.1663e-07, -2.0497e-07, -3.0772e-08,\n",
      "          -1.4267e-06,  8.9570e-08, -4.0509e-06, -1.8876e-05,  2.0238e-07,\n",
      "           6.6535e-08,  2.4708e-07, -4.8657e-09, -3.6573e-07,  7.8544e-09,\n",
      "          -6.8982e-08,  4.8089e-07, -1.4776e-09, -2.3595e-08, -6.8129e-06,\n",
      "           6.9381e-10, -2.3210e-07, -1.1164e-06, -2.4618e-06,  2.6487e-09,\n",
      "           4.8181e-08, -9.3347e-06,  5.3015e-06,  3.2759e-07, -4.6297e-09,\n",
      "          -1.5417e-08, -6.7973e-10,  1.9806e-06,  2.1304e-08, -2.3451e-09,\n",
      "           9.6351e-06,  8.7539e-07,  6.9686e-08, -1.6368e-06, -7.8648e-06,\n",
      "           4.2370e-07, -9.8600e-08, -4.2517e-07,  4.9696e-07, -6.1395e-09,\n",
      "           5.2236e-09,  1.1263e-06,  3.4314e-09,  8.3416e-08, -2.0653e-08,\n",
      "           3.1180e-06,  4.5942e-06,  5.5429e-07, -1.1697e-08,  3.1334e-06,\n",
      "           3.5614e-07, -2.9946e-07,  2.4542e-06,  2.1006e-07,  3.4275e-07,\n",
      "           5.9653e-07, -6.7314e-08, -2.6987e-06, -1.2318e-07, -1.9951e-05,\n",
      "           4.1791e-09,  9.0749e-08,  1.4211e-07, -1.2994e-08,  7.7482e-09,\n",
      "           2.0325e-06,  3.2872e-08,  1.8770e-07, -3.6565e-08, -4.4565e-07,\n",
      "           5.4924e-09, -2.8814e-07, -1.0286e-05, -1.2779e-05, -3.6414e-09,\n",
      "           4.2905e-08,  9.2467e-08,  1.1076e-06,  6.8068e-07,  1.0772e-08,\n",
      "          -2.5271e-07, -1.2429e-08, -1.7459e-07,  3.1883e-06,  3.0819e-08,\n",
      "           8.3064e-07,  1.4967e-06,  3.5149e-07,  1.9145e-06,  1.0433e-05,\n",
      "          -4.0918e-06,  3.2103e-06, -1.2838e-09, -1.7808e-08, -9.6167e-06,\n",
      "           4.1772e-06,  9.4751e-08,  7.0124e-09, -5.8482e-08,  4.1038e-06,\n",
      "           9.3137e-08,  1.4279e-08, -8.8191e-09,  2.0127e-08, -2.0090e-07,\n",
      "           1.7485e-06,  5.9445e-09, -6.4444e-07, -5.8941e-07, -3.4347e-08,\n",
      "           1.4494e-06,  1.6853e-08, -5.9534e-08, -1.2081e-06, -1.1556e-07,\n",
      "          -2.2007e-06, -1.6601e-06,  2.1164e-07,  6.2411e-08,  1.6189e-06,\n",
      "          -1.4095e-05,  1.2285e-05,  1.9836e-09, -1.1830e-09,  1.4245e-07,\n",
      "          -4.2388e-09, -7.5148e-06, -5.9049e-06, -4.7331e-06,  1.6933e-08,\n",
      "          -4.7904e-08,  3.3373e-09, -1.2266e-08, -1.7535e-08, -9.2677e-07,\n",
      "          -1.2172e-06, -3.0298e-08, -1.1858e-07, -6.2225e-06,  7.0328e-08,\n",
      "           6.8054e-08, -7.1502e-07, -2.6513e-07,  2.7297e-06,  2.3259e-09,\n",
      "           7.3877e-08, -7.2392e-10,  3.1065e-07,  2.8778e-08,  4.6586e-09,\n",
      "           2.0189e-07, -1.7172e-09,  1.2696e-07,  2.4887e-09, -6.3959e-08,\n",
      "           1.3936e-07, -1.6669e-07,  4.6270e-07,  1.2864e-07, -5.1439e-06,\n",
      "           1.0699e-07, -1.8113e-06,  8.5400e-10,  2.5707e-07, -3.5609e-06,\n",
      "           1.6756e-05,  3.0093e-06, -4.4207e-07, -9.3632e-08, -1.4868e-05,\n",
      "           1.2654e-07, -1.3183e-05, -1.4881e-06,  2.0645e-05,  4.9586e-07,\n",
      "           6.1311e-08,  5.1878e-06,  1.2307e-05, -5.6192e-07, -6.8642e-07,\n",
      "           3.7236e-06,  5.0237e-09, -2.3857e-08, -7.7664e-08, -8.3865e-09,\n",
      "           1.5629e-06,  2.6984e-07,  5.2005e-08, -3.9755e-06, -1.7400e-06,\n",
      "          -1.7448e-07,  1.1338e-07, -1.3464e-06, -4.8910e-06,  1.4077e-07,\n",
      "          -4.3792e-07, -4.1272e-08,  7.2853e-08, -8.9580e-08, -4.6144e-09,\n",
      "          -7.4259e-07, -1.5908e-05,  3.6718e-06,  9.4396e-06,  5.6789e-08,\n",
      "           1.7059e-09,  2.5213e-06, -4.6034e-08,  5.2949e-08,  5.4982e-08,\n",
      "          -5.4109e-08,  2.1200e-06,  3.1344e-07,  9.6897e-08, -2.8529e-07,\n",
      "          -1.0593e-07, -1.7308e-06,  2.8259e-07, -3.0128e-06,  2.8530e-07,\n",
      "          -4.6122e-07, -6.8373e-06, -3.2311e-07, -2.2906e-08, -2.5482e-07,\n",
      "          -2.2110e-09, -1.1769e-06,  7.2977e-08, -5.7473e-07, -1.1251e-07,\n",
      "           1.0031e-07,  6.1135e-07, -1.1308e-08, -2.1041e-06,  2.4877e-08,\n",
      "           4.2130e-06, -6.9241e-09, -9.0283e-08, -7.0564e-08,  3.2511e-09,\n",
      "           2.1240e-06,  2.7619e-07, -2.1908e-06,  5.6387e-08,  2.2908e-08,\n",
      "          -5.2895e-09,  5.7410e-06, -2.2186e-07,  8.8366e-09, -2.1829e-06,\n",
      "          -1.5606e-07, -7.4005e-09,  1.4464e-07,  6.0941e-08,  1.8124e-07,\n",
      "          -5.9934e-06, -4.1108e-08, -9.2321e-06, -1.2959e-07, -2.9012e-06,\n",
      "           4.0460e-06,  4.3070e-06,  1.2612e-07,  5.2592e-08, -2.4102e-05,\n",
      "           1.0350e-07, -1.0870e-08, -5.3403e-07,  2.3729e-08, -6.2486e-06,\n",
      "           6.4484e-07, -2.0996e-05,  2.8569e-06,  6.8823e-08,  4.8678e-07,\n",
      "          -8.1302e-07,  3.0096e-07, -5.7045e-07]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1839e-07, -3.4249e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9451e-07,\n",
      "           3.1838e-07, -3.4252e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9452e-07,\n",
      "           3.1838e-07, -3.4245e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9452e-07,\n",
      "           3.1839e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7536e-07,  1.5568e-07,  ..., -7.9445e-07,\n",
      "           3.1837e-07, -3.4245e-06]]], device='cuda:0'), 'cls_feats': tensor([[-2.5329e-01,  2.7298e-01, -2.7280e-01, -1.6352e-01, -2.5890e-01,\n",
      "         -2.7271e-01, -2.7103e-01, -2.7263e-01,  2.7364e-01,  2.4064e-01,\n",
      "          2.7141e-01,  2.7301e-01,  2.6473e-01, -4.2325e-06,  4.4231e-07,\n",
      "          1.5733e-01,  2.6908e-01, -2.5862e-01,  2.7277e-01, -2.7285e-01,\n",
      "          2.6162e-01,  2.7369e-01,  2.7239e-01, -2.7115e-01,  2.5504e-01,\n",
      "          2.7245e-01, -2.7290e-01,  2.7163e-01,  2.7095e-01,  2.7171e-01,\n",
      "         -2.7310e-01,  1.1716e-01,  1.7293e-01, -6.8875e-03,  2.7160e-01,\n",
      "          2.7121e-01, -2.6871e-01, -2.7221e-01, -2.6997e-01,  4.9694e-08,\n",
      "          9.2227e-03,  2.6083e-01, -2.7138e-01, -2.6946e-01,  2.7263e-01,\n",
      "         -2.7328e-01, -2.7197e-01, -2.7241e-01, -2.5311e-01,  2.7223e-01,\n",
      "          2.6809e-01, -2.7347e-01, -2.7346e-01, -2.7066e-01, -2.7207e-01,\n",
      "          2.5937e-01,  2.7125e-01,  2.7321e-01,  2.7196e-01, -2.5437e-01,\n",
      "         -2.7341e-01,  3.1951e-07,  2.7169e-01, -2.6424e-01, -2.7358e-01,\n",
      "          1.6841e-02,  1.5248e-01, -2.6188e-01,  1.5418e-01,  2.7122e-01,\n",
      "          2.7234e-01, -2.0134e-08,  2.7261e-01,  2.5505e-01, -1.6933e-01,\n",
      "         -2.7271e-01, -2.7113e-01, -2.7173e-01,  2.7162e-01, -2.7094e-01,\n",
      "          2.7282e-01,  2.7289e-01, -1.2384e-02,  2.7327e-01,  2.5079e-01,\n",
      "         -5.2214e-09,  2.7274e-01,  2.5253e-01, -2.7112e-01,  2.6295e-01,\n",
      "          7.2820e-02,  1.5179e-01, -2.6720e-01,  2.6206e-01, -2.5522e-01,\n",
      "          2.5453e-01,  2.7262e-01,  2.7172e-01, -2.5858e-01, -2.7368e-01,\n",
      "         -1.7650e-01, -2.7364e-01, -2.7761e-06, -2.7050e-01, -2.7242e-01,\n",
      "         -9.5408e-07,  2.8162e-07, -6.6777e-08,  2.7260e-01, -2.7262e-01,\n",
      "         -3.2197e-02, -2.7090e-01, -2.7245e-01,  2.5116e-01,  5.3597e-03,\n",
      "          2.6201e-01, -1.3350e-03, -9.9751e-02, -2.7307e-01,  2.6879e-06,\n",
      "          2.6327e-01, -2.5784e-01,  7.6070e-05,  2.6247e-01,  2.7262e-01,\n",
      "         -2.2627e-09,  2.4026e-01,  2.7228e-01,  2.7261e-01, -2.6966e-01,\n",
      "          9.4408e-07, -2.7287e-01, -3.0215e-06, -2.5881e-01, -4.4230e-08,\n",
      "         -2.6581e-01,  2.6156e-01,  2.7199e-01,  1.0778e-01,  2.7290e-01,\n",
      "          3.6665e-10, -6.5719e-03,  3.9806e-03, -2.6437e-01,  2.6979e-01,\n",
      "          5.7351e-03,  2.6954e-01,  2.7312e-01, -2.5813e-01,  2.6934e-01,\n",
      "         -2.7057e-01,  2.7349e-01,  2.7225e-01,  9.3279e-09, -2.7260e-01,\n",
      "         -3.3164e-05, -2.5894e-01, -2.7316e-01, -2.6113e-01, -2.7235e-01,\n",
      "          1.2625e-01, -2.7337e-01,  2.3563e-01, -2.5498e-01, -2.6821e-01,\n",
      "          2.6304e-01,  2.7362e-01, -2.5284e-01,  2.7051e-01, -2.7178e-01,\n",
      "          1.6742e-07, -2.3239e-01, -2.7683e-02, -2.6194e-01, -2.7192e-01,\n",
      "          2.4778e-07,  2.7302e-01, -2.7223e-01,  1.2483e-08,  2.5061e-01,\n",
      "         -9.8509e-02,  2.7330e-01,  2.7125e-01, -2.7161e-01,  2.7224e-01,\n",
      "         -2.6130e-01,  2.2664e-01,  1.0688e-10, -2.6499e-01,  2.7301e-01,\n",
      "          2.7192e-01,  2.6404e-01, -2.6144e-01,  2.7179e-01, -2.5356e-01,\n",
      "          2.7134e-01,  2.5808e-01,  2.7103e-01,  2.6942e-01,  2.7246e-01,\n",
      "         -2.7130e-01, -2.7259e-01,  2.7295e-01,  2.7233e-01,  3.3845e-07,\n",
      "         -2.0712e-01, -2.6847e-01,  2.7179e-01,  2.6877e-01, -2.6997e-01,\n",
      "         -2.6171e-01, -2.7193e-01,  2.0173e-01, -2.7287e-01,  2.6270e-04,\n",
      "         -2.8069e-06, -2.7149e-01,  2.6112e-01, -2.7188e-01,  2.5542e-01,\n",
      "          3.3272e-05,  1.4977e-01,  2.7326e-01, -2.5716e-01,  1.5103e-01,\n",
      "          2.6368e-01,  2.7272e-01,  2.7164e-01,  2.7301e-01,  2.7327e-01,\n",
      "          2.6694e-01, -3.0225e-08,  2.5672e-01, -2.7213e-01, -2.7201e-01,\n",
      "          2.7264e-01, -1.2435e-02, -2.6663e-01,  2.6263e-01,  2.7366e-01,\n",
      "          2.6474e-01,  2.6283e-01,  2.6809e-01,  2.7153e-01, -1.1546e-01,\n",
      "          2.7087e-01, -1.0964e-02,  2.6851e-01, -1.4769e-01,  2.7090e-01,\n",
      "          2.7092e-01,  2.7378e-01,  2.4240e-03,  2.5407e-01,  2.5658e-01,\n",
      "         -2.7265e-01, -2.5323e-03, -2.0187e-04, -2.7371e-01,  2.7115e-01,\n",
      "         -2.7124e-01, -2.7291e-01,  2.4970e-01,  5.9815e-07, -2.7289e-01,\n",
      "         -2.4732e-01,  2.7261e-01,  2.6020e-01,  2.6386e-01,  2.6247e-01,\n",
      "          2.6751e-08, -2.7202e-01,  2.3406e-01, -2.7045e-01,  2.7338e-01,\n",
      "         -2.6875e-01,  2.5824e-01,  2.7291e-01,  2.7290e-01,  2.4673e-01,\n",
      "         -2.7322e-01, -2.6165e-01,  2.6366e-01,  2.7266e-01,  3.6768e-05,\n",
      "         -2.7326e-01, -2.7268e-02, -2.6397e-01,  1.1433e-01, -2.6522e-01,\n",
      "         -2.6601e-01,  1.3108e-01, -2.7206e-01, -2.7207e-01, -1.2430e-05,\n",
      "         -2.7117e-01, -2.7116e-01, -2.5257e-01, -2.6989e-01,  2.5703e-01,\n",
      "         -2.7190e-01, -2.7168e-01,  2.7014e-01,  2.6280e-01, -2.7317e-01,\n",
      "          2.7230e-01,  7.2003e-07, -3.0523e-04,  2.7236e-01,  2.9854e-06,\n",
      "          2.1700e-01,  2.4211e-01, -1.9442e-06, -2.6873e-01,  2.7277e-01,\n",
      "         -2.7360e-01,  2.7330e-01,  2.7135e-01, -2.6965e-01, -2.2647e-02,\n",
      "          1.6124e-07,  2.7255e-01,  2.7328e-01, -2.7367e-01,  2.5577e-01,\n",
      "         -2.7231e-01, -2.6937e-01, -2.7147e-01, -1.1090e-01, -2.6133e-01,\n",
      "         -2.7257e-01, -2.6162e-01,  2.6645e-01,  2.2533e-07, -2.7141e-01,\n",
      "          2.7209e-01,  3.8620e-04,  1.6536e-02, -2.6058e-01,  2.6870e-01,\n",
      "         -2.6980e-01,  3.1584e-07, -2.7198e-01, -1.5198e-01, -2.7295e-01,\n",
      "         -2.7343e-01,  2.5247e-01,  2.5445e-01,  1.5226e-01,  2.7351e-01,\n",
      "          2.7271e-01,  2.6702e-01,  1.5340e-01,  2.7023e-01, -2.5675e-01,\n",
      "         -2.7217e-01,  2.6960e-01,  2.5103e-01, -2.7228e-01, -1.0551e-01,\n",
      "         -2.4441e-01,  2.7025e-01, -2.6249e-01,  2.7232e-01,  8.4238e-08,\n",
      "         -2.7359e-01, -2.7176e-01, -2.7291e-01, -2.7263e-01, -2.7089e-01,\n",
      "          2.3009e-02, -2.5393e-01, -2.7267e-01,  2.7359e-01,  2.7277e-01,\n",
      "          2.6673e-01, -2.7256e-01, -2.7292e-01,  2.5438e-01,  2.4872e-01,\n",
      "         -2.6349e-02,  2.7306e-01,  2.7272e-01, -2.7240e-01,  2.6908e-01,\n",
      "         -2.5230e-01,  2.7099e-01,  3.2165e-06,  2.2942e-03,  1.9737e-02,\n",
      "         -2.7261e-01,  2.5198e-01,  2.6387e-01,  2.7236e-01,  2.7049e-01,\n",
      "         -8.5241e-03, -2.7017e-01, -2.6183e-01,  2.7123e-01,  2.7288e-01,\n",
      "          1.3550e-01,  1.6019e-01,  2.7247e-01,  2.7167e-01, -2.7295e-01,\n",
      "          2.7165e-01, -2.6089e-01,  2.4908e-01, -2.7323e-01, -2.6206e-01,\n",
      "         -2.7173e-01,  2.7181e-01, -2.6966e-01,  1.2801e-06, -2.7334e-01,\n",
      "         -2.6206e-01, -2.7279e-01,  2.7226e-01,  2.7166e-01, -2.7324e-01,\n",
      "          1.2125e-08, -2.6470e-01, -2.7231e-01, -9.4538e-04, -1.6635e-01,\n",
      "         -1.3581e-01,  2.6416e-01, -1.6150e-01, -2.7024e-01, -2.7223e-01,\n",
      "         -2.3759e-01,  2.7153e-01,  2.7048e-01,  2.7280e-01,  2.5597e-01,\n",
      "          2.7314e-01, -1.6542e-01,  2.7044e-01,  2.4861e-01,  3.6537e-04,\n",
      "          2.7301e-01, -2.6170e-01, -2.7230e-01, -5.2143e-08, -2.7126e-01,\n",
      "          2.7298e-01, -1.6175e-08, -2.7138e-01,  2.7295e-01,  2.6272e-01,\n",
      "         -2.0160e-06,  2.7323e-01, -1.2852e-04,  2.7220e-01,  2.7177e-01,\n",
      "          2.7006e-01,  7.5270e-04,  2.7072e-01,  2.7324e-01, -2.2270e-01,\n",
      "          2.7274e-01, -2.7306e-01, -4.1577e-07, -2.7281e-01, -1.0313e-04,\n",
      "         -2.5432e-01, -2.7204e-01,  2.6971e-01, -2.7316e-01, -2.7034e-01,\n",
      "         -2.7253e-01,  4.1148e-07, -2.7264e-01,  2.7142e-01, -2.2444e-01,\n",
      "         -2.2670e-01,  6.9491e-07,  2.7069e-01, -2.7274e-01, -2.5658e-01,\n",
      "          2.6244e-01,  2.6912e-01, -2.6680e-01, -2.5361e-02,  2.7273e-01,\n",
      "         -2.7295e-01,  2.6339e-01,  2.4183e-01, -2.7272e-01,  2.3183e-01,\n",
      "          2.6350e-01,  5.8954e-03, -2.7357e-01, -2.6848e-01,  1.4947e-01,\n",
      "          2.7316e-01, -2.6478e-01,  2.6547e-01,  2.6885e-01, -2.7347e-01,\n",
      "         -2.7233e-01,  2.1855e-06, -3.2388e-02, -2.7295e-01,  2.5127e-01,\n",
      "         -2.8110e-02,  2.7183e-01,  2.4543e-01,  1.3275e-01,  2.7332e-01,\n",
      "         -2.6173e-01, -4.4202e-03,  2.2200e-01,  2.6853e-01, -4.4395e-04,\n",
      "         -2.7194e-01,  2.6979e-02,  2.5757e-01,  2.2338e-01, -2.7329e-01,\n",
      "          2.6293e-01,  2.4051e-01,  2.6220e-01, -2.7191e-01,  2.7227e-01,\n",
      "         -2.5946e-01,  3.5099e-02,  2.6400e-01, -2.7336e-01, -2.7260e-01,\n",
      "         -2.7285e-01,  5.2348e-04,  2.7274e-01, -2.7351e-01,  2.7193e-01,\n",
      "          1.8870e-02, -2.0834e-01,  2.5499e-01, -2.7214e-01,  6.2968e-08,\n",
      "          2.7236e-01,  2.5419e-01, -2.7341e-01,  1.5647e-04, -2.7237e-01,\n",
      "          1.3710e-03,  2.4738e-01,  2.7279e-01,  2.6177e-01, -2.7283e-01,\n",
      "          4.4240e-03,  2.6228e-01, -2.5398e-01, -2.6876e-01, -2.6492e-01,\n",
      "         -2.7282e-01,  1.0813e-08,  4.0760e-06, -2.7280e-01, -2.7259e-01,\n",
      "          2.5444e-01,  2.6198e-01,  2.7152e-01, -2.6067e-01,  2.7049e-01,\n",
      "         -2.4644e-01,  2.7136e-01, -2.7154e-01, -2.6196e-01, -2.7146e-01,\n",
      "          2.7064e-01, -2.7271e-01, -2.7218e-01, -2.7300e-01,  2.7229e-01,\n",
      "          2.6744e-01,  9.3937e-06, -1.2705e-02,  2.6767e-01,  2.6920e-01,\n",
      "          2.7285e-01, -2.7282e-01, -6.2177e-08, -2.6961e-01,  2.4165e-01,\n",
      "         -2.6665e-01,  1.3670e-03,  2.7087e-01, -2.6168e-01, -2.7303e-01,\n",
      "         -2.7342e-01,  2.5364e-01, -1.3879e-01, -2.7343e-01,  2.2210e-06,\n",
      "         -2.7116e-01, -2.6262e-01,  2.6627e-01,  2.7142e-01, -2.7356e-01,\n",
      "         -2.6524e-01,  2.7126e-01, -2.6414e-01,  2.6202e-01,  2.5281e-01,\n",
      "          2.7308e-01, -4.2828e-08, -2.2151e-07,  2.7244e-01,  2.6131e-01,\n",
      "         -2.7234e-01, -2.7266e-01, -2.7312e-01, -2.5223e-01, -2.6841e-01,\n",
      "          2.7149e-01, -1.4834e-08,  2.2038e-05,  2.7381e-01, -2.7356e-01,\n",
      "          2.7182e-01, -2.6702e-01,  2.7306e-01, -2.7175e-01, -2.7184e-01,\n",
      "         -2.7141e-01,  5.9924e-03, -1.0006e-02, -1.4745e-04, -2.5374e-01,\n",
      "          2.7147e-01,  8.8581e-06,  2.5426e-01, -1.9334e-03,  2.7121e-01,\n",
      "         -1.7211e-01,  2.4070e-02, -2.7326e-01, -2.7158e-01, -2.7188e-01,\n",
      "         -1.6470e-01,  1.1724e-08,  2.7160e-01, -3.1781e-07,  2.9881e-02,\n",
      "         -2.6133e-01,  2.7309e-01,  2.7135e-01, -1.7179e-05,  2.7253e-01,\n",
      "         -2.7346e-01, -2.5500e-01, -2.7213e-01, -2.7293e-01,  1.1335e-02,\n",
      "          2.7219e-01, -2.7200e-01, -2.3388e-01,  2.7184e-01,  2.7010e-01,\n",
      "         -2.4659e-01,  1.1877e-04,  2.6927e-01, -2.6908e-01,  2.7006e-01,\n",
      "         -2.6020e-01, -1.9822e-04,  1.0538e-01, -4.8812e-03,  2.7079e-01,\n",
      "         -2.7316e-01, -2.7206e-01, -2.7165e-01, -2.7226e-01, -4.5433e-04,\n",
      "          2.6929e-01, -2.6833e-01, -2.5352e-01, -2.7220e-01,  2.7324e-01,\n",
      "          2.7257e-01,  2.6701e-01,  2.7214e-01, -2.7124e-01,  2.7099e-01,\n",
      "         -2.7240e-01, -2.5856e-01,  2.6703e-01, -1.4720e-05,  8.5388e-09,\n",
      "          2.2639e-04,  2.7091e-01,  2.7100e-01, -2.7262e-01, -1.6317e-01,\n",
      "          2.6928e-01, -2.7154e-01, -2.7302e-01, -1.6911e-01, -2.7054e-01,\n",
      "          4.3386e-07, -2.7071e-01, -2.7054e-01,  1.3794e-01,  2.5327e-01,\n",
      "          2.7136e-01,  2.7082e-01,  2.6493e-01, -8.8703e-07, -2.7009e-01,\n",
      "          2.2928e-01,  2.6872e-01,  2.7208e-01, -2.7159e-01, -1.4761e-06,\n",
      "         -2.5449e-01, -2.7302e-01, -2.7221e-01,  2.6903e-01, -2.7323e-01,\n",
      "         -2.7337e-01, -2.5867e-01,  2.6244e-01,  2.6236e-01,  2.7024e-01,\n",
      "         -2.5688e-01, -2.7190e-01, -2.7045e-01,  1.2533e-01,  2.7131e-01,\n",
      "         -1.9350e-05,  2.7349e-01, -4.5170e-06,  2.5484e-01, -2.7258e-01,\n",
      "          2.7195e-01, -2.7275e-01,  2.3883e-01,  2.6131e-01, -2.6139e-01,\n",
      "          2.6701e-01,  2.5422e-01, -2.7042e-01,  2.5555e-01,  2.1375e-02,\n",
      "         -2.7165e-01,  2.7207e-01, -1.5697e-02,  2.7171e-01, -2.4122e-02,\n",
      "         -2.7250e-01, -2.4917e-01, -2.7290e-01, -2.7202e-01, -2.6841e-02,\n",
      "          2.6774e-01,  2.7053e-01, -2.5754e-04,  2.6161e-01,  2.7343e-01,\n",
      "          2.7171e-01,  2.7215e-01, -2.7246e-01, -2.7303e-01, -1.1138e-02,\n",
      "         -9.8367e-09,  2.7236e-01, -2.7244e-01]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0705e-06, -1.7521e-07,  1.5456e-07, -2.1202e-08, -2.2476e-06,\n",
      "          3.2173e-06,  6.0811e-06, -5.3703e-09, -1.9807e-07, -7.9415e-07,\n",
      "         -1.5692e-07,  3.6335e-06, -3.8573e-08, -1.0093e-05, -1.3580e-08,\n",
      "         -1.8139e-07, -2.7724e-06,  9.1703e-09, -2.7452e-06, -5.7367e-09,\n",
      "          9.5353e-07,  3.9170e-09,  8.3201e-08, -1.0664e-08,  6.3288e-07,\n",
      "          3.1788e-06, -2.6123e-06, -1.0732e-09,  2.9125e-06,  4.4989e-06,\n",
      "         -9.3056e-06,  1.0643e-06, -4.5918e-07,  2.2646e-08, -9.7590e-07,\n",
      "          1.0921e-06,  5.4508e-08, -7.6128e-08, -2.9861e-09,  2.1609e-06,\n",
      "          7.3610e-08,  8.4742e-06,  3.4496e-08, -2.6085e-07, -4.6524e-06,\n",
      "          1.3929e-08, -5.0008e-07, -2.3619e-06, -2.0132e-06, -2.7828e-06,\n",
      "          1.0898e-07, -4.3159e-06, -3.1896e-08, -1.0233e-09, -2.3536e-06,\n",
      "         -1.7547e-09, -9.5498e-06,  2.6606e-06,  2.5922e-07, -5.3656e-08,\n",
      "          1.0648e-05, -2.6302e-07, -3.8063e-07,  1.8673e-08, -7.3548e-08,\n",
      "         -4.4497e-06,  5.8488e-10,  8.2012e-07, -1.0048e-08,  4.5198e-08,\n",
      "         -5.1587e-09, -4.5402e-07, -3.1521e-08,  7.7532e-07,  2.2411e-07,\n",
      "         -3.2163e-07,  2.4312e-09,  4.7526e-08, -1.8406e-09,  1.8258e-07,\n",
      "         -1.8963e-05,  3.5911e-10, -8.7442e-08, -2.1156e-06, -1.6889e-06,\n",
      "          1.6607e-08, -2.3158e-06,  1.8894e-07,  6.2720e-06, -3.0614e-07,\n",
      "          6.4272e-07,  8.4808e-10, -8.2978e-07,  3.7205e-06, -4.5556e-07,\n",
      "         -9.0431e-09, -2.6982e-08, -9.0076e-10, -1.1646e-08, -7.4008e-09,\n",
      "         -1.0095e-05,  4.5463e-07, -5.4923e-06,  4.2628e-06,  3.0881e-06,\n",
      "          4.9144e-06,  4.8263e-07,  1.9304e-05,  2.0120e-08, -1.8777e-08,\n",
      "          2.1247e-06,  2.9010e-06,  3.2299e-07, -7.7630e-08,  1.3161e-07,\n",
      "          5.7831e-06, -3.7161e-05, -2.9974e-07,  4.7228e-08, -3.7540e-07,\n",
      "         -5.0153e-06,  3.3485e-08, -4.8561e-06,  5.2390e-07,  3.9803e-06,\n",
      "          6.6331e-06,  1.0615e-06,  2.3658e-08,  1.9803e-07,  1.5534e-07,\n",
      "          1.2872e-05, -8.7163e-07, -2.0274e-07,  8.1007e-07,  1.9812e-05,\n",
      "          9.1543e-08, -1.8288e-07, -4.4274e-06, -8.2111e-06,  4.8516e-08,\n",
      "         -1.7234e-05,  3.3590e-06, -4.5906e-07,  3.0226e-06,  1.6611e-09,\n",
      "         -1.8436e-07,  7.9059e-07, -5.0428e-09,  2.3985e-07, -4.3570e-06,\n",
      "          2.7547e-06,  2.7828e-07, -2.0589e-07,  1.6140e-08, -8.7913e-06,\n",
      "         -4.1581e-06, -1.0303e-06, -4.2579e-06,  9.7744e-06,  3.5265e-08,\n",
      "          4.3416e-06,  3.5801e-07, -7.5727e-08,  1.3483e-06, -9.2882e-06,\n",
      "          1.5881e-06, -5.5673e-08,  1.8862e-10, -6.2908e-06,  1.1524e-07,\n",
      "          1.3282e-07,  1.1588e-08,  1.0572e-07, -1.8299e-10,  1.3464e-06,\n",
      "          7.6655e-07,  3.0214e-06,  4.7245e-07,  1.8281e-07, -8.3301e-09,\n",
      "         -5.3229e-08,  1.4018e-06, -3.9204e-07, -5.9981e-06,  1.8483e-06,\n",
      "         -1.2476e+01,  2.7466e-05,  1.7483e-07, -1.0044e-06,  1.1345e-06,\n",
      "          3.5873e-06,  1.6828e-08, -3.2386e-08,  1.3231e-06,  1.1067e-07,\n",
      "         -9.7024e-06, -3.4435e-06,  5.2040e-07,  9.8994e-09,  6.7235e-07,\n",
      "          1.1420e-07, -1.0131e-06, -1.9564e-06,  1.3763e-07,  2.0571e-09,\n",
      "          1.8330e-05,  2.3529e-09,  1.5280e-06, -1.3599e-05, -1.3387e-08,\n",
      "         -1.1265e-05,  6.5373e-06,  3.0832e-06,  2.0227e-09,  5.8327e-08,\n",
      "         -3.7830e-06, -2.6543e-07,  3.6740e-09, -1.6549e-09, -4.5770e-07,\n",
      "          1.5224e-09, -3.6159e-09,  8.5259e-07, -3.2185e-08,  1.1833e-05,\n",
      "         -3.8601e-08, -7.7616e-08, -1.2406e-07, -7.7444e-07,  3.5853e-07,\n",
      "         -1.1620e-06, -3.2386e-05,  5.1563e-08, -3.2455e-07, -1.0634e-08,\n",
      "         -4.7977e-06, -2.8738e-07,  2.0620e-06,  1.1597e-07,  1.9269e-07,\n",
      "          2.6304e-08, -1.5591e-06,  3.3739e-07,  6.7261e-09, -3.7009e-08,\n",
      "          2.0499e-05,  4.8093e-07,  6.9865e-08, -3.0791e-07,  2.4737e-09,\n",
      "          1.1568e-07, -1.1909e-08,  7.0830e-08, -3.8724e-07, -1.8492e-07,\n",
      "         -2.3770e-06, -4.9863e-08,  1.4903e-08,  1.7465e-07, -1.5729e-08,\n",
      "          1.3197e-06, -1.8862e-06, -3.4125e-07,  3.8081e-07,  4.1185e-09,\n",
      "          9.0387e-09, -7.8791e-07,  5.2688e-06,  1.0472e-06, -4.5152e-08,\n",
      "         -6.3991e-09, -2.5148e-05,  2.9618e-08, -3.2158e-08, -3.1126e-06,\n",
      "         -6.7059e-08, -1.1617e-05, -9.1126e-10, -1.2096e-06,  1.5687e-06,\n",
      "          1.2704e-05,  6.3805e-08,  2.4865e-07,  4.2421e-08, -1.3085e-06,\n",
      "          1.6238e-08,  1.1387e-07, -6.5243e-06, -3.7927e-09,  6.3239e-07,\n",
      "          2.1772e-07, -3.6238e-06,  7.8510e-06,  2.3451e-08, -1.6644e-08,\n",
      "         -6.7653e-09,  1.7376e-08,  4.2325e-06,  3.4382e-07, -7.9322e-07,\n",
      "          1.5175e-07, -2.0806e-06,  6.5979e-07, -1.1127e-06,  1.2323e-06,\n",
      "          8.6577e-07, -2.1816e-08, -2.4667e-07,  1.7908e-06, -2.7271e-06,\n",
      "         -8.2854e-07, -1.1972e-06,  5.9904e-06, -4.0014e-09,  1.3214e-06,\n",
      "         -3.6247e-09, -1.3826e-06,  1.5857e-07, -4.1422e-09,  1.3100e-06,\n",
      "         -1.9948e-07,  1.7546e-07, -1.6663e-07, -1.1876e-09,  1.2469e-05,\n",
      "          2.9894e-06, -6.9129e-08, -2.3283e-09,  4.1247e-07,  7.7483e-07,\n",
      "          9.1203e-08, -9.8638e-07, -6.5233e-07, -2.7579e-06,  6.2525e-07,\n",
      "          2.1035e-06, -2.5365e-07, -4.6151e-08, -8.3807e-07,  2.7435e-07,\n",
      "         -1.7637e-06, -2.1282e-05, -1.0119e-06,  5.4634e-07, -1.6888e-08,\n",
      "         -4.9618e-09,  7.0129e-06, -2.1212e-08, -3.2276e-08,  1.0739e-08,\n",
      "         -3.8581e-07,  2.0151e-06,  2.3192e-06,  2.5386e-06, -6.3311e-09,\n",
      "         -6.5924e-09,  1.6800e-05, -2.3732e-06,  1.1052e-08,  9.3492e-07,\n",
      "         -4.6413e-09, -4.5131e-09, -1.3413e-08, -1.7090e-08, -5.3563e-07,\n",
      "          3.9143e-08,  6.3290e-09, -3.9526e-06,  1.4251e-05,  1.4609e-09,\n",
      "         -4.7690e-06,  1.8617e-06, -2.3164e-06,  4.7677e-07,  5.5473e-07,\n",
      "         -6.5424e-07,  1.9043e-07,  2.8705e-09,  4.1889e-09,  3.1675e-08,\n",
      "         -5.6701e-07, -3.3418e-07, -4.8245e-06, -1.2208e-07, -1.2579e-05,\n",
      "         -1.0292e-08,  5.6189e-06, -3.3867e-07, -1.4220e-07, -1.1529e-06,\n",
      "          1.4955e-05, -9.9755e-06,  4.2098e-08, -4.7091e-07, -1.9709e-06,\n",
      "         -1.4910e-08,  6.7365e-08, -3.3627e-08,  1.7504e-06, -4.4959e-06,\n",
      "          7.6812e-06,  1.3357e-04, -1.0540e-07, -3.6329e-07,  2.1173e-07,\n",
      "          2.2562e-07,  1.1760e-07,  2.1037e-09, -1.2845e-08, -6.2611e-06,\n",
      "         -8.4922e-06, -6.6068e-09,  2.3698e-06,  1.0364e-06, -9.1273e-08,\n",
      "          7.9910e-07,  5.8195e-08,  3.8610e-07, -2.8259e-07, -2.8695e-06,\n",
      "          2.0003e-07,  8.7001e-09,  2.7057e-06,  9.6254e-06,  6.2056e-07,\n",
      "         -9.2256e-08,  4.0103e-06,  5.0245e-08, -3.0545e-09, -7.4120e-09,\n",
      "         -1.3020e-06, -5.8061e-07, -5.0467e-07,  2.0459e-06,  1.2453e-05,\n",
      "          2.8797e-09,  5.9096e-08, -1.3650e-07, -1.7231e-06,  5.6155e-08,\n",
      "         -7.2823e-06,  2.2833e-06,  1.0018e-06, -4.8581e-07, -2.4255e-08,\n",
      "         -1.1936e-06, -3.0576e-08, -1.3276e-07,  1.8906e-08, -3.4256e-07,\n",
      "         -4.7502e-07,  3.0308e-06, -1.9504e-06,  1.6687e-07, -1.4067e-07,\n",
      "         -3.2389e-06, -1.9622e-06,  1.2840e-06, -5.1929e-06, -5.7583e-09,\n",
      "          8.1122e-09, -2.4988e-07,  6.2490e-08, -7.1051e-06,  6.2890e-06,\n",
      "          1.5064e-06, -1.2966e-08, -6.0107e-07, -1.0128e-06,  2.2346e-06,\n",
      "         -9.7218e-09,  5.6699e-06, -2.6947e-06, -1.3278e-05, -1.7069e-08,\n",
      "          5.2235e-07, -9.7624e-09, -3.6445e-07, -7.0137e-07,  1.5170e-06,\n",
      "         -1.1140e-08, -8.9419e-08,  3.2134e-06, -3.2035e-08,  7.0367e-08,\n",
      "          3.3939e-08,  7.3384e-06,  6.1663e-07, -2.0497e-07, -3.0772e-08,\n",
      "         -1.4267e-06,  8.9570e-08, -4.0509e-06, -1.8876e-05,  2.0238e-07,\n",
      "          6.6535e-08,  2.4708e-07, -4.8657e-09, -3.6573e-07,  7.8544e-09,\n",
      "         -6.8982e-08,  4.8089e-07, -1.4776e-09, -2.3595e-08, -6.8129e-06,\n",
      "          6.9381e-10, -2.3210e-07, -1.1164e-06, -2.4618e-06,  2.6487e-09,\n",
      "          4.8181e-08, -9.3347e-06,  5.3015e-06,  3.2759e-07, -4.6297e-09,\n",
      "         -1.5417e-08, -6.7973e-10,  1.9806e-06,  2.1304e-08, -2.3451e-09,\n",
      "          9.6351e-06,  8.7539e-07,  6.9686e-08, -1.6368e-06, -7.8648e-06,\n",
      "          4.2370e-07, -9.8600e-08, -4.2517e-07,  4.9696e-07, -6.1395e-09,\n",
      "          5.2236e-09,  1.1263e-06,  3.4314e-09,  8.3416e-08, -2.0653e-08,\n",
      "          3.1180e-06,  4.5942e-06,  5.5429e-07, -1.1697e-08,  3.1334e-06,\n",
      "          3.5614e-07, -2.9946e-07,  2.4542e-06,  2.1006e-07,  3.4275e-07,\n",
      "          5.9653e-07, -6.7314e-08, -2.6987e-06, -1.2318e-07, -1.9951e-05,\n",
      "          4.1791e-09,  9.0749e-08,  1.4211e-07, -1.2994e-08,  7.7482e-09,\n",
      "          2.0325e-06,  3.2872e-08,  1.8770e-07, -3.6565e-08, -4.4565e-07,\n",
      "          5.4924e-09, -2.8814e-07, -1.0286e-05, -1.2779e-05, -3.6414e-09,\n",
      "          4.2905e-08,  9.2467e-08,  1.1076e-06,  6.8068e-07,  1.0772e-08,\n",
      "         -2.5271e-07, -1.2429e-08, -1.7459e-07,  3.1883e-06,  3.0819e-08,\n",
      "          8.3064e-07,  1.4967e-06,  3.5149e-07,  1.9145e-06,  1.0433e-05,\n",
      "         -4.0918e-06,  3.2103e-06, -1.2838e-09, -1.7808e-08, -9.6167e-06,\n",
      "          4.1772e-06,  9.4751e-08,  7.0124e-09, -5.8482e-08,  4.1038e-06,\n",
      "          9.3137e-08,  1.4279e-08, -8.8191e-09,  2.0127e-08, -2.0090e-07,\n",
      "          1.7485e-06,  5.9445e-09, -6.4444e-07, -5.8941e-07, -3.4347e-08,\n",
      "          1.4494e-06,  1.6853e-08, -5.9534e-08, -1.2081e-06, -1.1556e-07,\n",
      "         -2.2007e-06, -1.6601e-06,  2.1164e-07,  6.2411e-08,  1.6189e-06,\n",
      "         -1.4095e-05,  1.2285e-05,  1.9836e-09, -1.1830e-09,  1.4245e-07,\n",
      "         -4.2388e-09, -7.5148e-06, -5.9049e-06, -4.7331e-06,  1.6933e-08,\n",
      "         -4.7904e-08,  3.3373e-09, -1.2266e-08, -1.7535e-08, -9.2677e-07,\n",
      "         -1.2172e-06, -3.0298e-08, -1.1858e-07, -6.2225e-06,  7.0328e-08,\n",
      "          6.8054e-08, -7.1502e-07, -2.6513e-07,  2.7297e-06,  2.3259e-09,\n",
      "          7.3877e-08, -7.2392e-10,  3.1065e-07,  2.8778e-08,  4.6586e-09,\n",
      "          2.0189e-07, -1.7172e-09,  1.2696e-07,  2.4887e-09, -6.3959e-08,\n",
      "          1.3936e-07, -1.6669e-07,  4.6270e-07,  1.2864e-07, -5.1439e-06,\n",
      "          1.0699e-07, -1.8113e-06,  8.5400e-10,  2.5707e-07, -3.5609e-06,\n",
      "          1.6756e-05,  3.0093e-06, -4.4207e-07, -9.3632e-08, -1.4868e-05,\n",
      "          1.2654e-07, -1.3183e-05, -1.4881e-06,  2.0645e-05,  4.9586e-07,\n",
      "          6.1311e-08,  5.1878e-06,  1.2307e-05, -5.6192e-07, -6.8642e-07,\n",
      "          3.7236e-06,  5.0237e-09, -2.3857e-08, -7.7664e-08, -8.3865e-09,\n",
      "          1.5629e-06,  2.6984e-07,  5.2005e-08, -3.9755e-06, -1.7400e-06,\n",
      "         -1.7448e-07,  1.1338e-07, -1.3464e-06, -4.8910e-06,  1.4077e-07,\n",
      "         -4.3792e-07, -4.1272e-08,  7.2853e-08, -8.9580e-08, -4.6144e-09,\n",
      "         -7.4259e-07, -1.5908e-05,  3.6718e-06,  9.4396e-06,  5.6789e-08,\n",
      "          1.7059e-09,  2.5213e-06, -4.6034e-08,  5.2949e-08,  5.4982e-08,\n",
      "         -5.4109e-08,  2.1200e-06,  3.1344e-07,  9.6897e-08, -2.8529e-07,\n",
      "         -1.0593e-07, -1.7308e-06,  2.8259e-07, -3.0128e-06,  2.8530e-07,\n",
      "         -4.6122e-07, -6.8373e-06, -3.2311e-07, -2.2906e-08, -2.5482e-07,\n",
      "         -2.2110e-09, -1.1769e-06,  7.2977e-08, -5.7473e-07, -1.1251e-07,\n",
      "          1.0031e-07,  6.1135e-07, -1.1308e-08, -2.1041e-06,  2.4877e-08,\n",
      "          4.2130e-06, -6.9241e-09, -9.0283e-08, -7.0564e-08,  3.2511e-09,\n",
      "          2.1240e-06,  2.7619e-07, -2.1908e-06,  5.6387e-08,  2.2908e-08,\n",
      "         -5.2895e-09,  5.7410e-06, -2.2186e-07,  8.8366e-09, -2.1829e-06,\n",
      "         -1.5606e-07, -7.4005e-09,  1.4464e-07,  6.0941e-08,  1.8124e-07,\n",
      "         -5.9934e-06, -4.1108e-08, -9.2321e-06, -1.2959e-07, -2.9012e-06,\n",
      "          4.0460e-06,  4.3070e-06,  1.2612e-07,  5.2592e-08, -2.4102e-05,\n",
      "          1.0350e-07, -1.0870e-08, -5.3403e-07,  2.3729e-08, -6.2486e-06,\n",
      "          6.4484e-07, -2.0996e-05,  2.8569e-06,  6.8823e-08,  4.8678e-07,\n",
      "         -8.1302e-07,  3.0096e-07, -5.7045e-07]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 9,  7],\n",
      "         [ 7, 10],\n",
      "         [ 4, 10],\n",
      "         [ 9,  5],\n",
      "         [ 7,  3],\n",
      "         [ 7,  7],\n",
      "         [10,  2],\n",
      "         [ 8,  1],\n",
      "         [ 9,  4],\n",
      "         [ 2,  1],\n",
      "         [ 4, 18],\n",
      "         [ 5, 10],\n",
      "         [ 8, 18],\n",
      "         [ 6,  3],\n",
      "         [ 3,  2],\n",
      "         [ 3,  0],\n",
      "         [ 6, 16],\n",
      "         [ 1,  3],\n",
      "         [ 9, 10],\n",
      "         [ 6,  2],\n",
      "         [ 2, 14],\n",
      "         [ 2,  6],\n",
      "         [ 6,  1],\n",
      "         [ 7, 16],\n",
      "         [ 3, 16],\n",
      "         [ 9,  3],\n",
      "         [ 6, 18],\n",
      "         [ 3, 18],\n",
      "         [10, 15],\n",
      "         [10,  0],\n",
      "         [ 3, 17],\n",
      "         [ 8,  0],\n",
      "         [ 4, 13],\n",
      "         [ 3, 11],\n",
      "         [ 1, 17],\n",
      "         [ 8, 11],\n",
      "         [ 0, 13],\n",
      "         [ 8,  8],\n",
      "         [ 9,  6],\n",
      "         [ 1,  6],\n",
      "         [ 1, 15],\n",
      "         [ 5,  7],\n",
      "         [ 2, 16],\n",
      "         [ 6,  6],\n",
      "         [ 1, 12],\n",
      "         [ 7, 18],\n",
      "         [ 2, 18],\n",
      "         [ 8,  2],\n",
      "         [ 0, 16],\n",
      "         [ 3,  5],\n",
      "         [ 6,  5],\n",
      "         [ 0,  7],\n",
      "         [ 4, 14],\n",
      "         [ 5, 12],\n",
      "         [ 0,  3],\n",
      "         [ 1,  7],\n",
      "         [ 2,  7],\n",
      "         [ 3,  8],\n",
      "         [10, 18],\n",
      "         [10,  8],\n",
      "         [ 9,  8],\n",
      "         [ 9, 14],\n",
      "         [ 1,  9],\n",
      "         [ 2,  0],\n",
      "         [ 4,  9],\n",
      "         [10,  1],\n",
      "         [ 5,  3],\n",
      "         [ 5,  9],\n",
      "         [ 0, 10],\n",
      "         [ 6, 17],\n",
      "         [ 7,  8],\n",
      "         [ 1, 10],\n",
      "         [ 4,  2],\n",
      "         [ 0,  5],\n",
      "         [ 6, 10],\n",
      "         [ 0,  6],\n",
      "         [ 7, 14],\n",
      "         [ 5,  5],\n",
      "         [ 5, 16],\n",
      "         [ 4, 15],\n",
      "         [ 9, 13],\n",
      "         [ 6,  9],\n",
      "         [ 5,  4],\n",
      "         [ 8,  7],\n",
      "         [ 3,  4],\n",
      "         [ 7,  1],\n",
      "         [ 5,  6],\n",
      "         [ 1,  2],\n",
      "         [10, 12],\n",
      "         [ 1, 18],\n",
      "         [10, 14],\n",
      "         [ 7,  5],\n",
      "         [ 4,  6],\n",
      "         [ 0, 11],\n",
      "         [ 9,  0],\n",
      "         [ 1,  5],\n",
      "         [ 3, 13],\n",
      "         [ 2,  3],\n",
      "         [ 6,  0],\n",
      "         [ 9, 11],\n",
      "         [ 8,  5],\n",
      "         [ 3,  9],\n",
      "         [10, 11],\n",
      "         [ 7, 15],\n",
      "         [ 7, 11],\n",
      "         [ 2, 15],\n",
      "         [ 4, 11],\n",
      "         [ 3,  7],\n",
      "         [ 6, 14],\n",
      "         [ 6, 12],\n",
      "         [ 7,  4],\n",
      "         [ 3, 12],\n",
      "         [ 8, 14],\n",
      "         [ 5, 17],\n",
      "         [ 4, 16],\n",
      "         [ 1,  8],\n",
      "         [ 9, 16],\n",
      "         [ 0,  8],\n",
      "         [ 2, 13],\n",
      "         [ 9, 15],\n",
      "         [ 9, 17],\n",
      "         [ 5,  2],\n",
      "         [ 8, 12],\n",
      "         [ 0,  4],\n",
      "         [ 4,  7],\n",
      "         [ 4, 17],\n",
      "         [ 1, 14],\n",
      "         [10, 17],\n",
      "         [ 5, 15],\n",
      "         [ 6, 11],\n",
      "         [ 4, 12],\n",
      "         [ 5, 18],\n",
      "         [ 2,  8],\n",
      "         [ 3,  1],\n",
      "         [ 4,  3],\n",
      "         [ 3,  3],\n",
      "         [ 2, 11],\n",
      "         [ 8, 15],\n",
      "         [ 4,  4],\n",
      "         [ 7, 13],\n",
      "         [ 0, 14],\n",
      "         [10,  5],\n",
      "         [ 0, 15],\n",
      "         [ 5, 13],\n",
      "         [ 6,  4],\n",
      "         [ 7, 12],\n",
      "         [ 9, 12],\n",
      "         [ 6,  8],\n",
      "         [ 2, 12],\n",
      "         [ 3, 15],\n",
      "         [ 5, 14],\n",
      "         [ 4,  5],\n",
      "         [ 0,  2],\n",
      "         [ 2, 17],\n",
      "         [ 0,  1],\n",
      "         [ 0,  9],\n",
      "         [ 9,  1],\n",
      "         [ 2, 10],\n",
      "         [ 0, 18],\n",
      "         [ 7,  0],\n",
      "         [ 1, 13],\n",
      "         [ 8,  6],\n",
      "         [ 1,  4],\n",
      "         [ 5,  8],\n",
      "         [ 5,  0],\n",
      "         [10, 16],\n",
      "         [ 4,  0],\n",
      "         [ 1, 16],\n",
      "         [ 4,  1],\n",
      "         [ 7,  6],\n",
      "         [ 6, 13],\n",
      "         [ 2,  5],\n",
      "         [ 7,  2],\n",
      "         [ 3, 10],\n",
      "         [ 5,  1],\n",
      "         [ 8, 16],\n",
      "         [10,  9],\n",
      "         [ 7, 17],\n",
      "         [ 1,  1],\n",
      "         [ 1,  0],\n",
      "         [ 6,  7],\n",
      "         [ 8, 13],\n",
      "         [ 8,  3],\n",
      "         [10, 10],\n",
      "         [10,  6],\n",
      "         [ 0,  0],\n",
      "         [ 3,  6],\n",
      "         [ 8,  4],\n",
      "         [ 6, 15],\n",
      "         [ 9, 18],\n",
      "         [ 4,  8],\n",
      "         [10,  7],\n",
      "         [ 5, 11],\n",
      "         [ 7,  9],\n",
      "         [10,  3],\n",
      "         [ 2,  4],\n",
      "         [ 2,  2],\n",
      "         [ 8, 17],\n",
      "         [ 8, 10],\n",
      "         [ 9,  9],\n",
      "         [ 1, 11],\n",
      "         [ 0, 17],\n",
      "         [ 3, 14],\n",
      "         [10, 13],\n",
      "         [ 0, 12],\n",
      "         [ 2,  9],\n",
      "         [ 8,  9],\n",
      "         [ 9,  2],\n",
      "         [10,  4]]]), (11, 19)), 'cls_output': tensor([[0.9757]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/164085027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 1817\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch_junsheng_39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29fd19f11c6b89e267402bb3227bc1208f7e2c9719aa03eba13baf7684fe5867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
