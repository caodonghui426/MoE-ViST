{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__module__': '__main__',\n",
       " 'debug': (True,),\n",
       " 'exp_name': 'vilt',\n",
       " 'seed': 101,\n",
       " 'batch_size': 4096,\n",
       " 'train_batch_size': 1,\n",
       " 'valid_batch_size': 4,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'root_path': 'E:\\\\\\\\Download\\\\\\\\xiangguan',\n",
       " 'train_transform_keys': ['pixelbert'],\n",
       " 'val_transform_keys': ['pixelbert'],\n",
       " 'img_size': 384,\n",
       " 'max_image_len': -1,\n",
       " 'patch_size': 32,\n",
       " 'draw_false_image': 1,\n",
       " 'image_only': False,\n",
       " 'senser_input_num': 11,\n",
       " 'vqav2_label_size': 3129,\n",
       " 'max_text_len': 40,\n",
       " 'tokenizer': 'bert-base-uncased',\n",
       " 'vocab_size': 30522,\n",
       " 'whole_word_masking': False,\n",
       " 'mlm_prob': 0.15,\n",
       " 'draw_false_text': 0,\n",
       " 'vit': 'vit_base_patch32_384',\n",
       " 'hidden_size': 768,\n",
       " 'num_heads': 12,\n",
       " 'num_layers': 12,\n",
       " 'mlp_ratio': 4,\n",
       " 'drop_rate': 0.1,\n",
       " 'optim_type': 'adamw',\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.01,\n",
       " 'decay_power': 1,\n",
       " 'max_epoch': 3,\n",
       " 'max_steps': 25000,\n",
       " 'warmup_steps': 2500,\n",
       " 'end_lr': 0,\n",
       " 'lr_mult': 1,\n",
       " 'get_recall_metric': False,\n",
       " 'resume_from': None,\n",
       " 'fast_dev_run': False,\n",
       " 'val_check_interval': 1.0,\n",
       " 'test_only': False,\n",
       " 'data_root': '',\n",
       " 'log_dir': 'result',\n",
       " 'per_gpu_batchsize': 0,\n",
       " 'num_gpus': 1,\n",
       " 'num_nodes': 1,\n",
       " 'load_path': 'weights/vilt_200k_mlm_itm.ckpt',\n",
       " 'num_workers': 1,\n",
       " 'precision': 16,\n",
       " '__dict__': <attribute '__dict__' of 'config' objects>,\n",
       " '__weakref__': <attribute '__weakref__' of 'config' objects>,\n",
       " '__doc__': None}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    debug = True,\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 1\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    root_path = r'E:\\\\Download\\\\xiangguan' # 存放数据的根目录\n",
    "    \n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Sensor\n",
    "    senser_input_num = 11\n",
    "    \n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    decay_power = 1\n",
    "    max_epoch = 3\n",
    "    max_steps = 25000\n",
    "    warmup_steps = 2500\n",
    "    end_lr = 0\n",
    "    lr_mult = 1  # multiply lr for downstream heads\n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "    # PL Trainer Setting\n",
    "    resume_from = None\n",
    "    fast_dev_run = False\n",
    "    val_check_interval = 1.0\n",
    "    test_only = False\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "config = vars(config)\n",
    "config = dict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=0):\n",
    "    import torch\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import random\n",
    "    torch.manual_seed(seed)  # 为CPU设置随机种子\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    if torch.cuda.is_available():\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed(seed)  # 为当前GPU设置随机种子\n",
    "        torch.cuda.manual_seed_all(seed)  # 为所有GPU设置随机种子\n",
    "        #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "setup_seed(seed=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "翔冠数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>device_id</th>\n",
       "      <th>collect_time</th>\n",
       "      <th>url</th>\n",
       "      <th>picture_id</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>illuminance</th>\n",
       "      <th>soil_temperature</th>\n",
       "      <th>soil_humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>photosynthetic</th>\n",
       "      <th>sun_exposure_time</th>\n",
       "      <th>COz</th>\n",
       "      <th>soil_ph</th>\n",
       "      <th>weather_id</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>xiangguanD3</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.66</td>\n",
       "      <td>51.78</td>\n",
       "      <td>76.73</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.02</td>\n",
       "      <td>981.09</td>\n",
       "      <td>2.48</td>\n",
       "      <td>7.92</td>\n",
       "      <td>0.1</td>\n",
       "      <td>659.34</td>\n",
       "      <td>6.80</td>\n",
       "      <td>45288.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\\\Download\\\\xiangguan\\pic\\xiangguanD3-2021-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xiangguanD3</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.76</td>\n",
       "      <td>56.13</td>\n",
       "      <td>226.02</td>\n",
       "      <td>8.32</td>\n",
       "      <td>8.02</td>\n",
       "      <td>980.99</td>\n",
       "      <td>3.37</td>\n",
       "      <td>28.71</td>\n",
       "      <td>0.1</td>\n",
       "      <td>660.33</td>\n",
       "      <td>6.80</td>\n",
       "      <td>45292.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\\\Download\\\\xiangguan\\pic\\xiangguanD3-2021-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>xiangguanD4</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.70</td>\n",
       "      <td>52.00</td>\n",
       "      <td>240.80</td>\n",
       "      <td>5.50</td>\n",
       "      <td>26.10</td>\n",
       "      <td>75.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>158.00</td>\n",
       "      <td>6.82</td>\n",
       "      <td>45289.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\\\Download\\\\xiangguan\\pic\\xiangguanD4-2021-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>xiangguanD4</td>\n",
       "      <td>14/5/2021 04</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.90</td>\n",
       "      <td>55.80</td>\n",
       "      <td>670.80</td>\n",
       "      <td>5.50</td>\n",
       "      <td>26.00</td>\n",
       "      <td>15.30</td>\n",
       "      <td>2.37</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>157.00</td>\n",
       "      <td>6.82</td>\n",
       "      <td>45293.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\\\Download\\\\xiangguan\\pic\\xiangguanD4-2021-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>xiangguanD3</td>\n",
       "      <td>14/5/2021 05</td>\n",
       "      <td>https://daqing-haikang-image.oss-cn-hangzhou.a...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.76</td>\n",
       "      <td>59.90</td>\n",
       "      <td>428.97</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.02</td>\n",
       "      <td>980.99</td>\n",
       "      <td>2.13</td>\n",
       "      <td>57.42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>659.34</td>\n",
       "      <td>6.80</td>\n",
       "      <td>45296.0</td>\n",
       "      <td>出苗</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\\\Download\\\\xiangguan\\pic\\xiangguanD3-2021-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    device_id  collect_time  \\\n",
       "0           0  xiangguanD3  14/5/2021 04   \n",
       "1           1  xiangguanD3  14/5/2021 04   \n",
       "2           2  xiangguanD4  14/5/2021 04   \n",
       "3           3  xiangguanD4  14/5/2021 04   \n",
       "4           4  xiangguanD3  14/5/2021 05   \n",
       "\n",
       "                                                 url  picture_id  temperature  \\\n",
       "0  https://daqing-haikang-image.oss-cn-hangzhou.a...        34.0        13.66   \n",
       "1  https://daqing-haikang-image.oss-cn-hangzhou.a...        34.0        13.76   \n",
       "2  https://daqing-haikang-image.oss-cn-hangzhou.a...        35.0        13.70   \n",
       "3  https://daqing-haikang-image.oss-cn-hangzhou.a...        35.0        13.90   \n",
       "4  https://daqing-haikang-image.oss-cn-hangzhou.a...        38.0        13.76   \n",
       "\n",
       "   humidity  illuminance  soil_temperature  soil_humidity  pressure  \\\n",
       "0     51.78        76.73              8.42           8.02    981.09   \n",
       "1     56.13       226.02              8.32           8.02    980.99   \n",
       "2     52.00       240.80              5.50          26.10     75.00   \n",
       "3     55.80       670.80              5.50          26.00     15.30   \n",
       "4     59.90       428.97              8.22           8.02    980.99   \n",
       "\n",
       "   wind_speed  photosynthetic  sun_exposure_time     COz  soil_ph  weather_id  \\\n",
       "0        2.48            7.92                0.1  659.34     6.80     45288.0   \n",
       "1        3.37           28.71                0.1  660.33     6.80     45292.0   \n",
       "2        1.85            6.00                0.1  158.00     6.82     45289.0   \n",
       "3        2.37           18.00                0.1  157.00     6.82     45293.0   \n",
       "4        2.13           57.42                0.1  659.34     6.80     45296.0   \n",
       "\n",
       "  label_name  label                                         image_path  \n",
       "0         出苗      0  E:\\\\Download\\\\xiangguan\\pic\\xiangguanD3-2021-0...  \n",
       "1         出苗      0  E:\\\\Download\\\\xiangguan\\pic\\xiangguanD3-2021-0...  \n",
       "2         出苗      0  E:\\\\Download\\\\xiangguan\\pic\\xiangguanD4-2021-0...  \n",
       "3         出苗      0  E:\\\\Download\\\\xiangguan\\pic\\xiangguanD4-2021-0...  \n",
       "4         出苗      0  E:\\\\Download\\\\xiangguan\\pic\\xiangguanD3-2021-0...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xiangguan = pd.read_csv(\"xiangguan-corn.csv\")\n",
    "df_xiangguan['image_path'] = df_xiangguan['url'].map(lambda x:os.path.join(config['root_path'],'pic',x.split('/')[-1]))\n",
    "df_xiangguan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化非object列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'picture_id', 'temperature', 'humidity', 'illuminance', 'soil_temperature', 'soil_humidity', 'pressure', 'wind_speed', 'photosynthetic', 'sun_exposure_time', 'COz', 'soil_ph', 'weather_id', 'label']\n"
     ]
    }
   ],
   "source": [
    "number_title = []\n",
    "for title in df_xiangguan:\n",
    "    # print(df_xiangguan[title].head())\n",
    "    if df_xiangguan[title].dtype != \"object\":\n",
    "        number_title.append(title)\n",
    "        x_min = df_xiangguan[title].min()\n",
    "        x_max = df_xiangguan[title].max()\n",
    "        # print(x_min,x_max)\n",
    "        df_xiangguan[title] = df_xiangguan[title].map(lambda x:(x-x_min)/(x_max - x_min))\n",
    "print(number_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 11\n"
     ]
    }
   ],
   "source": [
    "xiangguan_sensor = ['temperature', 'humidity', 'illuminance', 'soil_temperature', 'soil_humidity', 'pressure', 'wind_speed', 'photosynthetic', 'sun_exposure_time', 'COz', 'soil_ph']\n",
    "\n",
    "df_xiangguan['sensor'] = df_xiangguan[xiangguan_sensor].values.tolist()\n",
    "print(\"input dim:\",len(xiangguan_sensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_xiangguan.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.DataFrame({\n",
    "#     \"a\":np.random.randn(500),\n",
    "#     \"b\":np.random.randn(500),\n",
    "#     \"c\":np.random.randn(500),\n",
    "#     \"d\":np.random.randn(500),\n",
    "#     \"image_path\":\"assets/vilt.png\",\n",
    "    \n",
    "# })\n",
    "# test_df['label'] = test_df.a + 2*test_df.b + 3*test_df.c + 4*test_df.d\n",
    "# test_df['sensor'] = test_df[['a','b','c','d','a','b','c','d','a','b']].values.tolist()\n",
    "# test_df.head()\n",
    "# test_df.to_csv(\"test_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 21)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_xiangguan\n",
    "if config['debug']:\n",
    "    df = df[:100]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransforms = transforms.Compose([\n",
    "    transforms.Resize((config[\"img_size\"],config[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.7136, 0.7118, 0.6788],\n",
    "    std=[0.3338, 0.3453, 0.3020],\n",
    "    \n",
    ")\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img =  Image.open(path).convert('RGB')\n",
    "    img = myTransforms(img)\n",
    "    return img\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df         = df\n",
    "        self.label      = label\n",
    "        self.sensors = df['sensor'].tolist()\n",
    "        self.img_paths  = df['image_path'].tolist()   \n",
    "        if self.label:\n",
    "            self.labels = df['label'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path  = self.img_paths[index]\n",
    "        img = load_img(img_path)\n",
    "        sensor = self.sensors[index]\n",
    "        sensor = torch.tensor(sensor).unsqueeze(0) #[1,n]\n",
    "        if self.label:\n",
    "            label = self.labels[index]\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
    "        else:\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BuildDataset(df=df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'],shuffle=True)\n",
    "valid_loader = DataLoader(train_dataset, batch_size=config['valid_batch_size'],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 384])\n",
      "torch.Size([1, 1, 11])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_14200\\3759876696.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    }
   ],
   "source": [
    "img,sensor,label = next(iter(train_loader))\n",
    "print(img.shape)\n",
    "print(sensor.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self, config,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config[\"hidden_size\"]) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config[\"hidden_size\"])\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # if self.config[\"load_path\"] == \"\":\n",
    "        self.transformer = getattr(vit, self.config[\"vit\"])(\n",
    "                pretrained=False, config=self.config\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config[\"hidden_size\"])\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config[\"hidden_size\"],output_class_n)\n",
    "        # ===================== Downstream ===================== #\n",
    "        # if (\n",
    "        #     self.config[\"load_path\"] != \"\"\n",
    "        #     and not self.config[\"test_only\"]\n",
    "        # ):\n",
    "        #     ckpt = torch.load(self.config[\"load_path\"], map_location=\"cpu\")\n",
    "        #     if isinstance(ckpt,OrderedDict):\n",
    "\n",
    "        #         state_dict = ckpt\n",
    "        #     else:\n",
    "        #         state_dict = ckpt[\"state_dict\"]\n",
    "        #     self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        hs = self.config[\"hidden_size\"]\n",
    "\n",
    "        # vilt_utils.set_metrics(self) # 设定模型评价\n",
    "\n",
    "        # ===================== load downstream (test_only) ======================\n",
    "\n",
    "        if self.config[\"load_path\"] != \"\" and self.config[\"test_only\"]:\n",
    "            ckpt = torch.load(self.config[\"load_path\"], map_location=\"cpu\")\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config['device'])\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config['device'])\n",
    "       \n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=self.config[\"max_image_len\"],\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config['device']) # 序列数量\n",
    "        image_masks = image_masks.to(config['device'])\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config['device'])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config['device'])\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        # cls_output = m(cls_output)\n",
    "\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 990.47 MiB already allocated; 16.14 MiB free; 1.12 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m sensorViLTransformerSS(config,sensor_class_n\u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39msenser_input_num\u001b[39m\u001b[39m'\u001b[39m],output_class_n \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(config[\u001b[39m'\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:612\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    610\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 612\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    358\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 359\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    361\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    363\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    364\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    370\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    358\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 359\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    361\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    363\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    364\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    370\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    358\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 359\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    361\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    363\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    364\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    370\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:381\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m param \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m     \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    378\u001b[0m     \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 381\u001b[0m         param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    382\u001b[0m     should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    383\u001b[0m     \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:610\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m    609\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 610\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 990.47 MiB already allocated; 16.14 MiB free; 1.12 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model = sensorViLTransformerSS(config,sensor_class_n= config['senser_input_num'],output_class_n = 1)\n",
    "model.to(config['device'])\n",
    "print(config['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.mse_loss #均方误差损失函数\n",
    "# criterion = F.mae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
    "    for step, (img, sensor,label) in pbar:         \n",
    "        # img = img.to(device, dtype=torch.float)\n",
    "        # sensor  = sensor.to(device, dtype=torch.float)\n",
    "        # label  = label.to(device, dtype=torch.float)\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred = model(batch)\n",
    "        label = label.to(config['device']).unsqueeze(1)\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        #一坨优化\n",
    "        optimizer.zero_grad()#每一次反向传播之前都要归零梯度\n",
    "        loss.backward()      #反向传播\n",
    "        optimizer.step()     #固定写法\n",
    "        scheduler.step()\n",
    "     \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_mem=f'{mem:0.2f} GB')\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "    history = defaultdict(list)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        \n",
    "        # val_loss, val_scores = valid_one_epoch(model, valid_loader, \n",
    "                                                #  device=CFG.device, \n",
    "                                                #  epoch=epoch)\n",
    "        # val_dice, val_jaccard = val_scores\n",
    "    \n",
    "        history['Train Loss'].append(train_loss)\n",
    "        # history['Valid Loss'].append(val_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        # deep copy the model\n",
    "        # if val_dice >= best_dice:\n",
    "            # print(f\"{c_}Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n",
    "            # best_dice    = val_dice\n",
    "            # best_jaccard = val_jaccard\n",
    "            # best_epoch   = epoch\n",
    "            # run.summary[\"Best Dice\"]    = best_dice\n",
    "            # run.summary[\"Best Jaccard\"] = best_jaccard\n",
    "            # run.summary[\"Best Epoch\"]   = best_epoch\n",
    "            # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            # PATH = os.path.join(CFG.model_output_path, f\"best_epoch-{fold:02d}.bin\")\n",
    "            # torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            # wandb.save(PATH)\n",
    "            # print(f\"Model Saved{sr_} to path:\",PATH)\n",
    "            \n",
    "        # last_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # PATH = os.path.join(CFG.model_output_path,f\"last_epoch-{fold:02d}.bin\")\n",
    "        # torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "    \n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.2, weight_decay=0.0001)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000, \n",
    "                                                   eta_min=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: GeForce GTX 1050\n",
      "\n",
      "Epoch 1/3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_14200\\3759876696.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
      "Train :   0%|          | 0/50 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 994.71 MiB already allocated; 14.14 MiB free; 1.12 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model, history \u001b[39m=\u001b[39m run_training(model, optimizer, scheduler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                 device\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                 num_epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 35\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(model, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, scheduler, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                    dataloader\u001b[39m=\u001b[39;49mtrain_loader, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                    device\u001b[39m=\u001b[39;49mdevice, epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# val_loss, val_scores = valid_one_epoch(model, valid_loader, \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                                         \u001b[39m#  device=CFG.device, \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                                         \u001b[39m#  epoch=epoch)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# val_dice, val_jaccard = val_scores\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32md:\\Desktop\\hit\\多模态\\代码\\ViLT\\my_vilt.ipynb Cell 35\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#一坨优化\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\u001b[39m#每一次反向传播之前都要归零梯度\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()      \u001b[39m#反向传播\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()     \u001b[39m#固定写法\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/hit/%E5%A4%9A%E6%A8%A1%E6%80%81/%E4%BB%A3%E7%A0%81/ViLT/my_vilt.ipynb#X36sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[0;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    215\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    216\u001b[0m         relevant_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         retain_graph\u001b[39m=\u001b[39mretain_graph,\n\u001b[0;32m    220\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph)\n\u001b[1;32m--> 221\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 130\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    131\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph,\n\u001b[0;32m    132\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 994.71 MiB already allocated; 14.14 MiB free; 1.12 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                                device=config['device'],\n",
    "                                num_epochs=config['max_epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'embedding_test_dict.pt')\n",
    "print(model)\n",
    "model.eval()\n",
    "device = config[\"device\"]\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = dict()\n",
    "    batch[\"image\"] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n",
    "\n",
    "examples=[\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "        ],\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "        ],\n",
    "        [\n",
    "            \"6212487_1cca7f3f_1024x1024.jpg\",\n",
    "        ],\n",
    "    ],\n",
    "\n",
    "n = 1\n",
    "sensor = torch.randn(1,1,config['senser_input_num'])\n",
    "out = infer(examples[0][n][0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].cpu().numpy()[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e32618030883af29da10316c76c83da9a02f65dadf9a9b09d160d0d4f5840e5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
