{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    debug = False\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # root_path = r'E:\\\\Download\\\\xiangguan' # 存放数据的根目录\n",
    "    root_path = r'/home/junsheng/data/xiangguan' # 存放数据的根目录\n",
    "    n_fold = 5\n",
    "\n",
    "    model_name = \"sensorViLOnlyTransformerSS\"\n",
    "    # wandb \n",
    "    wandb_name = \"vilt|水稻|290仅图片\"\n",
    "    \n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Sensor\n",
    "    # senser_input_num = 11 # 翔冠的传感器参数\n",
    "    senser_input_num = 19 # 天航的传感器参数\n",
    "    \n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-3 #0.0015#2e-3 #\n",
    "    weight_decay = 1e-4 # 0.01 ->1e-4\n",
    "    decay_power = 1\n",
    "    max_epoch = 50\n",
    "    # T_max = 8000/train_batch_size*max_epoch \n",
    "    T_max = 1000/train_batch_size*max_epoch \n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "# config = vars(config)\n",
    "# config = dict(config)\n",
    "config\n",
    "\n",
    "if config.debug:\n",
    "    config.max_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)  # 为CPU设置随机种子\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)  # 为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有GPU设置随机种子\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "setup_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_MODE\"] = 'dryrun' # 离线模式\n",
    "try:\n",
    "    # wandb.log(key=\"*******\") # if debug\n",
    "    wandb.login() # storage in ~/.netrc file\n",
    "    anonymous = None\n",
    "except:\n",
    "    anonymous = \"must\"\n",
    "    print('\\nGet your W&B access token from here: https://wandb.ai/authorize\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pic_key</th>\n",
       "      <th>date_hour</th>\n",
       "      <th>date</th>\n",
       "      <th>co2</th>\n",
       "      <th>stemp</th>\n",
       "      <th>stemp2</th>\n",
       "      <th>stemp3</th>\n",
       "      <th>stemp4</th>\n",
       "      <th>stemp5</th>\n",
       "      <th>...</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>press</th>\n",
       "      <th>solar</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind_d</th>\n",
       "      <th>wind_sp</th>\n",
       "      <th>LAI</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>282</td>\n",
       "      <td>/789/1655496854_1655496673_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>624.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>991.1</td>\n",
       "      <td>2.52</td>\n",
       "      <td>17.26</td>\n",
       "      <td>274.3</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655496854_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283</td>\n",
       "      <td>/789/1655496854_1655496673_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>624.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>991.2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>17.18</td>\n",
       "      <td>268.7</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655496854_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>284</td>\n",
       "      <td>/789/1655504090_1655503874_4.jpg</td>\n",
       "      <td>2022-06-18 06</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>617.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>991.9</td>\n",
       "      <td>8.84</td>\n",
       "      <td>17.75</td>\n",
       "      <td>248.6</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655504090_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285</td>\n",
       "      <td>/789/1655504090_1655503874_4.jpg</td>\n",
       "      <td>2022-06-18 06</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>617.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>9.20</td>\n",
       "      <td>17.83</td>\n",
       "      <td>265.7</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655504090_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>/789/1655511249_1655511073_4.jpg</td>\n",
       "      <td>2022-06-18 08</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>604.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.1</td>\n",
       "      <td>18.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>992.6</td>\n",
       "      <td>17.75</td>\n",
       "      <td>18.98</td>\n",
       "      <td>275.4</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655511249_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                           pic_key      date_hour        date    co2  \\\n",
       "0    282  /789/1655496854_1655496673_4.jpg  2022-06-18 04  2022-06-18  624.0   \n",
       "1    283  /789/1655496854_1655496673_4.jpg  2022-06-18 04  2022-06-18  624.0   \n",
       "2    284  /789/1655504090_1655503874_4.jpg  2022-06-18 06  2022-06-18  617.0   \n",
       "3    285  /789/1655504090_1655503874_4.jpg  2022-06-18 06  2022-06-18  617.0   \n",
       "4    286  /789/1655511249_1655511073_4.jpg  2022-06-18 08  2022-06-18  604.0   \n",
       "\n",
       "   stemp  stemp2  stemp3  stemp4  stemp5  ...  pm10  pm25  press  solar  \\\n",
       "0   19.8    19.6    19.8    19.3    19.1  ...   6.0   6.0  991.1   2.52   \n",
       "1   19.8    19.6    19.8    19.3    19.1  ...   7.0   7.0  991.2   5.93   \n",
       "2   19.5    19.5    19.6    19.1    19.0  ...   5.0   5.0  991.9   8.84   \n",
       "3   19.5    19.4    19.5    19.1    19.0  ...   3.0   3.0  992.0   9.20   \n",
       "4   19.3    19.2    19.4    19.1    18.9  ...   1.0   1.0  992.6  17.75   \n",
       "\n",
       "    temp wind_d wind_sp       LAI  \\\n",
       "0  17.26  274.3    3.75  1.626667   \n",
       "1  17.18  268.7    2.67  1.626667   \n",
       "2  17.75  248.6    2.07  1.626667   \n",
       "3  17.83  265.7    2.95  1.626667   \n",
       "4  18.98  275.4    3.62  1.626667   \n",
       "\n",
       "                                          image_path     label  \n",
       "0  /home/junsheng/data/tianhang_rice/1655496854_1...  1.626667  \n",
       "1  /home/junsheng/data/tianhang_rice/1655496854_1...  1.626667  \n",
       "2  /home/junsheng/data/tianhang_rice/1655504090_1...  1.626667  \n",
       "3  /home/junsheng/data/tianhang_rice/1655504090_1...  1.626667  \n",
       "4  /home/junsheng/data/tianhang_rice/1655511249_1...  1.626667  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tianhang = pd.read_csv(\"/home/junsheng/ViLT/data/290-tianhang-rice.csv\")\n",
    "df_tianhang['image_path'] = df_tianhang['pic_key'].map(lambda x:os.path.join('/home/junsheng/data/tianhang_rice',x.split('/')[-1]))\n",
    "df_tianhang['label'] = df_tianhang['LAI']\n",
    "df_tianhang = df_tianhang.dropna()\n",
    "df_tianhang = df_tianhang.reset_index()\n",
    "df_tianhang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n",
      "763\n",
      "['1658016892_1658016671_4.jpg', '1658225733_1658225474_4.jpg', '1655288067_1655287873_4.jpg', '1658052866_1658052671_4.jpg', '1654647264_1654647073_4.jpg', '1654553669_1654553474_4.jpg', '1655374456_1655374273_4.jpg', '1654236845_1654236677_4.jpg', '1654640051_1654639874_4.jpg', '1658139297_1658139075_4.jpg', '1658110492_1658110273_4.jpg', '1655093685_1655093475_4.jpg', '1658441703_1658441473_4.jpg', '1651652028_1651651878_4.jpg', '1651716813_1651716678_4.jpg', '1654841694_1654841477_4.jpg', '1654805672_1654805473_4.jpg', '1657844100_1657843874_4.jpg', '1658175261_1658175072_4.jpg', '1655453622_1655453472_4.jpg', '1655352923_1655352676_4.jpg', '1654063992_1654063873_4.jpg', '1653804793_1653804673_4.jpg', '1662185718_1662185474_4.jpg', '1655432051_1655431872_4.jpg', '1654892090_1654891870_4.jpg', '1658117667_1658117473_4.jpg', '1655108049_1655107872_4.jpg', '1654560853_1654560675_4.jpg', '1654496068_1654495874_4.jpg', '1654589713_1654589476_4.jpg', '1657880094_1657879872_4.jpg', '1658391283_1658391073_4.jpg', '1655064891_1655064672_4.jpg', '1654402410_1654402275_4.jpg', '1658434505_1658434272_4.jpg', '1654920876_1654920676_4.jpg', '1655180123_1655179876_4.jpg', '1658002442_1658002270_4.jpg', '1655280872_1655280676_4.jpg', '1662235975_1662235870_4.jpg', '1654676073_1654675874_4.jpg', '1657952162_1657951875_4.jpg', '1654668871_1654668675_4.jpg', '1655331267_1655331073_4.jpg', '1657966494_1657966275_4.jpg', '1658204117_1658203876_4.jpg', '1658347963_1658347870_4.jpg', '1654071218_1654071073_4.jpg', '1657916032_1657915873_4.jpg', '1655410456_1655410272_4.jpg', '1658376935_1658376674_4.jpg', '1655367332_1655367076_4.jpg', '1658283283_1658283073_4.jpg', '1655360119_1655359877_4.jpg', '1662257735_1662257476_4.jpg', '1658398473_1658398273_4.jpg', '1654409629_1654409476_4.jpg', '1657959267_1657959073_4.jpg', '1654416840_1654416674_4.jpg', '1655237753_1655237473_4.jpg', '1657836923_1657836672_4.jpg', '1662171336_1662171077_4.jpg', '1654568081_1654567876_4.jpg', '1654150362_1654150272_4.jpg', '1655000043_1654999876_4.jpg', '1655014466_1655014276_4.jpg', '1657793630_1657793472_4.jpg', '1662192998_1662192677_4.jpg', '1654301585_1654301472_4.jpg', '1658103262_1658103071_4.jpg', '1653891196_1653891072_4.jpg', '1653984798_1653984674_4.jpg', '1658189717_1658189475_4.jpg', '1658009676_1658009470_4.jpg', '1658384087_1658383873_4.jpg', '1654337599_1654337475_4.jpg', '1662156938_1662156676_4.jpg', '1654042417_1654042274_4.jpg', '1658124904_1658124673_4.jpg', '1654812854_1654812674_4.jpg', '1655324074_1655323872_4.jpg', '1655345689_1655345476_4.jpg', '1654942456_1654942275_4.jpg', '1657872923_1657872672_4.jpg', '1658362490_1658362272_4.jpg', '1651709638_1651709479_4.jpg', '1651573597_1651573421_4.jpg', '1654928060_1654927876_4.jpg', '1658290521_1658290271_4.jpg', '1654323209_1654323074_4.jpg', '1653811995_1653811873_4.jpg', '1654740852_1654740675_4.jpg', '1654316002_1654315873_4.jpg', '1658297652_1658297474_4.jpg', '1653963189_1653963073_4.jpg', '1657779278_1657779072_4.jpg', '1655338447_1655338276_4.jpg', '1655158478_1655158272_4.jpg', '1655244857_1655244673_4.jpg', '1655417662_1655417472_4.jpg', '1654978452_1654978272_4.jpg', '1654632851_1654632673_4.jpg', '1651630435_1651630278_4.jpg', '1654596904_1654596674_4.jpg', '1662264895_1662264673_4.jpg', '1655007297_1655007077_4.jpg', '1657865697_1657865473_4.jpg', '1662200035_1662199871_4.jpg', '1662164109_1662163876_4.jpg', '1657764908_1657764674_4.jpg', '1654906476_1654906276_4.jpg', '1654748068_1654747873_4.jpg', '1654330398_1654330274_4.jpg', '1662250580_1662250277_4.jpg', '1657944983_1657944676_4.jpg', '1658456084_1658455876_4.jpg', '1654546450_1654546273_4.jpg', '1657937686_1657937473_4.jpg', '1654848876_1654848676_4.jpg', '1654719269_1654719074_4.jpg', '1655460872_1655460672_4.jpg', '1653977592_1653977474_4.jpg', '1654827275_1654827077_4.jpg', '1654899281_1654899073_4.jpg', '1654856110_1654855874_4.jpg', '1654834532_1654834277_4.jpg', '1658312049_1658311871_4.jpg', '1658448910_1658448675_4.jpg', '1654244018_1654243874_4.jpg', '1655165678_1655165475_4.jpg', '1658132109_1658131872_4.jpg', '1655028850_1655028673_4.jpg', '1655172891_1655172677_4.jpg', '1658045663_1658045471_4.jpg', '1658024117_1658023871_4.jpg', '1654582514_1654582274_4.jpg', '1654380811_1654380674_4.jpg', '1654395242_1654395076_4.jpg', '1655072036_1655071873_4.jpg', '1655252096_1655251876_4.jpg', '1662178592_1662178276_4.jpg', '1653876767_1653876672_4.jpg', '1654726446_1654726273_4.jpg', '1654762414_1654762272_4.jpg', '1654820080_1654819876_4.jpg', '1654308814_1654308675_4.jpg', '1655266456_1655266276_4.jpg', '1655115228_1655115072_4.jpg', '1655021644_1655021475_4.jpg', '1658355249_1658355072_4.jpg', '1654388033_1654387876_4.jpg', '1658182528_1658182272_4.jpg', '1654135995_1654135873_4.jpg', '1658211281_1658211076_4.jpg', '1658304910_1658304676_4.jpg', '1654913665_1654913476_4.jpg', '1655194475_1655194274_4.jpg', '1655079279_1655079074_4.jpg', '1654373600_1654373473_4.jpg', '1654503252_1654503075_4.jpg', '1654755224_1654755073_4.jpg', '1658038485_1658038272_4.jpg', '1654424007_1654423874_4.jpg', '1654992878_1654992675_4.jpg', '1658218520_1658218276_4.jpg', '1655259332_1655259076_4.jpg', '1654769607_1654769472_4.jpg', '1654157578_1654157472_4.jpg', '1654733643_1654733473_4.jpg', '1655086461_1655086274_4.jpg', '1655151293_1655151073_4.jpg', '1662243234_1662243074_4.jpg', '1654985688_1654985473_4.jpg', '1654128781_1654128673_4.jpg', '1655424844_1655424672_4.jpg', '1651644831_1651644678_4.jpg', '1657930515_1657930275_4.jpg', '1662149570_1662149469_4.jpg', '1653790398_1653790273_4.jpg', '1654488935_1654488677_4.jpg', '1658268849_1658268671_4.jpg', '1655201643_1655201475_4.jpg', '1654654421_1654654273_4.jpg', '1653955972_1653955872_4.jpg', '1654683242_1654683073_4.jpg', '1655187292_1655187077_4.jpg', '1654661632_1654661473_4.jpg', '1657923276_1657923072_4.jpg', '1657829686_1657829472_4.jpg', '1662286478_1662286271_4.jpg', '1662279350_1662279073_4.jpg', '1658031304_1658031072_4.jpg', '1662272091_1662271873_4.jpg', '1655100847_1655100673_4.jpg', '1657786528_1657786272_4.jpg', '1654510458_1654510274_4.jpg', '1654467235_1654467074_4.jpg', '1653783194_1653783073_4.jpg', '1653869579_1653869472_4.jpg', '1658276093_1658275874_4.jpg', '1654049596_1654049475_4.jpg', '1658261662_1658261472_4.jpg', '1651623244_1651623079_4.jpg', '1654481667_1654481477_4.jpg', '1655446445_1655446275_4.jpg', '1658096092_1658095873_4.jpg', '1658369662_1658369470_4.jpg', '1654215184_1654215075_4.jpg', '1654474472_1654474276_4.jpg', '1655439257_1655439074_4.jpg', '1657772058_1657771873_4.jpg', '1654575283_1654575076_4.jpg', '1654460013_1654459871_4.jpg', '1655273685_1655273477_4.jpg', '1658088875_1658088671_4.jpg', '1654935249_1654935073_4.jpg', '1653898396_1653898271_4.jpg', '1654222398_1654222273_4.jpg', '1657851281_1657851073_4.jpg', '1658196961_1658196676_4.jpg', '1657858531_1657858273_4.jpg']\n"
     ]
    }
   ],
   "source": [
    "# 检查图片下载的全不全\n",
    "pic = df_tianhang.image_path.map(lambda x:x.split('/')[-1]).unique()\n",
    "print(len(pic))\n",
    "file_ls = os.listdir(\"/home/junsheng/data/tianhang_rice\")\n",
    "print(len(file_ls))\n",
    "ret = list(set(pic) ^ set(file_ls))\n",
    "print(ret) #差集\n",
    "# assert len(pic)==len(file_ls),\"请检查下载的图片，缺了{}个\".format(len(pic)-len(file_ls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化非object列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'pic_key',\n",
       " 'date_hour',\n",
       " 'date',\n",
       " 'co2',\n",
       " 'stemp',\n",
       " 'stemp2',\n",
       " 'stemp3',\n",
       " 'stemp4',\n",
       " 'stemp5',\n",
       " 'shumi',\n",
       " 'shumi2',\n",
       " 'shumi3',\n",
       " 'shumi4',\n",
       " 'shumi5',\n",
       " 'ts',\n",
       " 'insert_time',\n",
       " 'humi',\n",
       " 'pm10',\n",
       " 'pm25',\n",
       " 'press',\n",
       " 'solar',\n",
       " 'temp',\n",
       " 'wind_d',\n",
       " 'wind_sp',\n",
       " 'LAI',\n",
       " 'image_path',\n",
       " 'label']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_tianhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'co2', 'stemp', 'stemp2', 'stemp3', 'stemp4', 'stemp5', 'shumi', 'shumi2', 'shumi3', 'shumi4', 'shumi5', 'humi', 'pm10', 'pm25', 'press', 'solar', 'temp', 'wind_d', 'wind_sp', 'LAI', 'label']\n",
      "{'index': (282, 1493), 'co2': (0.0, 1175.0), 'stemp': (13.3, 24.0), 'stemp2': (14.1, 22.8), 'stemp3': (14.1, 23.2), 'stemp4': (14.3, 22.3), 'stemp5': (14.7, 21.8), 'shumi': (73.9, 76.9), 'shumi2': (70.4, 74.8), 'shumi3': (67.5, 69.2), 'shumi4': (72.2, 74.2), 'shumi5': (69.8, 71.8), 'humi': (31.0, 100.0), 'pm10': (0.0, 1333.0), 'pm25': (0.0, 1333.0), 'press': (981.1, 1009.0), 'solar': (0.0, 200.0), 'temp': (7.39, 32.0), 'wind_d': (0.0, 359.8), 'wind_sp': (0.0, 9.41), 'LAI': (1.3458333333333334, 2.2466666666666666), 'label': (1.3458333333333334, 2.2466666666666666)}\n"
     ]
    }
   ],
   "source": [
    "number_title = []\n",
    "recorder = {}\n",
    "for title in df_tianhang:\n",
    "    # print(df_xiangguan[title].head())\n",
    "    if title == 'raw_label':\n",
    "        continue\n",
    "    if df_tianhang[title].dtype != \"object\":\n",
    "        \n",
    "        number_title.append(title)\n",
    "        x_min = df_tianhang[title].min()\n",
    "        x_max = df_tianhang[title].max()\n",
    "        # print(x_min,x_max)\n",
    "        recorder[title] = (x_min,x_max)\n",
    "        df_tianhang[title] = df_tianhang[title].map(lambda x:(x-x_min)/(x_max - x_min))\n",
    "print(number_title)\n",
    "print(recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 19\n"
     ]
    }
   ],
   "source": [
    "# xiangguan_sensor = ['temperature', 'humidity', 'illuminance', 'soil_temperature', 'soil_humidity', 'pressure', 'wind_speed', 'photosynthetic', 'sun_exposure_time', 'COz', 'soil_ph']\n",
    "tianhang_sensor = ['co2', 'stemp', 'stemp2', 'stemp3', 'stemp4', 'stemp5', 'shumi', 'shumi2', 'shumi3', 'shumi4', 'shumi5', 'humi', 'pm10', 'pm25', 'press', 'solar', 'temp', 'wind_d', 'wind_sp']\n",
    "\n",
    "df_tianhang['sensor'] = df_tianhang[tianhang_sensor].values.tolist()\n",
    "print(\"input dim:\",len(tianhang_sensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082, 29)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_tianhang\n",
    "if config.debug:\n",
    "    df = df[:100]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0.0    217\n",
       "1.0    217\n",
       "2.0    216\n",
       "3.0    216\n",
       "4.0    216\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=config.seed)  \n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df,df.date)):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "df.groupby(['fold'])['label'].count()# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv(\"test_fold.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransforms = transforms.Compose([\n",
    "    transforms.Resize((config.img_size,config.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.7136, 0.7118, 0.6788],\n",
    "    std=[0.3338, 0.3453, 0.3020],\n",
    "    \n",
    ")\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img =  Image.open(path).convert('RGB')\n",
    "    img = myTransforms(img)\n",
    "    return img\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df         = df\n",
    "        self.label      = label\n",
    "        self.sensors = df['sensor'].tolist()\n",
    "        self.img_paths  = df['image_path'].tolist()   \n",
    "        if self.label:\n",
    "            self.labels = df['label'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path  = self.img_paths[index]\n",
    "        img = load_img(img_path)\n",
    "        sensor = self.sensors[index]\n",
    "        sensor = torch.tensor(sensor).unsqueeze(0) #[1,n]\n",
    "        if self.label:\n",
    "            label = self.labels[index]\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
    "        else:\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataloader(fold:int):\n",
    "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "\n",
    "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    print(\"train_df.shape:\",train_df.shape)\n",
    "    print(\"valid_df.shape:\",valid_df.shape)\n",
    "\n",
    "    train_data  = BuildDataset(df=train_df,label=True)\n",
    "    valid_data = BuildDataset(df=valid_df,label=True)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=config.train_batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=config.valid_batch_size,shuffle=False)\n",
    "    # test_loader = DataLoader(test_data, batch_size=config.test_batch_size,shuffle=False)\n",
    "    return train_loader,valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (865, 30)\n",
      "valid_df.shape: (217, 30)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = BuildDataset(df=df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.train_batch_size,shuffle=True)\n",
    "# valid_loader = DataLoader(train_dataset, batch_size=config.valid_batch_size,shuffle=True)\n",
    "train_loader,valid_loader = fetch_dataloader(fold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_404322/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 384, 384])\n",
      "torch.Size([32, 1, 19])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "img,sensor,label = next(iter(train_loader))\n",
    "print(img.shape)\n",
    "print(sensor.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorViLOnlyTransformerSS-仅vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sensorViLOnlyTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self, sensor_class_n, output_class_n):\n",
    "        super().__init__()\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "            pretrained=True, config=vars(config)\n",
    "        )\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, output_class_n)\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device)\n",
    "\n",
    "            (\n",
    "                image_embeds,  # torch.Size([1, 217, 768])\n",
    "                image_masks,  # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "            torch.full_like(image_masks, image_token_type_idx)\n",
    "        )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size, 1).to(config.device)  # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = image_embeds\n",
    "        co_masks = image_masks\n",
    "\n",
    "        x = co_embeds.to(config.device)  # torch.Size([1, 145, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)  # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x)  # torch.Size([1, 240, 768])\n",
    "        image_feats = x\n",
    "        cls_feats = self.pooler(x)  # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "\n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "\n",
    "        ret = {\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats,  # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "\n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\": cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "\n",
    "        ret.update(self.infer(batch))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorViLTransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorOnlyViLTransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorOnlyViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        # mask_image=False,\n",
    "        # image_token_type_idx=1,\n",
    "        # image_embeds=None,\n",
    "        # image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        # if image_embeds is None and image_masks is None:\n",
    "        #     img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "        #     (\n",
    "        #         image_embeds, # torch.Size([1, 217, 768])\n",
    "        #         image_masks, # torch.Size([1, 217])\n",
    "        #         patch_index,\n",
    "        #         image_labels,\n",
    "        #     ) = self.transformer.visual_embed(\n",
    "        #         img,\n",
    "        #         max_image_len=config.max_image_len,\n",
    "        #         mask_it=mask_image,\n",
    "        #     )\n",
    "        # else:\n",
    "        #     patch_index, image_labels = (\n",
    "        #         None,\n",
    "        #         None,\n",
    "        #     )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        # image_embeds = image_embeds + self.token_type_embeddings(\n",
    "        #         torch.full_like(image_masks, image_token_type_idx)\n",
    "        #     )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        # batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(sensor_embeds.shape[1],1).to(config.device) # 序列数量\n",
    "        # image_masks = image_masks.to(config.device)\n",
    "        # co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        # co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "        co_embeds = sensor_embeds\n",
    "        co_masks = sensor_masks\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 1, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        # sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "        #     x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "        #     x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        # )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "        #    \"sensor_feats\":sensor_feats,\n",
    "            # \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            # \"image_labels\": image_labels,\n",
    "            # \"image_masks\": image_masks,\n",
    "           \n",
    "            # \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorResnet50TransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorResnet50TransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "        # resnet model\n",
    "        resnet_model = pretrainedmodels.__dict__[\"resnet50\"](\n",
    "    num_classes=1000, pretrained='imagenet')\n",
    "        features = list([resnet_model.conv1, resnet_model.bn1, resnet_model.relu, resnet_model.maxpool, resnet_model.layer1, resnet_model.layer2, resnet_model.layer3,resnet_model.layer4])\n",
    "        conv = nn.Conv2d(2048, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        bn = nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "        self.resnet_features = nn.Sequential(*features,conv,bn,relu)\n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        img = batch[\"image\"].to(config.device)\n",
    "        image_embeds = self.resnet_features(img) \n",
    "        image_embeds = image_embeds.flatten(2).transpose(1, 2)\n",
    "        image_masks = torch.ones(image_embeds.shape[0],image_embeds.shape[1],dtype=torch.int64).to(config.device)\n",
    "\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorResnet101TransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorResnet101TransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "        # resnet model\n",
    "        resnet_model = pretrainedmodels.__dict__[\"resnet101\"](\n",
    "    num_classes=1000, pretrained='imagenet')\n",
    "        features = list([resnet_model.conv1, resnet_model.bn1, resnet_model.relu, resnet_model.maxpool, resnet_model.layer1, resnet_model.layer2, resnet_model.layer3,resnet_model.layer4])\n",
    "        conv = nn.Conv2d(2048, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        bn = nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "        self.resnet_features = nn.Sequential(*features,conv,bn,relu)\n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        img = batch[\"image\"].to(config.device)\n",
    "        image_embeds = self.resnet_features(img) \n",
    "        image_embeds = image_embeds.flatten(2).transpose(1, 2)\n",
    "        image_masks = torch.ones(image_embeds.shape[0],image_embeds.shape[1],dtype=torch.int64).to(config.device)\n",
    "\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pretrained weights exist or were found for this model. Using random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 sensorViLOnlyTransformerSS(\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n",
      "1 Embedding(2, 768)\n",
      "2 VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "3 PatchEmbed(\n",
      "  (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      ")\n",
      "4 Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "5 Dropout(p=0.1, inplace=False)\n",
      "6 ModuleList(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "7 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "8 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "9 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "10 Linear(in_features=768, out_features=2304, bias=True)\n",
      "11 Dropout(p=0.0, inplace=False)\n",
      "12 Linear(in_features=768, out_features=768, bias=True)\n",
      "13 Dropout(p=0.1, inplace=False)\n",
      "14 Identity()\n",
      "15 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "16 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "17 Linear(in_features=768, out_features=3072, bias=True)\n",
      "18 GELU(approximate=none)\n",
      "19 Linear(in_features=3072, out_features=768, bias=True)\n",
      "20 Dropout(p=0.1, inplace=False)\n",
      "21 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "22 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "23 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "24 Linear(in_features=768, out_features=2304, bias=True)\n",
      "25 Dropout(p=0.0, inplace=False)\n",
      "26 Linear(in_features=768, out_features=768, bias=True)\n",
      "27 Dropout(p=0.1, inplace=False)\n",
      "28 Identity()\n",
      "29 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "30 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "31 Linear(in_features=768, out_features=3072, bias=True)\n",
      "32 GELU(approximate=none)\n",
      "33 Linear(in_features=3072, out_features=768, bias=True)\n",
      "34 Dropout(p=0.1, inplace=False)\n",
      "35 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "36 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "37 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "38 Linear(in_features=768, out_features=2304, bias=True)\n",
      "39 Dropout(p=0.0, inplace=False)\n",
      "40 Linear(in_features=768, out_features=768, bias=True)\n",
      "41 Dropout(p=0.1, inplace=False)\n",
      "42 Identity()\n",
      "43 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "44 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "45 Linear(in_features=768, out_features=3072, bias=True)\n",
      "46 GELU(approximate=none)\n",
      "47 Linear(in_features=3072, out_features=768, bias=True)\n",
      "48 Dropout(p=0.1, inplace=False)\n",
      "49 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "50 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "51 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "52 Linear(in_features=768, out_features=2304, bias=True)\n",
      "53 Dropout(p=0.0, inplace=False)\n",
      "54 Linear(in_features=768, out_features=768, bias=True)\n",
      "55 Dropout(p=0.1, inplace=False)\n",
      "56 Identity()\n",
      "57 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "58 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "59 Linear(in_features=768, out_features=3072, bias=True)\n",
      "60 GELU(approximate=none)\n",
      "61 Linear(in_features=3072, out_features=768, bias=True)\n",
      "62 Dropout(p=0.1, inplace=False)\n",
      "63 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "64 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "65 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "66 Linear(in_features=768, out_features=2304, bias=True)\n",
      "67 Dropout(p=0.0, inplace=False)\n",
      "68 Linear(in_features=768, out_features=768, bias=True)\n",
      "69 Dropout(p=0.1, inplace=False)\n",
      "70 Identity()\n",
      "71 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "72 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "73 Linear(in_features=768, out_features=3072, bias=True)\n",
      "74 GELU(approximate=none)\n",
      "75 Linear(in_features=3072, out_features=768, bias=True)\n",
      "76 Dropout(p=0.1, inplace=False)\n",
      "77 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "78 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "79 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "80 Linear(in_features=768, out_features=2304, bias=True)\n",
      "81 Dropout(p=0.0, inplace=False)\n",
      "82 Linear(in_features=768, out_features=768, bias=True)\n",
      "83 Dropout(p=0.1, inplace=False)\n",
      "84 Identity()\n",
      "85 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "86 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "87 Linear(in_features=768, out_features=3072, bias=True)\n",
      "88 GELU(approximate=none)\n",
      "89 Linear(in_features=3072, out_features=768, bias=True)\n",
      "90 Dropout(p=0.1, inplace=False)\n",
      "91 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "92 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "93 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "94 Linear(in_features=768, out_features=2304, bias=True)\n",
      "95 Dropout(p=0.0, inplace=False)\n",
      "96 Linear(in_features=768, out_features=768, bias=True)\n",
      "97 Dropout(p=0.1, inplace=False)\n",
      "98 Identity()\n",
      "99 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "100 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "101 Linear(in_features=768, out_features=3072, bias=True)\n",
      "102 GELU(approximate=none)\n",
      "103 Linear(in_features=3072, out_features=768, bias=True)\n",
      "104 Dropout(p=0.1, inplace=False)\n",
      "105 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "106 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "107 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "108 Linear(in_features=768, out_features=2304, bias=True)\n",
      "109 Dropout(p=0.0, inplace=False)\n",
      "110 Linear(in_features=768, out_features=768, bias=True)\n",
      "111 Dropout(p=0.1, inplace=False)\n",
      "112 Identity()\n",
      "113 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "114 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "115 Linear(in_features=768, out_features=3072, bias=True)\n",
      "116 GELU(approximate=none)\n",
      "117 Linear(in_features=3072, out_features=768, bias=True)\n",
      "118 Dropout(p=0.1, inplace=False)\n",
      "119 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "120 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "121 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "122 Linear(in_features=768, out_features=2304, bias=True)\n",
      "123 Dropout(p=0.0, inplace=False)\n",
      "124 Linear(in_features=768, out_features=768, bias=True)\n",
      "125 Dropout(p=0.1, inplace=False)\n",
      "126 Identity()\n",
      "127 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "128 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "129 Linear(in_features=768, out_features=3072, bias=True)\n",
      "130 GELU(approximate=none)\n",
      "131 Linear(in_features=3072, out_features=768, bias=True)\n",
      "132 Dropout(p=0.1, inplace=False)\n",
      "133 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "134 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "135 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "136 Linear(in_features=768, out_features=2304, bias=True)\n",
      "137 Dropout(p=0.0, inplace=False)\n",
      "138 Linear(in_features=768, out_features=768, bias=True)\n",
      "139 Dropout(p=0.1, inplace=False)\n",
      "140 Identity()\n",
      "141 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "142 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "143 Linear(in_features=768, out_features=3072, bias=True)\n",
      "144 GELU(approximate=none)\n",
      "145 Linear(in_features=3072, out_features=768, bias=True)\n",
      "146 Dropout(p=0.1, inplace=False)\n",
      "147 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "148 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "149 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "150 Linear(in_features=768, out_features=2304, bias=True)\n",
      "151 Dropout(p=0.0, inplace=False)\n",
      "152 Linear(in_features=768, out_features=768, bias=True)\n",
      "153 Dropout(p=0.1, inplace=False)\n",
      "154 Identity()\n",
      "155 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "156 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "157 Linear(in_features=768, out_features=3072, bias=True)\n",
      "158 GELU(approximate=none)\n",
      "159 Linear(in_features=3072, out_features=768, bias=True)\n",
      "160 Dropout(p=0.1, inplace=False)\n",
      "161 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "162 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "163 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "164 Linear(in_features=768, out_features=2304, bias=True)\n",
      "165 Dropout(p=0.0, inplace=False)\n",
      "166 Linear(in_features=768, out_features=768, bias=True)\n",
      "167 Dropout(p=0.1, inplace=False)\n",
      "168 Identity()\n",
      "169 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "170 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "171 Linear(in_features=768, out_features=3072, bias=True)\n",
      "172 GELU(approximate=none)\n",
      "173 Linear(in_features=3072, out_features=768, bias=True)\n",
      "174 Dropout(p=0.1, inplace=False)\n",
      "175 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "176 Linear(in_features=768, out_features=768, bias=True)\n",
      "177 Tanh()\n",
      "178 Pooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "179 Linear(in_features=768, out_features=768, bias=True)\n",
      "180 Tanh()\n",
      "181 Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def build_model(model_name: str,pre_train):\n",
    "    if model_name[:6] == \"resnet50\":\n",
    "        model = pretrainedmodels.__dict__[config.model_name](\n",
    "            num_classes=1000, pretrained='imagenet')\n",
    "        dim_feats = model.last_linear.in_features  # =2048\n",
    "        nb_classes = 1\n",
    "        model.last_linear = nn.Linear(dim_feats, nb_classes)\n",
    "        return model\n",
    "    if model_name == \"se_resnet50\":\n",
    "        model = pretrainedmodels.__dict__[config.model_name](\n",
    "            num_classes=1000, pretrained='imagenet')\n",
    "        model.last_linear = nn.Linear(204800, 1,bias=True)\n",
    "        return model\n",
    "    if model_name == \"efficientnet-b4\": # efficient net\n",
    "        # refer:https://github.com/lukemelas/EfficientNet-PyTorch#example-classification\n",
    "        nb_classes = 1\n",
    "        if pre_train:\n",
    "            model = EfficientNet.from_pretrained(config.model_name)# 'efficientnet-b4'\n",
    "        else:\n",
    "            model = EfficientNet.from_name(config.model_name)# 'efficientnet-b4'\n",
    "        model._fc = nn.Linear(1792, nb_classes)\n",
    "        return model\n",
    "\n",
    "    if model_name == \"sensorOnlyViLTransformerSS\": #仅传感器\n",
    "        model = sensorOnlyViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "    if model_name == \"sensorViLOnlyTransformerSS\": # 仅vit图像\n",
    "        model = sensorViLOnlyTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "        \n",
    "    if model_name == \"sensorResnet50TransformerSS\":\n",
    "        model = sensorResnet50TransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "    if model_name == \"sensorResnet101TransformerSS\":\n",
    "        model = sensorResnet101TransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "\n",
    "    if model_name == \"sensorViLTransformerSS\":\n",
    "        model = sensorViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "\n",
    "model = build_model(config.model_name,True)\n",
    "model.to(config.device)\n",
    "print(config.device)\n",
    "for i,m in enumerate(model.modules()):\n",
    "    print(i,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sensor = torch.rand(config.senser_input_num)\n",
    "# # sensor = torch.ones(config.senser_input_num)\n",
    "# print(sensor)\n",
    "# sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "# batch = {}\n",
    "# batch['sensor'] = sensor\n",
    "# batch['image'] = \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\"\n",
    "# model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.mse_loss #均方误差损失函数\n",
    "# criterion = F.mae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
    "    for step, (img, sensor,label) in pbar:         \n",
    "        # img = img.to(device, dtype=torch.float)\n",
    "        # sensor  = sensor.to(device, dtype=torch.float)\n",
    "        # label  = label.to(device, dtype=torch.float)\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        #一坨优化\n",
    "        optimizer.zero_grad()#每一次反向传播之前都要归零梯度\n",
    "        loss.backward()      #反向传播\n",
    "        optimizer.step()     #固定写法\n",
    "        scheduler.step()\n",
    "     \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_mem=f'{mem:0.2f} GB')\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    val_scores = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
    "    for step, (img, sensor,label) in pbar:               \n",
    "        \n",
    "        \n",
    "        batch_size = img.size(0)\n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred  = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        \n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_memory=f'{mem:0.2f} GB')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "     # init wandb\n",
    "    run = wandb.init(project=\"vilt\",\n",
    "                    config={k: v for k, v in dict(vars(config)).items() if '__' not in k},\n",
    "                    # config={k: v for k, v in dict(config).items() if '__' not in k},\n",
    "                    anonymous=anonymous,\n",
    "                    # name=f\"vilt|fold-{config.valid_fold}\",\n",
    "                    name=config.wandb_name,\n",
    "                    # group=config.wandb_group,\n",
    "                    )\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_valid_loss = 9999\n",
    "    history = defaultdict(list)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        val_loss = valid_one_epoch(model,valid_loader,device=device,optimizer=optimizer)\n",
    "        history['Train Loss'].append(train_loss)\n",
    "        history['Valid Loss'].append(val_loss)\n",
    "\n",
    "        wandb.log({\"Train Loss\": train_loss,\n",
    "                    \"Valid Loss\": val_loss,\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "                })\n",
    "        if best_valid_loss > val_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            # model_file_path = os.path.join(wandb.run.dir,\"epoch-{}-{}.bin\".format(epoch,wandb.run.id))\n",
    "            model_file_path = os.path.join(wandb.run.dir,\"epoch-best.bin\")\n",
    "            run.summary[\"Best Epoch\"] = epoch\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(\"model save to\", model_file_path)\n",
    "            \n",
    "    os.system(\"cp /home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb {}\".format(wandb.run.dir))\n",
    "    run.finish()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=config.T_max, \n",
    "                                                   eta_min=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:39g5um8l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /home/junsheng/ViLT/wandb/offline-run-20221102_171610-39g5um8l<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20221102_171610-39g5um8l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:39g5um8l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: NVIDIA GeForce RTX 3090\n",
      "\n",
      "Epoch 1/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :   0%|          | 0/28 [00:00<?, ?it/s]/tmp/ipykernel_404322/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
      "Train : 100%|██████████| 28/28 [00:59<00:00,  2.11s/it, gpu_mem=6.15 GB, lr=0.00100, train_loss=0.2356]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.89it/s, gpu_memory=2.43 GB, lr=0.00100, valid_loss=0.2304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 2/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.20s/it, gpu_mem=6.10 GB, lr=0.00100, train_loss=0.1801]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.48it/s, gpu_memory=2.44 GB, lr=0.00100, valid_loss=0.0705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 3/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:03<00:00,  2.28s/it, gpu_mem=6.10 GB, lr=0.00099, train_loss=0.0761]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.70it/s, gpu_memory=2.44 GB, lr=0.00099, valid_loss=0.0695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 4/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.24s/it, gpu_mem=6.10 GB, lr=0.00099, train_loss=0.0771]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.60it/s, gpu_memory=2.44 GB, lr=0.00099, valid_loss=0.0744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.22s/it, gpu_mem=6.10 GB, lr=0.00098, train_loss=0.0745]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.60it/s, gpu_memory=2.44 GB, lr=0.00098, valid_loss=0.0909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.22s/it, gpu_mem=6.10 GB, lr=0.00097, train_loss=0.0800]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.64it/s, gpu_memory=2.44 GB, lr=0.00097, valid_loss=0.0699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.22s/it, gpu_mem=6.10 GB, lr=0.00096, train_loss=0.0779]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.57it/s, gpu_memory=2.44 GB, lr=0.00096, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 8/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:00<00:00,  2.17s/it, gpu_mem=6.10 GB, lr=0.00095, train_loss=0.0768]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.53it/s, gpu_memory=2.44 GB, lr=0.00095, valid_loss=0.0732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.20s/it, gpu_mem=6.10 GB, lr=0.00094, train_loss=0.0711]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.71it/s, gpu_memory=2.44 GB, lr=0.00094, valid_loss=0.0739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:00<00:00,  2.17s/it, gpu_mem=6.10 GB, lr=0.00092, train_loss=0.0718]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.60it/s, gpu_memory=2.44 GB, lr=0.00092, valid_loss=0.0739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.21s/it, gpu_mem=6.10 GB, lr=0.00091, train_loss=0.0777]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.61it/s, gpu_memory=2.44 GB, lr=0.00091, valid_loss=0.0707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.25s/it, gpu_mem=6.10 GB, lr=0.00089, train_loss=0.0723]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.56it/s, gpu_memory=2.44 GB, lr=0.00089, valid_loss=0.0709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.21s/it, gpu_mem=6.10 GB, lr=0.00087, train_loss=0.0774]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.56it/s, gpu_memory=2.44 GB, lr=0.00087, valid_loss=0.0853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.24s/it, gpu_mem=6.10 GB, lr=0.00085, train_loss=0.0723]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.67it/s, gpu_memory=2.44 GB, lr=0.00085, valid_loss=0.0702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.24s/it, gpu_mem=6.10 GB, lr=0.00083, train_loss=0.0703]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.62it/s, gpu_memory=2.44 GB, lr=0.00083, valid_loss=0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.23s/it, gpu_mem=6.10 GB, lr=0.00081, train_loss=0.0754]\n",
      "Valid : 100%|██████████| 55/55 [00:16<00:00,  3.43it/s, gpu_memory=2.44 GB, lr=0.00081, valid_loss=0.0710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.21s/it, gpu_mem=6.10 GB, lr=0.00079, train_loss=0.0698]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.56it/s, gpu_memory=2.44 GB, lr=0.00079, valid_loss=0.0712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.21s/it, gpu_mem=6.10 GB, lr=0.00077, train_loss=0.0708]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.52it/s, gpu_memory=2.44 GB, lr=0.00077, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 19/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.19s/it, gpu_mem=6.10 GB, lr=0.00074, train_loss=0.0709]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.65it/s, gpu_memory=2.44 GB, lr=0.00074, valid_loss=0.0696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:00<00:00,  2.17s/it, gpu_mem=6.10 GB, lr=0.00072, train_loss=0.0711]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.64it/s, gpu_memory=2.44 GB, lr=0.00072, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 21/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.23s/it, gpu_mem=6.10 GB, lr=0.00069, train_loss=0.0707]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.49it/s, gpu_memory=2.44 GB, lr=0.00069, valid_loss=0.0791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.21s/it, gpu_mem=6.10 GB, lr=0.00067, train_loss=0.0737]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.47it/s, gpu_memory=2.44 GB, lr=0.00067, valid_loss=0.0707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.25s/it, gpu_mem=6.10 GB, lr=0.00064, train_loss=0.0710]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.67it/s, gpu_memory=2.44 GB, lr=0.00064, valid_loss=0.0706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.21s/it, gpu_mem=6.10 GB, lr=0.00061, train_loss=0.0739]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.57it/s, gpu_memory=2.44 GB, lr=0.00061, valid_loss=0.0719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.20s/it, gpu_mem=6.10 GB, lr=0.00059, train_loss=0.0724]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.55it/s, gpu_memory=2.44 GB, lr=0.00059, valid_loss=0.0709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.23s/it, gpu_mem=6.10 GB, lr=0.00056, train_loss=0.0706]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.63it/s, gpu_memory=2.44 GB, lr=0.00056, valid_loss=0.0703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:01<00:00,  2.21s/it, gpu_mem=6.10 GB, lr=0.00053, train_loss=0.0710]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.49it/s, gpu_memory=2.44 GB, lr=0.00053, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:03<00:00,  2.26s/it, gpu_mem=6.10 GB, lr=0.00050, train_loss=0.0703]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.60it/s, gpu_memory=2.44 GB, lr=0.00050, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.23s/it, gpu_mem=6.10 GB, lr=0.00047, train_loss=0.0698]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.54it/s, gpu_memory=2.44 GB, lr=0.00047, valid_loss=0.0700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.24s/it, gpu_mem=6.10 GB, lr=0.00045, train_loss=0.0716]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.58it/s, gpu_memory=2.44 GB, lr=0.00045, valid_loss=0.0702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.24s/it, gpu_mem=6.10 GB, lr=0.00042, train_loss=0.0702]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.64it/s, gpu_memory=2.44 GB, lr=0.00042, valid_loss=0.0739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.22s/it, gpu_mem=6.10 GB, lr=0.00039, train_loss=0.0731]\n",
      "Valid : 100%|██████████| 55/55 [00:15<00:00,  3.51it/s, gpu_memory=2.44 GB, lr=0.00039, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:02<00:00,  2.22s/it, gpu_mem=6.10 GB, lr=0.00036, train_loss=0.0701]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.88it/s, gpu_memory=2.44 GB, lr=0.00036, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:59<00:00,  2.13s/it, gpu_mem=6.10 GB, lr=0.00034, train_loss=0.0701]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.92it/s, gpu_memory=2.44 GB, lr=0.00034, valid_loss=0.0696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.09s/it, gpu_mem=6.10 GB, lr=0.00031, train_loss=0.0701]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.79it/s, gpu_memory=2.44 GB, lr=0.00031, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 36/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.10s/it, gpu_mem=6.10 GB, lr=0.00029, train_loss=0.0699]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.87it/s, gpu_memory=2.44 GB, lr=0.00029, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.09s/it, gpu_mem=6.10 GB, lr=0.00026, train_loss=0.0698]\n",
      "Valid : 100%|██████████| 55/55 [00:13<00:00,  3.94it/s, gpu_memory=2.44 GB, lr=0.00026, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.10s/it, gpu_mem=6.10 GB, lr=0.00024, train_loss=0.0695]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.80it/s, gpu_memory=2.44 GB, lr=0.00024, valid_loss=0.0696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.10s/it, gpu_mem=6.10 GB, lr=0.00022, train_loss=0.0697]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.91it/s, gpu_memory=2.44 GB, lr=0.00022, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.09s/it, gpu_mem=6.10 GB, lr=0.00019, train_loss=0.0699]\n",
      "Valid : 100%|██████████| 55/55 [00:13<00:00,  3.94it/s, gpu_memory=2.44 GB, lr=0.00019, valid_loss=0.0695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.10s/it, gpu_mem=6.10 GB, lr=0.00017, train_loss=0.0700]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.84it/s, gpu_memory=2.44 GB, lr=0.00017, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.10s/it, gpu_mem=6.10 GB, lr=0.00015, train_loss=0.0696]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.78it/s, gpu_memory=2.44 GB, lr=0.00015, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:59<00:00,  2.11s/it, gpu_mem=6.10 GB, lr=0.00013, train_loss=0.0698]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.77it/s, gpu_memory=2.44 GB, lr=0.00013, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [01:00<00:00,  2.16s/it, gpu_mem=6.10 GB, lr=0.00012, train_loss=0.0696]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.74it/s, gpu_memory=2.44 GB, lr=0.00012, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:57<00:00,  2.07s/it, gpu_mem=6.10 GB, lr=0.00010, train_loss=0.0695]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.78it/s, gpu_memory=2.44 GB, lr=0.00010, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:59<00:00,  2.11s/it, gpu_mem=6.10 GB, lr=0.00008, train_loss=0.0695]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.87it/s, gpu_memory=2.44 GB, lr=0.00008, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 47/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.07s/it, gpu_mem=6.10 GB, lr=0.00007, train_loss=0.0696]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.83it/s, gpu_memory=2.44 GB, lr=0.00007, valid_loss=0.0695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.09s/it, gpu_mem=6.10 GB, lr=0.00006, train_loss=0.0698]\n",
      "Valid : 100%|██████████| 55/55 [00:13<00:00,  3.94it/s, gpu_memory=2.44 GB, lr=0.00006, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n",
      "Epoch 49/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:59<00:00,  2.11s/it, gpu_mem=6.10 GB, lr=0.00005, train_loss=0.0699]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.91it/s, gpu_memory=2.44 GB, lr=0.00005, valid_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 28/28 [00:58<00:00,  2.08s/it, gpu_mem=6.10 GB, lr=0.00004, train_loss=0.0695]\n",
      "Valid : 100%|██████████| 55/55 [00:14<00:00,  3.81it/s, gpu_memory=2.44 GB, lr=0.00004, valid_loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u/files/epoch-best.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid Loss</td><td>█▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Epoch</td><td>50</td></tr><tr><td>Train Loss</td><td>0.06953</td></tr><tr><td>Valid Loss</td><td>0.06931</td></tr><tr><td>lr</td><td>4e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /home/junsheng/ViLT/wandb/offline-run-20221102_171830-1eo7zl2u<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20221102_171830-1eo7zl2u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model, history = run_training(model, optimizer, scheduler,device=config.device,num_epochs=config.max_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 384, 384]) torch.Size([4, 1, 19]) tensor([0.3117, 0.3117, 0.3117, 0.3117])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_404322/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    }
   ],
   "source": [
    "for (img,sensor,label) in valid_loader:\n",
    "    print(img.shape,sensor.shape,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'embedding_test_dict.pt')\n",
    "# print(model)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"/home/junsheng/ViLT/wandb/offline-run-20220811_120519-nzfb1xoz/files/epoch-best.bin\"))\n",
    "model.eval()\n",
    "device = config.device\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = dict()\n",
    "    batch[\"image\"] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6990, 0.0601, 0.3458, 0.8420, 0.1736, 0.1298, 0.0605, 0.8901, 0.3970,\n",
      "        0.8872, 0.7845, 0.3332, 0.6795, 0.8333, 0.7319, 0.5027, 0.4591, 0.5530,\n",
      "        0.4810])\n",
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'image_feats': tensor([[[-0.1994,  0.3416,  0.1723,  ..., -0.2676, -0.4869, -0.1080],\n",
      "         [-0.2128,  0.2874,  0.1768,  ..., -0.2872, -0.5295, -0.1094],\n",
      "         [-0.2155,  0.3161,  0.1778,  ..., -0.3494, -0.5558, -0.0876],\n",
      "         ...,\n",
      "         [-0.2040,  0.3333,  0.1753,  ..., -0.2490, -0.4936, -0.1066],\n",
      "         [-0.1332,  0.4074,  0.1747,  ..., -0.0809, -0.3763, -0.1795],\n",
      "         [-0.1595,  0.3816,  0.1747,  ..., -0.1714, -0.4256, -0.1477]]],\n",
      "       device='cuda:0'), 'cls_feats': tensor([[ 9.8374e-01, -9.6417e-01,  9.3732e-01,  9.9422e-01, -9.3084e-01,\n",
      "          9.8513e-01, -1.1341e-03,  9.7230e-01, -9.8064e-01, -9.8677e-01,\n",
      "         -1.1921e-02, -7.6884e-03,  5.1244e-04,  9.7231e-01,  9.5166e-01,\n",
      "         -9.5652e-01,  9.7604e-01,  9.6867e-01, -9.7843e-01, -9.8917e-01,\n",
      "         -1.9478e-02, -4.9938e-03,  9.8420e-01,  9.2803e-01, -9.9262e-01,\n",
      "         -9.4341e-01, -9.8072e-01, -9.4294e-01, -8.9410e-01,  9.9572e-01,\n",
      "          9.7988e-01, -5.3949e-03,  9.4248e-01,  9.3644e-01, -2.7875e-03,\n",
      "          1.5556e-02, -4.9912e-03, -9.9047e-01, -9.3825e-01,  9.4286e-01,\n",
      "         -9.1370e-01,  9.5654e-01,  9.7743e-01,  9.9370e-01,  9.8342e-01,\n",
      "         -9.7159e-01, -9.7587e-01, -9.5477e-01,  9.6645e-01, -9.9117e-01,\n",
      "          9.7955e-01, -7.8548e-04,  3.4126e-03, -9.0454e-01,  9.7962e-01,\n",
      "         -9.7205e-01,  9.9829e-03,  9.1940e-01, -9.9574e-01, -9.9089e-01,\n",
      "          1.4194e-02,  1.2933e-03, -9.7910e-01,  9.9554e-01,  6.2091e-01,\n",
      "          9.8682e-01,  9.5665e-01,  1.4928e-02, -9.0863e-01,  9.8395e-01,\n",
      "          6.2954e-03, -9.8032e-01,  9.7396e-01, -9.9314e-01, -9.8537e-01,\n",
      "         -1.3539e-02, -9.4569e-04, -4.4191e-03, -9.5651e-05,  9.8273e-01,\n",
      "          9.7544e-01, -2.9666e-04, -9.8666e-01,  9.6127e-01,  5.0876e-03,\n",
      "         -8.8870e-01, -9.7631e-01, -9.8020e-01,  2.7872e-04, -9.4886e-01,\n",
      "         -9.7861e-01,  9.8773e-01,  1.5989e-03, -5.7311e-03,  9.8500e-01,\n",
      "         -9.3356e-01, -9.4389e-01, -9.9079e-01,  4.2385e-03,  9.7681e-01,\n",
      "         -9.7250e-01, -9.7940e-01, -9.9016e-01, -9.7977e-01, -9.5155e-01,\n",
      "         -9.2020e-01,  9.8069e-01,  2.0763e-02,  5.4946e-02,  9.8185e-01,\n",
      "         -9.5550e-01, -9.6481e-01, -9.3550e-01, -9.8486e-01,  3.1768e-04,\n",
      "         -9.7051e-01,  9.8720e-01,  8.8922e-03, -9.6967e-01,  9.5741e-01,\n",
      "         -8.5773e-03, -8.9007e-01,  9.6161e-01, -1.6142e-03, -1.4246e-02,\n",
      "          9.0938e-01, -9.6385e-01,  9.9569e-01,  9.8665e-01,  8.7002e-01,\n",
      "          9.9750e-01, -3.1952e-05,  8.8629e-05,  4.8664e-04,  9.7372e-01,\n",
      "         -9.8207e-01, -3.0989e-03,  9.8118e-01, -2.3451e-04, -8.1624e-03,\n",
      "         -5.7886e-03,  2.3552e-01, -9.6060e-01,  9.7776e-01, -3.5993e-03,\n",
      "         -9.7740e-01, -7.9110e-01, -9.8417e-01,  9.7389e-01, -9.7968e-01,\n",
      "          5.1604e-03,  3.6265e-03, -1.6599e-02, -8.6779e-01,  1.4979e-03,\n",
      "          9.5952e-01,  8.0572e-01,  9.9362e-01, -1.2055e-03,  9.8680e-01,\n",
      "          9.4302e-01,  5.1127e-04,  1.5641e-03,  9.8945e-01,  9.8771e-01,\n",
      "         -9.8239e-01, -9.7451e-01,  9.7575e-01, -2.3883e-03,  9.7577e-01,\n",
      "         -2.2135e-03,  6.8966e-03, -9.3218e-01,  9.7101e-01,  9.9416e-01,\n",
      "         -1.6666e-03,  9.6241e-01, -1.8462e-03,  9.3761e-01, -8.8565e-01,\n",
      "          9.8716e-01,  4.2563e-03,  8.3149e-01, -8.5630e-01,  1.3655e-03,\n",
      "         -9.8336e-01,  2.3065e-02, -9.0172e-01,  9.6789e-01, -9.7318e-01,\n",
      "         -9.6586e-01,  9.9193e-01,  9.7336e-01, -9.7945e-01, -9.8279e-01,\n",
      "          1.3908e-02,  8.7270e-01,  9.8092e-01,  9.9068e-01,  9.8721e-01,\n",
      "          9.2891e-01, -9.8675e-01,  8.2567e-01, -9.7802e-01, -9.0913e-01,\n",
      "          9.8570e-01,  9.9763e-01,  9.5145e-01,  1.7414e-03,  9.8780e-01,\n",
      "          9.3813e-01,  4.3861e-03,  9.2689e-01, -9.6520e-03,  9.5878e-01,\n",
      "          9.7636e-01,  9.6614e-01,  9.7017e-01, -9.7488e-01,  9.7673e-01,\n",
      "         -9.2419e-01,  9.8927e-01,  2.7501e-03,  9.6725e-01,  9.8563e-01,\n",
      "          9.6553e-01,  1.1069e-02, -9.6299e-01,  9.9853e-01, -1.9487e-02,\n",
      "         -9.4447e-01, -9.8846e-01, -9.4638e-01,  9.9570e-01, -8.5905e-02,\n",
      "         -9.5592e-01,  8.6569e-01,  2.0476e-02,  9.5197e-01,  2.2842e-03,\n",
      "         -9.6145e-01, -2.1182e-03, -9.6719e-01, -9.7280e-01,  9.8044e-01,\n",
      "         -3.2270e-03, -9.8772e-01,  8.4087e-03,  9.9024e-01, -9.7874e-01,\n",
      "          9.8082e-01,  9.8498e-01,  1.3687e-03, -9.3811e-01,  8.1776e-04,\n",
      "         -1.3056e-03,  9.7595e-01,  9.1094e-01,  9.8709e-01,  8.1918e-03,\n",
      "         -9.5156e-01,  9.6122e-01,  1.1043e-03, -9.8198e-01, -9.4571e-01,\n",
      "         -9.5049e-01, -5.4786e-03, -5.8228e-03,  6.1010e-03, -9.3251e-01,\n",
      "         -5.9531e-03,  9.8611e-01, -9.8864e-01,  9.9403e-01,  9.2300e-01,\n",
      "         -8.8007e-01,  9.3339e-01, -1.6263e-02,  8.3243e-01,  9.6382e-01,\n",
      "         -9.7729e-01,  9.8013e-01, -1.3954e-02,  9.8283e-01, -5.8024e-01,\n",
      "          9.4912e-01,  9.0569e-03, -2.3661e-03, -9.7618e-01, -9.7523e-01,\n",
      "         -9.8104e-01, -9.7287e-01,  6.0368e-05,  9.7435e-01,  3.1216e-03,\n",
      "          3.7439e-03, -9.8042e-01,  9.9086e-01,  9.8699e-01, -5.4332e-01,\n",
      "          9.7320e-01,  8.8056e-01, -9.7167e-01, -9.2714e-01,  9.9502e-01,\n",
      "         -9.6583e-01,  9.7046e-01,  1.7548e-02, -2.1264e-03, -9.8880e-01,\n",
      "          9.5767e-01,  9.7936e-01,  9.0961e-01, -9.9299e-01,  9.3097e-01,\n",
      "         -9.4764e-01,  9.7487e-01,  9.7576e-01, -9.9396e-01, -9.9395e-01,\n",
      "          9.7727e-01, -9.9596e-01, -9.7675e-01,  9.6221e-01,  9.6586e-01,\n",
      "          9.8038e-01, -9.8643e-01,  9.5195e-01, -9.8398e-01,  9.8145e-01,\n",
      "          9.4100e-01,  9.6092e-01,  9.5434e-01, -8.9482e-01, -9.9578e-01,\n",
      "         -9.8522e-01, -9.7838e-01, -9.8659e-01, -9.8455e-01,  9.6692e-01,\n",
      "          6.2103e-03, -8.4835e-01,  5.3947e-01, -1.2378e-02, -9.7606e-01,\n",
      "         -9.6944e-01, -9.6939e-01, -9.4794e-01, -9.6875e-01, -9.7862e-01,\n",
      "          9.2038e-01, -4.0458e-03,  9.7454e-01, -9.7628e-01, -2.1028e-03,\n",
      "          1.6271e-03, -4.8379e-03,  1.0509e-03, -9.7989e-01,  7.5770e-03,\n",
      "         -9.7128e-01, -9.6364e-04, -9.9695e-01,  9.9648e-01,  9.6851e-01,\n",
      "         -9.9766e-01, -9.5505e-01,  9.5478e-01, -9.6075e-01,  9.8430e-01,\n",
      "          1.2328e-02,  9.9340e-01, -9.8872e-01, -4.9256e-03, -9.8025e-01,\n",
      "          3.3851e-02, -2.9070e-03,  9.9874e-01,  5.8587e-03,  9.8486e-01,\n",
      "         -9.6483e-01,  9.9887e-01, -9.8592e-01, -9.9413e-01, -2.7981e-02,\n",
      "         -9.8666e-01, -7.7550e-03, -9.9721e-01, -3.4427e-03, -9.8549e-01,\n",
      "          9.9005e-01,  6.9735e-04,  5.2411e-03, -9.3288e-01, -9.7774e-01,\n",
      "          9.3709e-01,  9.7922e-01, -9.0916e-01,  9.6744e-01,  9.3698e-01,\n",
      "          9.4919e-01, -9.9101e-03,  9.1390e-01, -1.0292e-02, -1.8658e-03,\n",
      "          2.6741e-03,  9.3438e-01, -9.8537e-01,  1.4012e-03,  5.5028e-03,\n",
      "         -9.9058e-01,  9.8988e-01,  9.7247e-01,  1.7989e-02,  1.2882e-03,\n",
      "          9.7300e-01,  9.5079e-01,  9.7488e-01, -9.7997e-01,  2.3293e-02,\n",
      "         -9.6809e-01, -8.4351e-01,  9.8497e-01,  2.7300e-01,  2.1907e-02,\n",
      "          9.7932e-01, -9.8250e-01,  1.0200e-02, -9.9306e-01, -9.6424e-01,\n",
      "          9.7919e-01,  9.8665e-01,  9.9394e-01,  9.8501e-01,  8.9371e-01,\n",
      "         -1.8566e-02, -9.8219e-01,  6.2588e-03, -9.8087e-01, -9.8200e-01,\n",
      "         -9.8496e-01,  8.7793e-04,  9.8532e-01, -9.8093e-01, -9.3614e-01,\n",
      "          9.6449e-01, -9.8223e-01, -9.8234e-01,  9.9687e-01,  3.9118e-03,\n",
      "          9.8716e-01, -9.8471e-01,  9.4812e-01, -9.8181e-01,  1.5137e-02,\n",
      "          9.7893e-01,  8.7401e-01,  3.2189e-06,  9.6073e-01, -9.9618e-01,\n",
      "          9.8395e-01,  5.4129e-03, -9.7190e-01,  9.7575e-01, -9.6694e-01,\n",
      "          1.3286e-03,  9.7777e-01, -1.6418e-03, -1.7182e-01, -9.8514e-01,\n",
      "          2.6543e-02, -9.8519e-01, -9.5641e-01, -4.3697e-04,  9.7790e-01,\n",
      "         -9.7703e-01, -9.9468e-01, -9.8394e-01, -6.9995e-03, -9.0916e-01,\n",
      "         -9.6650e-01, -5.1442e-01, -9.9498e-01,  9.6304e-01,  9.8988e-01,\n",
      "          7.8809e-04, -9.7879e-01,  9.5183e-01,  9.8935e-01,  9.7669e-01,\n",
      "          9.5928e-01, -9.4495e-01, -9.2905e-01, -9.6208e-01,  3.6398e-04,\n",
      "          2.1517e-03,  9.7930e-01, -1.4665e-03,  9.4051e-01,  9.8788e-01,\n",
      "         -9.8264e-01, -9.8065e-01, -9.8361e-01, -9.2477e-01, -9.6151e-01,\n",
      "         -9.6318e-01, -9.8044e-01,  9.8630e-01, -1.8166e-03, -9.5193e-01,\n",
      "          9.9263e-01, -9.4924e-01, -9.8890e-01,  4.7964e-04, -9.9639e-01,\n",
      "          9.9332e-01,  9.8370e-01,  9.6744e-01,  8.1629e-03,  7.9339e-04,\n",
      "         -9.7992e-01, -9.9384e-01, -9.6665e-01, -9.5771e-03, -9.8634e-01,\n",
      "         -9.3259e-01, -9.9714e-01,  9.5522e-01,  8.1912e-03,  9.9724e-01,\n",
      "          9.8370e-01,  9.9792e-01, -9.8062e-01, -9.4734e-01, -9.1070e-01,\n",
      "          9.9211e-01,  9.8366e-01,  1.3567e-03,  8.6471e-01, -1.3105e-03,\n",
      "         -9.8762e-01, -3.9669e-03, -9.5151e-01, -9.4620e-01, -9.3205e-01,\n",
      "         -9.5415e-01,  9.7658e-01, -9.8669e-01,  9.6844e-01, -9.2534e-01,\n",
      "          9.5809e-01, -2.4972e-03, -9.6881e-01, -9.9086e-01, -9.7796e-01,\n",
      "          9.7424e-01,  9.7979e-01, -9.9116e-01,  1.6012e-03, -9.4856e-01,\n",
      "          1.9228e-02,  9.7363e-01,  9.5330e-01,  9.8966e-01,  1.5734e-03,\n",
      "         -8.0617e-01,  9.8469e-01, -9.9709e-01, -9.5490e-01, -2.2732e-03,\n",
      "         -9.0950e-03, -2.6925e-02, -9.5236e-01,  9.3658e-01,  9.8729e-01,\n",
      "         -9.4989e-01,  9.7603e-01,  1.2494e-02,  9.5857e-01, -9.9821e-01,\n",
      "         -9.6815e-01, -1.0843e-02,  9.8715e-01, -9.7975e-01,  9.8242e-01,\n",
      "         -4.1780e-03, -9.7202e-01, -9.6945e-01,  6.3395e-02, -9.9517e-01,\n",
      "         -9.7317e-01, -9.9284e-01, -9.2470e-01, -9.6129e-01,  9.7405e-01,\n",
      "          9.9118e-01,  9.7935e-01, -9.8063e-01,  9.6145e-01, -6.5339e-03,\n",
      "         -8.8937e-03,  9.7089e-01, -9.8933e-01, -8.4108e-01, -9.5251e-01,\n",
      "         -9.8201e-01, -1.8883e-03, -9.7293e-01, -9.1424e-01,  9.6747e-01,\n",
      "         -9.6816e-01,  9.2610e-01, -9.9662e-01, -9.7268e-01,  9.8271e-01,\n",
      "          2.1370e-03, -9.3549e-01, -9.8508e-01, -9.8663e-01, -9.9688e-01,\n",
      "         -9.1461e-01,  8.7526e-01, -2.7271e-02,  9.7367e-01,  9.5909e-01,\n",
      "         -4.9713e-03,  9.8549e-01, -9.8637e-01, -9.8865e-01,  7.3424e-03,\n",
      "          9.8485e-01, -9.8620e-01,  1.9221e-01, -9.9544e-01, -9.8180e-01,\n",
      "          1.3513e-02,  6.8324e-04, -9.9172e-01, -9.2437e-01, -9.2443e-01,\n",
      "          9.3198e-01, -9.7414e-01,  9.6401e-01, -1.5671e-03,  9.7927e-01,\n",
      "         -9.9128e-01, -2.2113e-02, -9.9068e-01, -9.9269e-01,  9.8598e-01,\n",
      "         -3.2681e-03,  9.4139e-01, -9.8487e-01, -1.4102e-03,  9.8202e-01,\n",
      "         -9.7677e-01, -9.8808e-01,  9.5177e-01,  2.8126e-03, -9.9140e-01,\n",
      "          9.8523e-01, -9.7517e-01, -9.8575e-01, -1.6791e-02, -9.9046e-01,\n",
      "         -9.8253e-01, -9.7711e-01, -7.5771e-03, -9.8549e-01,  9.3772e-01,\n",
      "          9.9648e-01, -9.9891e-01,  9.5904e-01, -9.4871e-01,  9.8166e-01,\n",
      "          1.1139e-02, -3.5731e-03, -2.5159e-03, -6.9160e-01,  9.8803e-01,\n",
      "         -9.5978e-01, -2.2485e-01,  9.8813e-01, -9.9240e-01, -9.7996e-01,\n",
      "          8.6689e-01,  9.0567e-01,  9.8170e-01, -8.8113e-01,  1.5401e-02,\n",
      "         -9.6918e-01, -2.8132e-03,  9.8974e-01, -9.8375e-01, -1.0560e-02,\n",
      "          9.5355e-01,  1.5417e-02, -9.9371e-01, -9.8650e-01,  6.2311e-03,\n",
      "          9.8231e-01,  9.5870e-01,  9.2138e-01, -9.7789e-01, -1.1307e-03,\n",
      "          9.6608e-01, -9.7996e-01, -9.6773e-01, -9.7387e-01,  3.8096e-02,\n",
      "          9.9377e-01,  9.9361e-01, -9.4164e-01, -9.5232e-01, -9.8233e-01,\n",
      "          6.9497e-01, -9.7509e-01, -9.9258e-01, -8.7031e-03,  9.7792e-01,\n",
      "          4.1439e-03, -9.6791e-01,  9.8061e-01,  9.3832e-01, -1.0664e-02,\n",
      "         -9.8064e-01, -9.8705e-01,  9.7234e-01, -9.6243e-01,  7.1930e-03,\n",
      "         -9.9188e-01,  3.8251e-03,  9.7883e-01,  2.9615e-04, -1.0266e-02,\n",
      "         -9.3690e-01,  9.8862e-01,  9.7959e-01,  9.8405e-01, -9.5955e-01,\n",
      "         -5.6754e-04,  9.7154e-01, -6.8499e-01,  9.7816e-01, -9.4353e-01,\n",
      "         -9.6578e-01, -9.2753e-01, -7.2935e-04,  1.7636e-02, -9.8525e-01,\n",
      "          9.8893e-01, -9.7347e-01, -2.3382e-02,  2.4503e-02, -4.0979e-01,\n",
      "         -9.6358e-01,  9.8324e-01,  9.9248e-01, -9.8302e-01, -9.5763e-03,\n",
      "         -8.2218e-01,  2.5393e-03, -9.5426e-01,  9.8621e-01,  9.8155e-01,\n",
      "         -7.8210e-04, -1.5225e-03, -8.1497e-01]], device='cuda:0'), 'raw_cls_feats': tensor([[-1.9935e-01,  3.4158e-01,  1.7226e-01,  9.5159e-02,  5.1758e-01,\n",
      "          9.1433e-01,  2.1565e-01,  1.4415e+00, -1.0985e-01,  2.6169e-01,\n",
      "         -5.1817e-01,  2.0629e-01, -4.2916e-01, -2.1707e-01, -1.7481e-01,\n",
      "         -3.1198e-02, -1.3351e-01,  1.0459e+00,  1.0184e+00,  1.1140e+00,\n",
      "         -5.6217e-02, -5.5323e-01,  3.2873e-01, -2.7098e-01, -4.9513e-01,\n",
      "         -3.8964e-01,  2.2574e-02, -1.2504e+00, -1.4573e-01, -1.5576e+00,\n",
      "          1.3498e-01, -3.9924e-01, -2.6346e-01,  2.7947e-01,  1.3345e-01,\n",
      "         -5.5195e-01,  1.1138e+00,  7.3245e-02,  2.4266e-02,  1.0391e+00,\n",
      "         -6.8140e-02,  5.7761e-03, -3.0982e-01,  1.0916e-01,  3.5255e-01,\n",
      "         -2.8080e-01,  3.2512e-01, -4.7159e-02,  6.1275e-01,  2.4365e-01,\n",
      "          1.0292e-01,  4.8175e-01, -3.7534e-01,  5.3039e-01, -6.1327e-02,\n",
      "         -1.2104e-01,  9.8139e-02,  1.4633e-01, -1.0661e-01, -3.4192e-01,\n",
      "         -3.1747e-01,  4.3275e-02, -1.1638e+00, -5.3018e-01, -2.6564e-01,\n",
      "         -1.6973e-01, -3.3774e-02,  5.4628e-01,  1.4907e+00, -2.9217e-01,\n",
      "         -1.2698e+00,  5.1438e-01,  6.8290e-02, -1.0916e+00,  7.8331e-02,\n",
      "         -1.5592e-01, -4.7991e-01, -4.5328e-02, -7.7093e-01, -3.6783e-01,\n",
      "          7.3048e-01, -2.4867e+00, -5.2087e-01,  6.7883e-01,  3.9054e-01,\n",
      "         -1.0689e+00, -9.9952e-01,  5.2696e-01,  3.2018e-01,  3.7264e-01,\n",
      "         -1.4085e+00, -1.2538e+00,  1.0732e-01,  3.2117e-02, -1.5446e-01,\n",
      "          2.1702e-02, -5.5764e-01,  4.7341e-01, -6.6232e-02,  1.3388e-01,\n",
      "         -5.5661e-02,  1.7595e-01,  7.4581e-01, -2.8180e-01, -9.8359e-02,\n",
      "         -5.1636e-01,  9.1152e-01,  1.0981e-01, -1.5838e-01, -5.2553e-01,\n",
      "         -7.8474e-02,  2.9364e-01, -1.9121e-01,  1.5601e+00,  1.3854e-01,\n",
      "          9.6039e-02, -4.9735e-01,  2.0156e-01, -8.8832e-01, -5.8856e-02,\n",
      "          3.5782e-01, -1.4256e-01, -9.2398e-02, -4.1070e-01, -7.3122e-01,\n",
      "          7.8775e-01, -2.7573e-01, -1.2975e+00,  2.1294e-01, -9.6534e-02,\n",
      "          1.6676e-01, -2.1417e-01, -5.7920e-02, -8.7545e-01, -1.4356e+00,\n",
      "          1.6526e-01,  2.8011e-01, -3.2418e-01,  8.1291e-01, -1.1045e+00,\n",
      "          4.1563e-01,  3.0781e-01, -1.5282e+00,  3.9994e-01, -5.8630e-01,\n",
      "         -3.3668e-01,  2.8215e-01,  1.5034e+00, -1.8088e-02, -6.4789e-02,\n",
      "          9.4908e-01,  1.0347e-01, -7.5748e-02, -3.2080e-01,  8.0937e-01,\n",
      "          1.9730e-02, -8.5707e-02, -6.8849e-02, -7.7595e-02, -4.8972e-01,\n",
      "          3.7382e-01, -3.2399e-02, -4.4057e-01, -3.1741e-01,  5.3848e-01,\n",
      "          1.3014e-01,  6.3765e-01,  5.2203e-01, -5.1607e-01,  2.3964e-01,\n",
      "         -6.3777e-01, -2.7201e-03, -4.9147e-01,  3.5414e-01,  2.9217e-02,\n",
      "         -5.9330e-02, -1.4434e-01, -2.6457e-01,  1.4993e-01,  2.3340e-02,\n",
      "         -1.9330e-01, -2.5918e-02,  6.5587e-01,  9.6960e-01, -1.1279e+00,\n",
      "         -2.3837e-01, -1.3423e+00,  1.5115e-01, -2.9606e-02, -5.2523e-02,\n",
      "         -1.0810e+00, -9.8748e-02,  1.2727e-01, -3.0296e-01,  9.8705e-01,\n",
      "          1.0595e+00,  6.1907e-01, -1.1353e-01, -1.8986e+00,  3.2390e-01,\n",
      "         -1.4284e+00,  3.3147e-01, -3.0626e-01,  4.1416e-01, -8.0700e-02,\n",
      "          1.8147e-01,  7.2849e-01,  2.0479e+00, -1.3557e-01, -4.0143e-01,\n",
      "          3.7006e-02, -3.1024e-03, -1.5299e+00,  4.2949e-01,  6.4867e-03,\n",
      "         -2.3233e-01, -7.8584e-02, -6.5124e-01, -2.0739e-01, -5.2950e-02,\n",
      "          4.8253e-01, -3.1166e-01, -2.3292e-01, -6.1248e-01, -2.7852e-01,\n",
      "          4.9407e-02, -5.8833e-02, -3.1119e-01, -6.3612e-01, -3.7937e-02,\n",
      "         -3.0854e-01,  3.5112e-01,  1.8149e-02, -4.4874e-01, -4.1296e-01,\n",
      "         -5.4755e-01,  4.5777e-01,  8.9341e-01,  2.3612e-02, -4.8778e-01,\n",
      "          6.9649e-04,  3.1384e-01, -6.6476e-01, -1.0115e+00,  1.3008e+00,\n",
      "          3.3078e-01, -3.3908e-01,  1.1039e+00,  1.4568e+00, -4.6037e-01,\n",
      "         -2.5130e-01,  1.4597e-01,  1.6632e-01,  4.7726e-01,  1.3083e+00,\n",
      "         -4.2598e-02, -3.0834e-01, -4.4946e-01, -1.7416e-01,  2.6451e-01,\n",
      "         -5.3878e-01, -9.2049e-01, -1.1989e+00,  1.5456e+00,  2.4770e-01,\n",
      "         -1.6181e+00,  3.5833e-02,  6.7474e-01,  1.0233e+00, -1.2313e-01,\n",
      "         -7.3299e-02, -1.4293e+00, -5.9102e-01,  6.7867e-01,  4.3865e-01,\n",
      "         -1.0727e+00, -2.4354e-02, -4.5466e-01, -9.9375e-02, -1.6477e-01,\n",
      "          2.4595e-01,  1.3067e+00, -9.7856e-01, -4.8224e-01,  2.0910e-01,\n",
      "          5.4254e-01,  1.4552e-02, -1.4044e+00,  2.8500e-01,  7.4066e-01,\n",
      "         -7.7652e-02, -7.0241e-02,  1.7921e-01, -1.9757e-01,  7.6776e-01,\n",
      "          1.0214e+00,  4.4992e-02,  9.7734e-01, -2.1682e-01,  1.7146e-01,\n",
      "         -1.7378e-01, -7.2950e-01, -5.2704e-02, -4.0204e-01, -5.4390e-01,\n",
      "          3.7846e-01,  2.1656e-02, -4.0454e-01,  3.0539e-02, -3.5424e-01,\n",
      "         -7.3087e-01,  7.7512e-01,  1.1211e+00,  1.8444e+00, -2.4602e+00,\n",
      "         -3.2232e-01,  5.8548e-01, -3.2865e-02,  2.5863e-01, -1.3003e-01,\n",
      "         -1.7034e+00, -8.1513e-01, -1.2464e-01, -9.7253e-02, -6.5720e-01,\n",
      "          2.7202e-01, -1.9314e-01, -2.1857e-01,  2.0856e-02, -1.8173e-01,\n",
      "         -6.2577e-01,  4.1901e-01, -2.3143e-01, -1.0151e+00,  7.2203e-01,\n",
      "          5.1235e-01, -6.0046e-01,  3.6008e-01, -5.6257e-01, -5.8535e-01,\n",
      "          7.0365e-01, -5.0966e-01,  4.2960e-01, -7.7241e-01, -2.8115e-01,\n",
      "         -6.2432e-02, -2.4043e-03, -2.3440e-01,  4.5982e-02, -6.9355e-01,\n",
      "          9.1520e-02, -1.8032e-01,  5.4696e-01, -1.2164e+00,  8.3957e-01,\n",
      "         -6.3343e-02,  2.0772e-01, -7.1571e-01, -6.0537e-01,  7.0274e-01,\n",
      "          2.6291e-01,  1.8011e-01,  2.3792e-01, -7.2036e-02,  4.8077e-01,\n",
      "         -6.0035e-02,  3.9252e-01, -2.4775e-01, -4.0075e-01,  8.6933e-01,\n",
      "          2.9282e-01,  5.3264e-01,  2.7144e-01,  2.6783e-01, -2.9747e-01,\n",
      "         -4.1004e-01, -3.5140e-02,  6.2850e-02, -4.4763e-01,  1.5989e+00,\n",
      "          7.8834e-01, -3.9880e-02,  7.9385e-02,  5.6063e-01, -1.3863e-02,\n",
      "          3.6115e-02,  3.2400e-01, -2.1392e-01,  1.4829e-01,  2.9179e-01,\n",
      "         -1.4537e-01, -3.6289e-01,  1.3212e-01,  4.1326e-01,  8.9440e-01,\n",
      "         -6.3309e-03,  7.5909e-01, -2.8316e-01, -1.4650e-01, -6.0470e-01,\n",
      "          7.9756e-01, -2.9282e-01,  6.9514e-01, -4.4884e-01,  3.8588e-02,\n",
      "         -6.7813e-01,  2.2908e-01,  3.6595e-01, -3.3943e-01,  7.1619e-02,\n",
      "         -1.0753e-01,  7.1782e-01, -1.1584e-01, -1.8730e-01,  5.4385e-01,\n",
      "         -2.2758e+00,  1.0176e-01, -7.7091e-01, -2.6472e-01,  3.1834e-01,\n",
      "          6.7696e-01,  1.3288e-01,  4.7298e-01,  6.5640e-01,  7.3986e-02,\n",
      "          4.8331e-01, -3.4832e-01,  1.0086e-01, -2.8969e-01,  4.7381e-01,\n",
      "         -1.0796e+00,  1.8081e+00,  4.1986e-01, -2.9634e-01, -1.0378e+00,\n",
      "          2.1523e-01,  1.1090e+00, -1.2659e+00, -1.4163e-01,  1.8913e+00,\n",
      "          1.0760e+00,  4.7900e-02,  6.0097e-01, -9.2529e-02, -4.2615e-01,\n",
      "         -1.3563e-01,  8.6160e-01,  1.8088e+00, -2.0969e-01, -1.2460e+00,\n",
      "         -3.6124e-02, -4.1232e-01,  3.2922e-01,  8.8151e-01, -1.6683e-01,\n",
      "         -9.3363e-01, -2.4104e-02, -2.3751e-01,  4.5838e-01, -1.0999e+00,\n",
      "          1.6217e-01,  2.5608e-01,  4.3048e-01, -7.1146e-02,  6.9366e-01,\n",
      "         -2.7717e-01, -8.9911e-02,  1.6876e+00, -7.9366e-01,  9.4785e-01,\n",
      "          1.7570e-01, -1.5899e-01, -8.5747e-02,  7.2204e-01, -1.6922e-01,\n",
      "         -3.4520e-01,  2.5948e-01,  6.2443e-01, -1.0766e-02,  2.6464e-01,\n",
      "          1.0100e+00,  8.1780e-02, -2.0296e-01, -1.3957e+00, -8.3034e-01,\n",
      "          1.1413e+00, -1.5255e-01,  2.3838e+00,  4.0498e-01, -8.2938e-01,\n",
      "         -7.9762e-01, -4.5375e-01,  5.1874e-02, -1.3770e-01, -1.1716e+00,\n",
      "          5.0681e-02,  1.6288e-01, -1.3919e-01,  1.5871e+00,  1.9765e-02,\n",
      "          2.2869e-01,  3.0153e-01,  1.5141e-01, -7.3229e-02,  1.0474e-03,\n",
      "         -9.4752e-01, -5.1835e-01, -3.1795e-01, -5.3101e-02,  2.3300e-01,\n",
      "          1.2085e+00, -2.9250e-01,  1.7161e+00, -1.8590e+00, -1.2020e-01,\n",
      "          6.8773e-01, -2.2005e-02,  4.1970e-01,  2.6636e-02, -6.0905e-02,\n",
      "          1.5696e-01, -1.8716e-01, -3.5738e-02,  1.2466e-01, -4.1792e-01,\n",
      "         -1.3941e-02, -6.6945e-02,  1.2864e-01,  1.4601e-01, -2.5314e-01,\n",
      "          4.2702e-02,  8.7438e-02,  3.4827e-01,  1.8130e+00, -3.7877e-01,\n",
      "          5.6381e-01,  6.1829e-01, -6.8065e-01,  1.0712e+00,  5.3057e-01,\n",
      "          1.0033e+00,  4.0801e-01, -7.9120e-02, -1.2691e-01,  5.5133e-02,\n",
      "          6.8723e-01,  7.2956e-02,  7.6249e-02, -5.5471e-01,  4.2967e-01,\n",
      "          6.0237e-02, -4.8667e-01,  1.1299e+00, -1.9965e-02, -6.5563e-02,\n",
      "          2.5151e-01,  5.6537e-02, -1.9484e-01,  6.8426e-01, -2.5547e-02,\n",
      "          6.1343e-01, -1.1671e+00,  4.8119e-01, -4.2346e-01, -3.8950e-01,\n",
      "         -2.0702e-01,  2.4723e-02,  1.9329e-01, -6.2018e-01, -9.5168e-01,\n",
      "         -9.4977e-01, -6.5184e-01,  2.3140e+00,  3.6212e-01, -4.8453e-01,\n",
      "          1.4488e-01,  2.0835e-02,  9.7877e-01, -4.0865e-01,  9.1376e-02,\n",
      "         -2.2058e-01,  1.0749e+00, -3.2682e-02, -5.0073e-01, -7.1348e-02,\n",
      "         -2.4097e-02,  6.7113e-01,  2.2570e-01, -6.5855e-01, -3.2748e-01,\n",
      "         -7.8980e-01, -2.8170e-02, -1.1549e-01,  1.1046e+00, -2.6101e-01,\n",
      "          5.3267e-01, -3.3843e-01,  4.9962e-01, -9.3314e-02,  5.8783e-01,\n",
      "          2.5138e-01, -6.0200e-01,  3.2096e-02,  2.7187e-01,  2.1937e-01,\n",
      "          2.9870e-01,  1.6707e+00, -3.9112e-02, -1.0954e-01,  3.8406e-02,\n",
      "         -8.4688e-01,  6.4003e-01, -1.4553e-01, -1.0595e+00, -3.4969e-01,\n",
      "         -4.7088e-01,  7.1810e-02,  1.2063e+00, -3.2396e-01,  7.9368e-03,\n",
      "          6.7993e-01,  9.5499e-02, -7.2419e-01, -1.9005e+00, -2.4741e-01,\n",
      "         -5.4574e-01,  2.0363e-01, -1.1237e+00, -2.8458e-01,  1.0957e-01,\n",
      "          1.7597e-02,  1.6103e-01, -2.6956e-02,  2.1066e-01,  3.7207e-01,\n",
      "         -1.5025e+00,  1.7378e+00,  3.3747e-01, -1.8029e-01, -1.4517e-01,\n",
      "         -3.9079e-02,  3.8858e-01, -7.5632e-01, -3.1371e-01,  2.7023e-01,\n",
      "          1.8573e-01, -1.4650e+00,  4.1687e-01, -1.4718e+00,  1.6632e+00,\n",
      "         -1.7808e-01, -1.8883e-01, -3.4168e-01,  6.9046e-02, -1.4128e+00,\n",
      "         -8.6815e-01,  1.9863e+00, -3.1293e-01, -8.1511e-02,  5.8649e-01,\n",
      "          9.3969e-01, -1.1409e+00,  2.9935e-01,  3.5983e-02,  3.9189e-01,\n",
      "         -1.2771e+00,  4.4804e-02, -4.2682e-01,  7.1518e-02, -7.2800e-01,\n",
      "         -9.4903e-02, -3.4536e-01,  3.3824e-01,  2.7668e-02,  1.4137e+00,\n",
      "          1.8376e-01,  1.7396e-01,  3.5024e-01, -1.0762e-01, -5.2884e-02,\n",
      "         -4.9131e-01,  5.6287e-01,  2.3086e-02,  2.7057e-01, -1.4399e+00,\n",
      "          1.0205e+00,  4.1703e-01,  2.1747e+00,  1.7867e-01,  2.3887e-02,\n",
      "         -8.3420e-01,  6.2174e-01, -1.0665e+00, -1.3410e-03, -2.4969e-01,\n",
      "         -3.3755e-01,  6.3708e-01, -1.4660e+00, -8.5358e-01,  1.3961e-01,\n",
      "         -1.1367e+00,  7.4544e-01,  1.5716e-01, -1.2174e-01, -1.8716e-01,\n",
      "          2.3472e-01, -1.1011e+00, -1.2309e-01,  8.0104e-01, -6.0607e-02,\n",
      "          3.2118e-01, -2.3988e-01, -4.1940e-01,  7.3584e-02, -8.0144e-02,\n",
      "         -1.5947e-02,  3.2827e-01, -1.2009e+00, -2.5061e-01, -2.0084e-01,\n",
      "         -1.4030e-01, -6.1821e-01,  8.5123e-01,  7.6353e-02, -1.8975e-01,\n",
      "          6.6205e-01,  1.2395e-02, -2.7889e-02, -2.8035e-01, -3.5196e-02,\n",
      "         -1.2722e-01, -1.7056e-01,  5.4630e-01, -5.4768e-01,  2.0007e+00,\n",
      "          1.7097e+00, -1.2520e-01,  1.0890e+00,  6.8653e-01,  2.3815e-01,\n",
      "          2.8291e-01, -1.9271e-01, -8.6463e-01,  4.4718e-01, -1.3125e+00,\n",
      "          6.6292e-01, -5.5283e-01,  3.3409e-01, -4.1942e-01, -1.4477e-01,\n",
      "          1.5150e-01, -2.8518e-02, -4.0130e-02, -2.5373e-01,  3.2530e-01,\n",
      "         -1.3476e-01,  6.4329e-01,  9.6375e-01,  1.0607e+00,  8.1498e-01,\n",
      "         -1.1539e-01, -1.2930e-01, -1.0664e-01, -3.3215e-01,  2.9756e-01,\n",
      "         -2.6763e-01, -4.8691e-01, -1.0799e-01]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 3,  0],\n",
      "         [ 9, 10],\n",
      "         [ 5,  2],\n",
      "         [ 6, 12],\n",
      "         [ 0, 15],\n",
      "         [10,  6],\n",
      "         [ 9, 13],\n",
      "         [ 3,  6],\n",
      "         [ 0,  8],\n",
      "         [ 0, 16],\n",
      "         [ 6, 10],\n",
      "         [ 6, 18],\n",
      "         [ 7, 11],\n",
      "         [ 2,  0],\n",
      "         [ 7,  6],\n",
      "         [ 3, 13],\n",
      "         [ 4,  0],\n",
      "         [ 5,  7],\n",
      "         [ 8, 12],\n",
      "         [ 5, 12],\n",
      "         [10, 12],\n",
      "         [ 0, 14],\n",
      "         [ 1, 18],\n",
      "         [ 7,  4],\n",
      "         [ 1, 12],\n",
      "         [ 9,  0],\n",
      "         [ 5,  9],\n",
      "         [ 8,  7],\n",
      "         [ 5, 14],\n",
      "         [ 3, 10],\n",
      "         [ 6,  5],\n",
      "         [ 5,  5],\n",
      "         [ 3, 15],\n",
      "         [ 1,  4],\n",
      "         [ 6, 15],\n",
      "         [ 2,  3],\n",
      "         [ 7,  3],\n",
      "         [ 9, 11],\n",
      "         [ 8,  9],\n",
      "         [10,  0],\n",
      "         [ 8,  5],\n",
      "         [ 2, 18],\n",
      "         [ 3,  4],\n",
      "         [ 3,  9],\n",
      "         [ 7,  7],\n",
      "         [ 0,  6],\n",
      "         [ 1,  7],\n",
      "         [ 5,  4],\n",
      "         [ 9, 15],\n",
      "         [ 2,  9],\n",
      "         [ 5, 16],\n",
      "         [ 3,  3],\n",
      "         [ 3, 18],\n",
      "         [ 1,  2],\n",
      "         [10, 11],\n",
      "         [ 2, 11],\n",
      "         [ 2, 17],\n",
      "         [ 9,  1],\n",
      "         [ 1, 14],\n",
      "         [ 4, 17],\n",
      "         [ 9,  9],\n",
      "         [ 4,  3],\n",
      "         [ 9,  8],\n",
      "         [ 4, 12],\n",
      "         [ 1, 11],\n",
      "         [10,  2],\n",
      "         [ 4,  1],\n",
      "         [ 7,  5],\n",
      "         [ 0, 17],\n",
      "         [ 7, 16],\n",
      "         [ 2, 14],\n",
      "         [ 2,  6],\n",
      "         [ 4,  5],\n",
      "         [10, 18],\n",
      "         [ 5, 18],\n",
      "         [ 8, 14],\n",
      "         [ 2,  7],\n",
      "         [ 8,  4],\n",
      "         [ 2,  8],\n",
      "         [ 2,  1],\n",
      "         [10,  3],\n",
      "         [ 6, 13],\n",
      "         [ 5, 15],\n",
      "         [ 1, 13],\n",
      "         [ 4,  7],\n",
      "         [ 4,  6],\n",
      "         [ 6,  6],\n",
      "         [ 3, 12],\n",
      "         [ 8, 11],\n",
      "         [ 5,  6],\n",
      "         [ 0,  4],\n",
      "         [ 1, 15],\n",
      "         [ 4, 11],\n",
      "         [ 8, 10],\n",
      "         [ 0, 12],\n",
      "         [ 3, 17],\n",
      "         [ 9, 18],\n",
      "         [ 7,  2],\n",
      "         [10, 17],\n",
      "         [10,  8],\n",
      "         [ 5,  1],\n",
      "         [ 6, 11],\n",
      "         [10,  1],\n",
      "         [10,  7],\n",
      "         [ 5,  0],\n",
      "         [ 2, 15],\n",
      "         [ 4, 13],\n",
      "         [ 3, 11],\n",
      "         [ 0, 13],\n",
      "         [ 2, 13],\n",
      "         [ 0,  2],\n",
      "         [ 7, 10],\n",
      "         [ 9, 14],\n",
      "         [ 5, 17],\n",
      "         [ 6,  4],\n",
      "         [ 8,  6],\n",
      "         [ 6,  0],\n",
      "         [ 2, 12],\n",
      "         [ 3,  2],\n",
      "         [ 1, 16],\n",
      "         [ 9,  6],\n",
      "         [ 0, 10],\n",
      "         [ 6,  2],\n",
      "         [ 3,  7],\n",
      "         [ 5, 13],\n",
      "         [ 7, 14],\n",
      "         [ 6,  7],\n",
      "         [ 8,  8],\n",
      "         [10,  5],\n",
      "         [ 7, 12],\n",
      "         [ 9,  2],\n",
      "         [ 6,  8],\n",
      "         [ 8,  1],\n",
      "         [ 1,  0],\n",
      "         [10, 15],\n",
      "         [10,  9],\n",
      "         [ 8,  3],\n",
      "         [ 7, 15],\n",
      "         [ 1,  1],\n",
      "         [ 0,  1],\n",
      "         [ 9,  4],\n",
      "         [ 0, 11],\n",
      "         [ 8,  2],\n",
      "         [ 8, 13],\n",
      "         [ 9, 17],\n",
      "         [ 4, 15],\n",
      "         [ 4,  4],\n",
      "         [10, 16],\n",
      "         [ 5,  3],\n",
      "         [ 2,  5],\n",
      "         [ 9, 16],\n",
      "         [ 7,  9],\n",
      "         [ 7, 13],\n",
      "         [ 6,  1],\n",
      "         [ 2, 10],\n",
      "         [ 8, 17],\n",
      "         [ 1, 17],\n",
      "         [ 4, 16],\n",
      "         [ 3,  1],\n",
      "         [ 5, 10],\n",
      "         [10, 14],\n",
      "         [ 0, 18],\n",
      "         [ 8,  0],\n",
      "         [ 4, 18],\n",
      "         [ 2,  2],\n",
      "         [ 3,  5],\n",
      "         [ 0,  7],\n",
      "         [ 1,  9],\n",
      "         [ 7,  8],\n",
      "         [ 2,  4],\n",
      "         [ 7,  1],\n",
      "         [ 4, 14],\n",
      "         [ 6, 16],\n",
      "         [10, 13],\n",
      "         [ 0,  0],\n",
      "         [ 7, 17],\n",
      "         [ 3, 16],\n",
      "         [ 9,  7],\n",
      "         [ 4, 10],\n",
      "         [10,  4],\n",
      "         [ 9,  5],\n",
      "         [ 0,  9],\n",
      "         [10, 10],\n",
      "         [ 6, 17],\n",
      "         [ 7, 18],\n",
      "         [ 8, 15],\n",
      "         [ 8, 18],\n",
      "         [ 0,  3],\n",
      "         [ 4,  2],\n",
      "         [ 6,  3],\n",
      "         [ 1,  5],\n",
      "         [ 9, 12],\n",
      "         [ 1,  8],\n",
      "         [ 1, 10],\n",
      "         [ 6,  9],\n",
      "         [ 7,  0],\n",
      "         [ 5,  8],\n",
      "         [ 3,  8],\n",
      "         [ 6, 14],\n",
      "         [ 0,  5],\n",
      "         [ 3, 14],\n",
      "         [ 4,  8],\n",
      "         [ 5, 11],\n",
      "         [ 1,  3],\n",
      "         [ 2, 16],\n",
      "         [ 8, 16],\n",
      "         [ 1,  6],\n",
      "         [ 9,  3],\n",
      "         [ 4,  9]]]), (11, 19)), 'cls_output': tensor([[0.4039]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_404322/3499233738.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sensor_feats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb Cell 59\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(sensor)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m sensor \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mtensor(sensor)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m# torch.Size([1, 1, 3])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m out \u001b[39m=\u001b[39m infer(examples[\u001b[39m0\u001b[39;49m],sensor)\n",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb Cell 59\u001b[0m in \u001b[0;36minfer\u001b[0;34m(img_filename, sensor)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     infer \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mprint\u001b[39m(infer)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     sensor_emb, img_emb \u001b[39m=\u001b[39m infer[\u001b[39m\"\u001b[39;49m\u001b[39msensor_feats\u001b[39;49m\u001b[39m\"\u001b[39;49m], infer[\u001b[39m\"\u001b[39m\u001b[39mimage_feats\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     cls_output \u001b[39m=\u001b[39m infer[\u001b[39m'\u001b[39m\u001b[39mcls_output\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [cls_output]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sensor_feats'"
     ]
    }
   ],
   "source": [
    "\n",
    "examples=[\n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\", #0\n",
    "            \n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-07-18-04-22-30-preset-18.jpeg\", # 3\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "n = 1\n",
    "sensor = torch.rand(config.senser_input_num)\n",
    "# sensor = torch.ones(config.senser_input_num)\n",
    "print(sensor)\n",
    "sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "out = infer(examples[0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.4540]], device='cuda:0')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4539965\n"
     ]
    }
   ],
   "source": [
    "print(out[0].cpu().numpy()[0][0])\n",
    "#0.00031266143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test by valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择三组生长期不同的数据去验证训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.query(\"fold==0\").reset_index(drop=True)\n",
    "df_test.to_csv(\"test_by_valid.csv\",index=False)\n",
    "sensor_test_list = df_test.sensor.tolist()\n",
    "image_test_list = df_test.image_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang.ipynb Cell 57\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m sensor \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mtensor(sensor_test_list[idx])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m out \u001b[39m=\u001b[39m infer(image_test_list[idx],sensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'infer' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 64\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.0827e-06, -1.7530e-07,  1.5452e-07, -2.2674e-08, -2.2496e-06,\n",
      "           3.2150e-06,  6.0465e-06, -5.3801e-09, -1.8847e-07, -7.7690e-07,\n",
      "          -1.5788e-07,  3.6213e-06, -3.8558e-08, -1.0063e-05, -1.4680e-08,\n",
      "          -1.8126e-07, -2.7730e-06,  9.1713e-09, -2.7553e-06, -5.3852e-09,\n",
      "           9.5103e-07,  3.8168e-09,  7.2278e-08, -1.0654e-08,  6.3138e-07,\n",
      "           3.1689e-06, -2.6612e-06, -1.0689e-09,  2.8926e-06,  4.4980e-06,\n",
      "          -9.2651e-06,  1.0652e-06, -4.1821e-07,  2.2096e-08, -9.7153e-07,\n",
      "           1.0851e-06,  7.0076e-08, -7.6146e-08, -2.9167e-09,  2.1692e-06,\n",
      "           7.3493e-08,  8.4667e-06,  3.4533e-08, -2.6099e-07, -4.6151e-06,\n",
      "           1.3888e-08,  3.3025e-07, -5.3568e-06, -2.0778e-06, -2.7470e-06,\n",
      "           1.0874e-07, -4.3026e-06, -3.1775e-08, -1.3380e-09, -2.3559e-06,\n",
      "          -1.7084e-09, -9.4255e-06,  2.6611e-06,  7.8697e-07, -5.3748e-08,\n",
      "           1.0649e-05, -2.6270e-07, -3.8041e-07,  1.9315e-08, -7.3593e-08,\n",
      "          -4.4816e-06,  5.6049e-10,  8.2062e-07, -1.0063e-08,  4.5128e-08,\n",
      "          -5.1747e-09, -4.5403e-07, -2.9219e-08,  7.7394e-07,  2.2411e-07,\n",
      "          -3.3084e-07,  2.4516e-09,  3.5821e-10, -1.8521e-09,  1.8354e-07,\n",
      "          -1.8954e-05,  2.7999e-10, -8.5999e-08, -2.1377e-06, -1.6716e-06,\n",
      "           1.6572e-08, -2.3315e-06,  1.8895e-07,  6.3496e-06, -3.1833e-07,\n",
      "           6.2023e-07,  8.4298e-10, -8.2967e-07,  3.7212e-06,  1.1039e-06,\n",
      "          -9.0300e-09, -2.6949e-08, -8.5791e-10, -1.1635e-08, -7.3959e-09,\n",
      "          -9.7975e-06,  4.5500e-07, -5.4868e-06,  4.2644e-06,  3.0767e-06,\n",
      "           4.9342e-06,  4.9311e-07,  1.9163e-05,  2.2566e-08, -1.8834e-08,\n",
      "           2.0761e-06,  2.9012e-06,  3.2295e-07, -7.7596e-08,  1.3192e-07,\n",
      "           5.7596e-06, -3.6992e-05, -3.0007e-07,  4.7662e-08, -3.7218e-07,\n",
      "          -5.0134e-06,  3.3447e-08, -4.8486e-06,  5.2302e-07,  3.9796e-06,\n",
      "           6.6157e-06,  1.0902e-06,  2.3589e-08,  1.0390e-07,  1.5728e-07,\n",
      "           1.2887e-05, -8.6768e-07, -2.0269e-07,  7.0103e-07,  1.9808e-05,\n",
      "           9.1876e-08, -1.8358e-07, -3.2414e-05, -8.2112e-06,  4.8515e-08,\n",
      "          -1.7336e-05,  3.3777e-06, -4.5661e-07,  3.0050e-06,  1.6503e-09,\n",
      "          -7.2160e-09,  7.9344e-07, -5.0495e-09,  2.4069e-07, -4.3610e-06,\n",
      "           2.7735e-06,  2.7815e-07, -2.0350e-07,  1.6112e-08, -2.0510e-06,\n",
      "          -4.1556e-06, -1.0095e-06, -4.1982e-06,  9.7768e-06,  3.5272e-08,\n",
      "           4.1931e-06,  3.6369e-07, -7.5657e-08,  1.3493e-06, -9.1426e-06,\n",
      "           1.5492e-06, -5.5606e-08,  2.8326e-10, -6.2274e-06,  1.1553e-07,\n",
      "           1.3244e-07,  1.1595e-08,  1.0701e-07, -1.7686e-10,  1.3434e-06,\n",
      "           7.6663e-07,  3.0214e-06,  3.1273e-06,  1.8162e-07, -8.7588e-09,\n",
      "          -5.3203e-08,  1.3994e-06, -3.9140e-07, -5.9820e-06,  1.8577e-06,\n",
      "          -2.9392e+00,  2.7478e-05,  1.7482e-07, -1.0052e-06,  1.1428e-06,\n",
      "           3.5725e-06,  1.6826e-08, -3.2411e-08,  1.3470e-06,  9.9076e-08,\n",
      "          -9.7098e-06, -3.4484e-06,  5.2059e-07,  9.9433e-09, -1.4315e-06,\n",
      "           1.1571e-07, -1.0162e-06, -1.9451e-06,  1.3364e-07,  2.0557e-09,\n",
      "           1.8360e-05,  2.3615e-09,  1.5429e-06, -1.3612e-05, -1.3375e-08,\n",
      "          -1.1263e-05,  6.5371e-06,  3.0841e-06,  2.1411e-09,  6.0650e-08,\n",
      "          -3.8018e-06, -2.6458e-07,  3.9600e-09, -1.6493e-09, -4.5734e-07,\n",
      "           1.6022e-09, -3.6367e-09,  8.5293e-07, -3.3017e-08,  1.1829e-05,\n",
      "          -3.8856e-08, -7.8512e-08, -1.2441e-07, -8.3563e-07,  3.5845e-07,\n",
      "          -1.1632e-06, -3.2378e-05,  5.1766e-08, -3.2791e-07, -1.0741e-08,\n",
      "          -4.8005e-06, -3.0898e-07,  2.0715e-06,  1.1634e-07,  1.9251e-07,\n",
      "          -6.8956e-08,  1.6680e-06,  3.3701e-07,  6.6928e-09, -3.8224e-08,\n",
      "           2.0501e-05,  4.8596e-07,  6.9212e-08, -3.0852e-07,  2.6557e-09,\n",
      "           1.1923e-07, -8.0101e-09,  7.0794e-08, -3.8855e-07, -1.8652e-07,\n",
      "          -2.3786e-06, -1.5048e-08,  1.0523e-08,  1.7478e-07, -1.3772e-08,\n",
      "           1.3210e-06, -1.8784e-06,  2.4997e-06,  3.5084e-07,  3.1842e-10,\n",
      "           9.0063e-09, -7.8309e-07,  5.2683e-06,  1.2784e-05, -4.2034e-08,\n",
      "          -6.3294e-09, -2.5147e-05,  2.9737e-08, -3.2164e-08, -3.1089e-06,\n",
      "          -6.6904e-08, -1.1640e-05, -8.9892e-10, -1.2145e-06,  1.5709e-06,\n",
      "           1.2708e-05,  6.3839e-08,  2.4791e-07,  7.2218e-08, -1.3138e-06,\n",
      "           1.6254e-08,  1.1444e-07, -6.5649e-06, -3.8067e-09,  6.3245e-07,\n",
      "           2.2508e-07, -3.5040e-06,  7.8615e-06,  2.3449e-08, -1.6639e-08,\n",
      "          -1.5493e-08,  1.7303e-08,  4.1829e-06,  3.4123e-07, -7.8461e-07,\n",
      "           1.5215e-07, -2.0604e-06,  6.5877e-07, -1.1104e-06,  1.2191e-06,\n",
      "           8.6716e-07, -1.7774e-08, -2.4495e-07,  1.7908e-06, -1.5804e-06,\n",
      "          -8.3130e-07, -3.4135e-07,  5.9923e-06, -4.0097e-09,  1.3151e-06,\n",
      "          -3.6523e-09, -1.2073e-06,  1.5975e-07, -4.1334e-09,  1.3149e-06,\n",
      "          -1.9978e-07,  1.8486e-07, -1.6621e-07, -1.2257e-09,  1.2581e-05,\n",
      "           3.0089e-06, -6.8363e-08, -2.2873e-09,  4.8389e-07,  7.7758e-07,\n",
      "           9.1260e-08, -1.0340e-06, -6.4681e-07, -2.7343e-06,  6.2495e-07,\n",
      "           2.1049e-06, -2.5364e-07, -4.6147e-08, -8.3495e-07,  2.7527e-08,\n",
      "          -1.7728e-06, -2.1282e-05, -9.3445e-07,  5.4204e-07, -1.6911e-08,\n",
      "          -4.9603e-09,  6.9281e-06, -2.1246e-08,  2.5717e-06,  1.0584e-08,\n",
      "          -9.4637e-08,  2.0147e-06,  2.3032e-06,  2.3860e-06, -2.1552e-08,\n",
      "          -6.6082e-09,  1.6799e-05, -2.3743e-06,  1.1095e-08,  9.3728e-07,\n",
      "          -4.6303e-09, -4.5186e-09, -6.7962e-09, -1.7106e-08, -5.3569e-07,\n",
      "           3.9490e-08,  6.3466e-09, -3.9532e-06,  1.4323e-05,  1.4552e-09,\n",
      "          -4.7709e-06,  1.8672e-06, -2.3148e-06,  4.7150e-07,  5.4807e-07,\n",
      "          -6.5502e-07,  1.8981e-07,  2.8571e-09,  4.2063e-09,  3.1712e-08,\n",
      "          -5.6101e-07, -3.3603e-07, -4.8259e-06, -1.1903e-07, -1.2574e-05,\n",
      "          -1.0291e-08,  5.6589e-06, -3.3860e-07, -1.4214e-07, -1.1464e-06,\n",
      "           1.4969e-05, -9.9780e-06,  4.2101e-08, -4.2471e-07, -1.9702e-06,\n",
      "          -1.4949e-08,  6.7702e-08, -3.3593e-08,  1.7504e-06, -4.4923e-06,\n",
      "           7.6709e-06, -3.6925e-04, -1.0527e-07, -3.6139e-07,  2.1184e-07,\n",
      "           2.2556e-07,  1.1836e-07,  2.1052e-09, -1.2651e-08, -6.2661e-06,\n",
      "          -9.0347e-06, -6.6332e-09,  2.4092e-06,  1.0344e-06, -9.1272e-08,\n",
      "           7.9553e-07,  5.8131e-08,  3.6408e-07, -6.0187e-07, -2.8670e-06,\n",
      "           2.0006e-07,  8.7373e-09, -2.4830e-06,  9.6273e-06,  6.2010e-07,\n",
      "          -9.2130e-08,  4.0188e-06,  4.9999e-08, -2.8333e-09, -7.3779e-09,\n",
      "          -1.2982e-06, -5.7213e-07, -2.5262e-06,  2.0549e-06,  1.2422e-05,\n",
      "           3.0170e-09,  6.0589e-08,  1.7087e-07, -1.6573e-06,  5.5917e-08,\n",
      "          -7.3781e-06,  6.5990e-06,  1.4642e-06, -4.5863e-07, -2.4139e-08,\n",
      "          -1.1933e-06, -3.2544e-08, -1.3277e-07, -1.0180e-07, -3.4549e-07,\n",
      "          -4.7493e-07,  3.0314e-06, -1.9502e-06,  1.6607e-07, -1.4059e-07,\n",
      "          -3.2411e-06, -1.2351e-06,  1.3018e-06, -5.1746e-06, -6.0567e-09,\n",
      "           8.0995e-09, -2.5051e-07,  6.3932e-08, -7.1051e-06,  6.2887e-06,\n",
      "           1.5063e-06, -1.3608e-08, -5.9592e-07, -1.0121e-06,  2.2344e-06,\n",
      "          -9.7173e-09,  5.5005e-06,  8.0114e-07, -1.3357e-05, -1.6829e-08,\n",
      "           5.2020e-07, -1.0246e-08, -3.6627e-07, -6.9186e-07,  1.5120e-06,\n",
      "          -1.1171e-08, -8.9179e-08,  3.1674e-06, -3.2056e-08,  7.5366e-08,\n",
      "           3.3973e-08,  7.3093e-06,  6.1867e-07, -2.1678e-07, -3.1126e-08,\n",
      "          -1.4283e-06,  8.8902e-08, -4.0483e-06, -1.8835e-05,  2.0241e-07,\n",
      "           6.6565e-08,  2.4728e-07, -4.9105e-09, -3.6491e-07,  4.1599e-09,\n",
      "          -6.8935e-08,  4.7498e-07, -1.4405e-09, -2.3634e-08, -6.7978e-06,\n",
      "           6.5777e-10, -2.2546e-07, -1.1316e-06, -2.4625e-06,  2.6516e-09,\n",
      "           4.7425e-08, -9.3632e-06,  5.2651e-06,  3.2771e-07, -4.6329e-09,\n",
      "          -1.5464e-08, -6.1605e-10,  1.9835e-06,  2.1300e-08, -2.4945e-09,\n",
      "           9.7230e-06, -5.8208e-06,  6.9670e-08, -1.6276e-06, -7.9235e-06,\n",
      "           4.2358e-07, -9.8639e-08, -4.0145e-07,  4.9376e-07, -6.1349e-09,\n",
      "           5.2211e-09,  1.1231e-06, -1.9232e-08,  8.3668e-08, -2.0639e-08,\n",
      "           3.2266e-06,  4.5991e-06,  5.6453e-07, -1.1690e-08,  3.1363e-06,\n",
      "           7.9319e-07,  6.3972e-07,  2.4431e-06,  1.3691e-07,  3.4397e-07,\n",
      "           5.9716e-07, -6.7640e-08,  4.2014e-07, -1.2256e-07, -1.9971e-05,\n",
      "           4.1802e-09,  7.2193e-07,  1.4406e-07, -1.3008e-08,  7.8154e-09,\n",
      "           1.9151e-06,  7.1253e-07,  1.8491e-07, -3.6584e-08, -7.2075e-07,\n",
      "           5.4406e-09, -2.8820e-07, -1.0357e-05, -1.2787e-05, -2.7474e-09,\n",
      "           4.3042e-08,  9.2539e-08,  1.1078e-06,  4.8793e-07,  1.0743e-08,\n",
      "          -4.2308e-07, -1.2427e-08, -1.7520e-07,  3.1959e-06,  3.0790e-08,\n",
      "           8.1962e-07,  1.5018e-06,  3.3025e-07,  1.9033e-06, -2.8258e-05,\n",
      "          -4.0832e-06,  3.2066e-06, -1.3506e-09, -1.7938e-08, -9.5366e-06,\n",
      "           4.1767e-06,  9.4760e-08,  7.0537e-09, -5.8291e-08,  4.0933e-06,\n",
      "           9.3245e-08,  1.4259e-08, -8.7811e-09,  2.0307e-08, -2.2486e-07,\n",
      "           3.6306e-06,  5.9481e-09, -6.3747e-07, -5.8849e-07, -3.4337e-08,\n",
      "           1.4729e-06,  1.7147e-08, -5.9682e-08, -1.2062e-06, -1.2547e-07,\n",
      "          -2.1901e-06, -1.6813e-06,  2.1180e-07,  6.2201e-08,  1.6180e-06,\n",
      "          -1.4157e-05,  1.2282e-05,  1.9979e-09, -1.0967e-09,  1.4225e-07,\n",
      "          -4.2317e-09, -7.5325e-06, -5.8693e-06, -4.7341e-06,  1.6854e-08,\n",
      "          -4.6850e-08,  3.3433e-09, -1.1984e-08, -1.7735e-08, -9.4798e-07,\n",
      "          -1.2299e-06, -2.9764e-08, -1.1842e-07, -6.1587e-06,  7.0409e-08,\n",
      "           6.7254e-08, -2.3478e-06, -2.6517e-07,  2.7608e-06,  2.3073e-09,\n",
      "           7.2569e-08, -7.0159e-10,  3.1021e-07,  2.9071e-08,  4.6543e-09,\n",
      "           1.9451e-07, -1.7354e-09,  1.3059e-07,  2.5021e-09, -6.0248e-08,\n",
      "           1.3928e-07, -1.6684e-07,  4.6277e-07,  1.2854e-07, -5.1473e-06,\n",
      "           1.0356e-07, -1.8853e-06,  8.8526e-10,  2.5783e-07, -3.5606e-06,\n",
      "           1.6781e-05,  3.0114e-06, -4.5087e-07, -9.2964e-08, -1.4801e-05,\n",
      "           1.2632e-07, -1.3177e-05, -5.8259e-07,  2.0617e-05,  4.9409e-07,\n",
      "           6.1154e-08,  5.1906e-06,  1.2295e-05, -5.4655e-07, -6.9090e-07,\n",
      "           3.7675e-06,  5.6017e-09, -2.4236e-08, -6.7151e-08, -8.4430e-09,\n",
      "          -7.9482e-07,  8.9041e-08,  5.2378e-08, -3.9485e-06, -1.8214e-06,\n",
      "          -1.7094e-07,  1.1340e-07, -1.3492e-06, -4.8415e-06,  1.4066e-07,\n",
      "          -4.3767e-07, -4.1373e-08,  7.2860e-08, -8.9656e-08, -4.6104e-09,\n",
      "          -7.4272e-07, -1.5907e-05,  3.6720e-06,  1.5925e-06,  5.7099e-08,\n",
      "           1.7195e-09,  2.5247e-06, -4.4720e-08,  5.2508e-08,  5.4985e-08,\n",
      "           7.8396e-08,  2.1274e-06,  3.1347e-07,  8.5338e-08, -2.8651e-07,\n",
      "          -1.0325e-07, -1.7460e-06,  3.5058e-06, -3.0202e-06,  2.8637e-07,\n",
      "          -4.5763e-07, -6.7738e-06, -3.2277e-07, -2.2951e-08, -2.5471e-07,\n",
      "          -2.1949e-09,  1.7677e-06,  6.9713e-08, -5.6668e-07, -1.1175e-07,\n",
      "           1.0015e-07,  6.1517e-07, -1.1322e-08, -2.1065e-06,  2.4826e-08,\n",
      "           4.2148e-06, -6.9230e-09, -8.9478e-08, -7.0129e-08,  3.3094e-09,\n",
      "           2.1308e-06, -1.3334e-08, -2.2185e-06,  5.5710e-08,  2.2817e-08,\n",
      "          -5.1039e-09,  5.7348e-06, -2.1602e-07,  8.8378e-09, -2.1920e-06,\n",
      "          -1.5670e-07, -7.1959e-09,  1.7429e-07,  3.3435e-07,  1.8197e-07,\n",
      "          -6.0268e-06, -4.1070e-08, -9.2149e-06, -1.2947e-07, -2.9009e-06,\n",
      "           4.0414e-06,  4.2962e-06,  1.2600e-07,  5.2677e-08, -2.4173e-05,\n",
      "          -1.6576e-06, -1.0721e-08, -5.0578e-07,  2.3554e-08, -6.2507e-06,\n",
      "           7.5529e-07, -2.0994e-05,  2.9447e-06,  6.8881e-08,  4.8714e-07,\n",
      "          -7.9799e-07,  3.2075e-07, -4.6073e-06]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1839e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9462e-07,\n",
      "           3.1838e-07, -3.4247e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5569e-07,  ..., -7.9456e-07,\n",
      "           3.1840e-07, -3.4248e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5567e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4248e-06]]], device='cuda:0'), 'cls_feats': tensor([[-6.2369e-02,  6.7438e-02, -6.7392e-02, -3.9794e-02, -6.3808e-02,\n",
      "         -6.7367e-02, -6.6934e-02, -6.7346e-02,  6.7608e-02,  5.9141e-02,\n",
      "          6.7032e-02,  6.7446e-02,  6.5307e-02, -1.1194e-06,  1.1023e-07,\n",
      "          3.8261e-02,  6.6431e-02, -6.3736e-02,  6.7383e-02, -6.7404e-02,\n",
      "          6.4508e-02,  6.7620e-02,  6.7284e-02, -6.6964e-02,  6.2817e-02,\n",
      "          6.7301e-02, -6.7417e-02,  6.7087e-02,  6.6913e-02,  6.7109e-02,\n",
      "         -6.7469e-02,  2.8371e-02,  4.2128e-02, -1.6269e-03,  6.7079e-02,\n",
      "          6.6979e-02, -6.6333e-02, -6.7239e-02, -6.6659e-02,  1.1936e-08,\n",
      "          2.0455e-03,  6.4305e-02, -6.7023e-02, -6.6528e-02,  6.7345e-02,\n",
      "         -6.7515e-02, -6.7175e-02, -6.7290e-02, -6.2325e-02,  6.7243e-02,\n",
      "          6.6173e-02, -6.7563e-02, -6.7561e-02, -6.6837e-02, -6.7201e-02,\n",
      "          6.3929e-02,  6.6990e-02,  6.7497e-02,  6.7174e-02, -6.2646e-02,\n",
      "         -6.7547e-02,  6.8752e-08,  6.7104e-02, -6.5181e-02, -6.7592e-02,\n",
      "          3.9844e-03,  3.7063e-02, -6.4575e-02,  3.7483e-02,  6.6982e-02,\n",
      "          6.7271e-02, -4.6332e-09,  6.7342e-02,  6.2820e-02, -4.1235e-02,\n",
      "         -6.7368e-02, -6.6958e-02, -6.7113e-02,  6.7086e-02, -6.6908e-02,\n",
      "          6.7395e-02,  6.7414e-02, -2.9495e-03,  6.7512e-02,  6.1731e-02,\n",
      "         -2.2837e-09,  6.7376e-02,  6.2175e-02, -6.6956e-02,  6.4848e-02,\n",
      "          1.7535e-02,  3.6893e-02, -6.5943e-02,  6.4621e-02, -6.2865e-02,\n",
      "          6.2686e-02,  6.7344e-02,  6.7112e-02, -6.3725e-02, -6.7618e-02,\n",
      "         -4.3014e-02, -6.7607e-02, -7.1798e-07, -6.6797e-02, -6.7291e-02,\n",
      "         -2.3954e-07,  6.3011e-08, -1.6364e-08,  6.7339e-02, -6.7345e-02,\n",
      "         -7.7168e-03, -6.6899e-02, -6.7301e-02,  6.1825e-02,  1.2671e-03,\n",
      "          6.4608e-02, -2.9232e-04, -2.4108e-02, -6.7459e-02,  6.0292e-07,\n",
      "          6.4932e-02, -6.3537e-02,  1.7254e-05,  6.4727e-02,  6.7343e-02,\n",
      "         -7.2030e-10,  5.9043e-02,  6.7256e-02,  6.7341e-02, -6.6580e-02,\n",
      "          2.3766e-07, -6.7408e-02, -6.7751e-07, -6.3785e-02, -9.5706e-09,\n",
      "         -6.5586e-02,  6.4491e-02,  6.7181e-02,  2.6073e-02,  6.7416e-02,\n",
      "         -7.2806e-11, -1.4449e-03,  8.7101e-04, -6.5215e-02,  6.6613e-02,\n",
      "          1.3377e-03,  6.6549e-02,  6.7473e-02, -6.3610e-02,  6.6497e-02,\n",
      "         -6.6815e-02,  6.7569e-02,  6.7249e-02,  2.1300e-09, -6.7339e-02,\n",
      "         -7.5100e-06, -6.3818e-02, -6.7483e-02, -6.4382e-02, -6.7273e-02,\n",
      "          3.0603e-02, -6.7539e-02,  5.7865e-02, -6.2802e-02, -6.6204e-02,\n",
      "          6.4873e-02,  6.7604e-02, -6.2256e-02,  6.6797e-02, -6.7126e-02,\n",
      "          3.8525e-08, -5.7044e-02, -6.4657e-03, -6.4590e-02, -6.7163e-02,\n",
      "          5.8586e-08,  6.7447e-02, -6.7242e-02,  3.4424e-09,  6.1683e-02,\n",
      "         -2.3804e-02,  6.7520e-02,  6.6989e-02, -6.7084e-02,  6.7247e-02,\n",
      "         -6.4425e-02,  5.5586e-02, -7.8754e-13, -6.5376e-02,  6.7446e-02,\n",
      "          6.7162e-02,  6.5131e-02, -6.4461e-02,  6.7130e-02, -6.2440e-02,\n",
      "          6.7013e-02,  6.3599e-02,  6.6932e-02,  6.6517e-02,  6.7303e-02,\n",
      "         -6.7004e-02, -6.7336e-02,  6.7429e-02,  6.7268e-02,  7.7366e-08,\n",
      "         -5.0665e-02, -6.6271e-02,  6.7131e-02,  6.6349e-02, -6.6659e-02,\n",
      "         -6.4530e-02, -6.7165e-02,  4.9314e-02, -6.7408e-02,  6.6034e-05,\n",
      "         -8.6171e-07, -6.7051e-02,  6.4380e-02, -6.7154e-02,  6.2915e-02,\n",
      "          7.6205e-06,  3.6394e-02,  6.7509e-02, -6.3362e-02,  3.6703e-02,\n",
      "          6.5037e-02,  6.7369e-02,  6.7092e-02,  6.7445e-02,  6.7513e-02,\n",
      "          6.5877e-02, -6.9070e-09,  6.3248e-02, -6.7217e-02, -6.7186e-02,\n",
      "          6.7350e-02, -2.8146e-03, -6.5798e-02,  6.4767e-02,  6.7612e-02,\n",
      "          6.5310e-02,  6.4819e-02,  6.6173e-02,  6.7063e-02, -2.7954e-02,\n",
      "          6.6891e-02, -2.4323e-03,  6.6283e-02, -3.5880e-02,  6.6900e-02,\n",
      "          6.6905e-02,  6.7645e-02,  5.3848e-04,  6.2569e-02,  6.3214e-02,\n",
      "         -6.7352e-02, -5.6301e-04, -3.8800e-05, -6.7625e-02,  6.6964e-02,\n",
      "         -6.6986e-02, -6.7418e-02,  6.1452e-02,  1.4334e-07, -6.7413e-02,\n",
      "         -6.0843e-02,  6.7342e-02,  6.4143e-02,  6.5083e-02,  6.4727e-02,\n",
      "          6.6866e-09, -6.7189e-02,  5.7467e-02, -6.6783e-02,  6.7541e-02,\n",
      "         -6.6345e-02,  6.3640e-02,  6.7420e-02,  6.7416e-02,  6.0693e-02,\n",
      "         -6.7499e-02, -6.4516e-02,  6.5031e-02,  6.7353e-02,  9.6023e-06,\n",
      "         -6.7508e-02, -6.3883e-03, -6.5111e-02,  2.7676e-02, -6.5434e-02,\n",
      "         -6.5638e-02,  3.1789e-02, -6.7198e-02, -6.7201e-02, -3.2726e-06,\n",
      "         -6.6969e-02, -6.6967e-02, -6.2186e-02, -6.6638e-02,  6.3329e-02,\n",
      "         -6.7157e-02, -6.7101e-02,  6.6704e-02,  6.4811e-02, -6.7486e-02,\n",
      "          6.7260e-02,  1.7546e-07, -5.8865e-05,  6.7277e-02,  7.4857e-07,\n",
      "          5.3150e-02,  5.9514e-02, -4.8562e-07, -6.6338e-02,  6.7382e-02,\n",
      "         -6.7597e-02,  6.7519e-02,  6.7017e-02, -6.6577e-02, -5.2690e-03,\n",
      "          3.9099e-08,  6.7325e-02,  6.7514e-02, -6.7615e-02,  6.3006e-02,\n",
      "         -6.7263e-02, -6.6503e-02, -6.7047e-02, -2.6836e-02, -6.4434e-02,\n",
      "         -6.7331e-02, -6.4508e-02,  6.5750e-02,  5.6114e-08, -6.7032e-02,\n",
      "          6.7208e-02,  9.6642e-05,  3.8482e-03, -6.4239e-02,  6.6331e-02,\n",
      "         -6.6616e-02,  6.8670e-08, -6.7178e-02, -3.6938e-02, -6.7429e-02,\n",
      "         -6.7554e-02,  6.2161e-02,  6.2668e-02,  3.7008e-02,  6.7574e-02,\n",
      "          6.7368e-02,  6.5898e-02,  3.7291e-02,  6.6727e-02, -6.3257e-02,\n",
      "         -6.7228e-02,  6.6562e-02,  6.1792e-02, -6.7257e-02, -2.5518e-02,\n",
      "         -6.0100e-02,  6.6731e-02, -6.4731e-02,  6.7266e-02,  1.8924e-08,\n",
      "         -6.7595e-02, -6.7123e-02, -6.7418e-02, -6.7345e-02, -6.6897e-02,\n",
      "          5.3282e-03, -6.2533e-02, -6.7356e-02,  6.7596e-02,  6.7384e-02,\n",
      "          6.5824e-02, -6.7330e-02, -6.7421e-02,  6.2648e-02,  6.1200e-02,\n",
      "         -6.0875e-03,  6.7457e-02,  6.7371e-02, -6.7288e-02,  6.6429e-02,\n",
      "         -6.2117e-02,  6.6923e-02,  7.2089e-07,  5.1562e-04,  4.7186e-03,\n",
      "         -6.7341e-02,  6.2035e-02,  6.5086e-02,  6.7276e-02,  6.6795e-02,\n",
      "         -1.8734e-03, -6.6711e-02, -6.4561e-02,  6.6985e-02,  6.7410e-02,\n",
      "          3.2877e-02,  3.8970e-02,  6.7305e-02,  6.7098e-02, -6.7430e-02,\n",
      "          6.7094e-02, -6.4319e-02,  6.1292e-02, -6.7501e-02, -6.4619e-02,\n",
      "         -6.7115e-02,  6.7135e-02, -6.6579e-02,  2.8509e-07, -6.7531e-02,\n",
      "         -6.4620e-02, -6.7388e-02,  6.7252e-02,  6.7096e-02, -6.7506e-02,\n",
      "          2.8633e-09, -6.5300e-02, -6.7263e-02, -2.4722e-04, -4.0494e-02,\n",
      "         -3.2954e-02,  6.5160e-02, -3.9293e-02, -6.6729e-02, -6.7242e-02,\n",
      "         -5.8362e-02,  6.7063e-02,  6.6790e-02,  6.7391e-02,  6.3057e-02,\n",
      "          6.7479e-02, -4.0264e-02,  6.6780e-02,  6.1174e-02,  8.4328e-05,\n",
      "          6.7444e-02, -6.4527e-02, -6.7261e-02, -1.2016e-08, -6.6994e-02,\n",
      "          6.7437e-02, -4.0952e-09, -6.7023e-02,  6.7428e-02,  6.4790e-02,\n",
      "         -5.2360e-07,  6.7501e-02, -3.3817e-05,  6.7234e-02,  6.7123e-02,\n",
      "          6.6683e-02,  1.6733e-04,  6.6854e-02,  6.7504e-02, -5.4591e-02,\n",
      "          6.7375e-02, -6.7458e-02, -1.0713e-07, -6.7394e-02, -2.3773e-05,\n",
      "         -6.2634e-02, -6.7195e-02,  6.6591e-02, -6.7483e-02, -6.6754e-02,\n",
      "         -6.7320e-02,  9.0393e-08, -6.7350e-02,  6.7033e-02, -5.5030e-02,\n",
      "         -5.5602e-02,  1.5041e-07,  6.6845e-02, -6.7374e-02, -6.3212e-02,\n",
      "          6.4717e-02,  6.6441e-02, -6.5841e-02, -6.0728e-03,  6.7371e-02,\n",
      "         -6.7430e-02,  6.4964e-02,  5.9442e-02, -6.7371e-02,  5.6901e-02,\n",
      "          6.4991e-02,  1.2980e-03, -6.7591e-02, -6.6276e-02,  3.6318e-02,\n",
      "          6.7483e-02, -6.5320e-02,  6.5499e-02,  6.6371e-02, -6.7563e-02,\n",
      "         -6.7270e-02,  5.6841e-07, -7.7628e-03, -6.7428e-02,  6.1854e-02,\n",
      "         -6.7351e-03,  6.7140e-02,  6.0360e-02,  3.2200e-02,  6.7526e-02,\n",
      "         -6.4536e-02, -1.1429e-03,  5.4412e-02,  6.6289e-02, -1.0021e-04,\n",
      "         -6.7169e-02,  6.3203e-03,  6.3468e-02,  5.4763e-02, -6.7518e-02,\n",
      "          6.4844e-02,  5.9107e-02,  6.4657e-02, -6.7161e-02,  6.7252e-02,\n",
      "         -6.3953e-02,  8.2347e-03,  6.5119e-02, -6.7534e-02, -6.7339e-02,\n",
      "         -6.7405e-02,  1.1444e-04,  6.7375e-02, -6.7574e-02,  6.7164e-02,\n",
      "          4.2723e-03, -5.0971e-02,  6.2805e-02, -6.7219e-02,  1.4702e-08,\n",
      "          6.7277e-02,  6.2599e-02, -6.7550e-02,  4.1347e-05, -6.7279e-02,\n",
      "          3.5698e-04,  6.0857e-02,  6.7389e-02,  6.4545e-02, -6.7400e-02,\n",
      "          1.0368e-03,  6.4676e-02, -6.2545e-02, -6.6346e-02, -6.5358e-02,\n",
      "         -6.7396e-02,  1.5532e-09,  1.0901e-06, -6.7390e-02, -6.7336e-02,\n",
      "          6.2665e-02,  6.4600e-02,  6.7060e-02, -6.4262e-02,  6.6794e-02,\n",
      "         -6.0619e-02,  6.7019e-02, -6.7066e-02, -6.4595e-02, -6.7044e-02,\n",
      "          6.6832e-02, -6.7367e-02, -6.7229e-02, -6.7443e-02,  6.7259e-02,\n",
      "          6.6007e-02,  2.4126e-06, -3.0179e-03,  6.6067e-02,  6.6460e-02,\n",
      "          6.7404e-02, -6.7397e-02, -1.5102e-08, -6.6566e-02,  5.9396e-02,\n",
      "         -6.5803e-02,  3.0325e-04,  6.6892e-02, -6.4522e-02, -6.7451e-02,\n",
      "         -6.7552e-02,  6.2458e-02, -3.3686e-02, -6.7554e-02,  5.6933e-07,\n",
      "         -6.6965e-02, -6.4766e-02,  6.5704e-02,  6.7033e-02, -6.7588e-02,\n",
      "         -6.5439e-02,  6.6992e-02, -6.5156e-02,  6.4610e-02,  6.2246e-02,\n",
      "          6.7464e-02, -1.0333e-08, -5.2990e-08,  6.7298e-02,  6.4427e-02,\n",
      "         -6.7270e-02, -6.7353e-02, -6.7473e-02, -6.2098e-02, -6.6256e-02,\n",
      "          6.7051e-02, -3.9792e-09,  5.0297e-06,  6.7651e-02, -6.7588e-02,\n",
      "          6.7137e-02, -6.5897e-02,  6.7459e-02, -6.7119e-02, -6.7142e-02,\n",
      "         -6.7032e-02,  1.4208e-03, -2.2531e-03, -3.4002e-05, -6.2485e-02,\n",
      "          6.7047e-02,  1.9950e-06,  6.2619e-02, -4.3295e-04,  6.6979e-02,\n",
      "         -4.1925e-02,  5.7241e-03, -6.7509e-02, -6.7074e-02, -6.7152e-02,\n",
      "         -4.0087e-02,  1.9418e-09,  6.7081e-02, -6.9224e-08,  7.1605e-03,\n",
      "         -6.4432e-02,  6.7467e-02,  6.7016e-02, -4.3124e-06,  6.7321e-02,\n",
      "         -6.7562e-02, -6.2809e-02, -6.7216e-02, -6.7424e-02,  2.5220e-03,\n",
      "          6.7233e-02, -6.7184e-02, -5.7422e-02,  6.7142e-02,  6.6692e-02,\n",
      "         -6.0657e-02,  2.6549e-05,  6.6478e-02, -6.6431e-02,  6.6683e-02,\n",
      "         -6.4142e-02, -4.8963e-05,  2.5485e-02, -1.1370e-03,  6.6871e-02,\n",
      "         -6.7483e-02, -6.7199e-02, -6.7092e-02, -6.7251e-02, -1.1900e-04,\n",
      "          6.6485e-02, -6.6237e-02, -6.2429e-02, -6.7234e-02,  6.7504e-02,\n",
      "          6.7331e-02,  6.5897e-02,  6.7221e-02, -6.6987e-02,  6.6922e-02,\n",
      "         -6.7288e-02, -6.3721e-02,  6.5902e-02, -3.2436e-06,  1.7997e-09,\n",
      "          5.9391e-05,  6.6901e-02,  6.6926e-02, -6.7343e-02, -3.9708e-02,\n",
      "          6.6481e-02, -6.7066e-02, -6.7447e-02, -4.1179e-02, -6.6806e-02,\n",
      "          9.8530e-08, -6.6850e-02, -6.6807e-02,  3.3475e-02,  6.2364e-02,\n",
      "          6.7018e-02,  6.6879e-02,  6.5360e-02, -1.9073e-07, -6.6690e-02,\n",
      "          5.6255e-02,  6.6337e-02,  6.7206e-02, -6.7079e-02, -3.2966e-07,\n",
      "         -6.2678e-02, -6.7446e-02, -6.7237e-02,  6.6416e-02, -6.7501e-02,\n",
      "         -6.7539e-02, -6.3749e-02,  6.4719e-02,  6.4697e-02,  6.6729e-02,\n",
      "         -6.3290e-02, -6.7158e-02, -6.6782e-02,  3.0375e-02,  6.7004e-02,\n",
      "         -4.9525e-06,  6.7568e-02, -1.1982e-06,  6.2767e-02, -6.7334e-02,\n",
      "          6.7172e-02, -6.7377e-02,  5.8678e-02,  6.4428e-02, -6.4449e-02,\n",
      "          6.5896e-02,  6.2609e-02, -6.6776e-02,  6.2950e-02,  5.0737e-03,\n",
      "         -6.7092e-02,  6.7201e-02, -3.7448e-03,  6.7109e-02, -5.7745e-03,\n",
      "         -6.7313e-02, -6.1316e-02, -6.7416e-02, -6.7189e-02, -6.4288e-03,\n",
      "          6.6084e-02,  6.6805e-02, -5.5808e-05,  6.4505e-02,  6.7553e-02,\n",
      "          6.7108e-02,  6.7221e-02, -6.7303e-02, -6.7451e-02, -2.6494e-03,\n",
      "         -2.3323e-09,  6.7276e-02, -6.7297e-02]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0827e-06, -1.7530e-07,  1.5452e-07, -2.2674e-08, -2.2496e-06,\n",
      "          3.2150e-06,  6.0465e-06, -5.3801e-09, -1.8847e-07, -7.7690e-07,\n",
      "         -1.5788e-07,  3.6213e-06, -3.8558e-08, -1.0063e-05, -1.4680e-08,\n",
      "         -1.8126e-07, -2.7730e-06,  9.1713e-09, -2.7553e-06, -5.3852e-09,\n",
      "          9.5103e-07,  3.8168e-09,  7.2278e-08, -1.0654e-08,  6.3138e-07,\n",
      "          3.1689e-06, -2.6612e-06, -1.0689e-09,  2.8926e-06,  4.4980e-06,\n",
      "         -9.2651e-06,  1.0652e-06, -4.1821e-07,  2.2096e-08, -9.7153e-07,\n",
      "          1.0851e-06,  7.0076e-08, -7.6146e-08, -2.9167e-09,  2.1692e-06,\n",
      "          7.3493e-08,  8.4667e-06,  3.4533e-08, -2.6099e-07, -4.6151e-06,\n",
      "          1.3888e-08,  3.3025e-07, -5.3568e-06, -2.0778e-06, -2.7470e-06,\n",
      "          1.0874e-07, -4.3026e-06, -3.1775e-08, -1.3380e-09, -2.3559e-06,\n",
      "         -1.7084e-09, -9.4255e-06,  2.6611e-06,  7.8697e-07, -5.3748e-08,\n",
      "          1.0649e-05, -2.6270e-07, -3.8041e-07,  1.9315e-08, -7.3593e-08,\n",
      "         -4.4816e-06,  5.6049e-10,  8.2062e-07, -1.0063e-08,  4.5128e-08,\n",
      "         -5.1747e-09, -4.5403e-07, -2.9219e-08,  7.7394e-07,  2.2411e-07,\n",
      "         -3.3084e-07,  2.4516e-09,  3.5821e-10, -1.8521e-09,  1.8354e-07,\n",
      "         -1.8954e-05,  2.7999e-10, -8.5999e-08, -2.1377e-06, -1.6716e-06,\n",
      "          1.6572e-08, -2.3315e-06,  1.8895e-07,  6.3496e-06, -3.1833e-07,\n",
      "          6.2023e-07,  8.4298e-10, -8.2967e-07,  3.7212e-06,  1.1039e-06,\n",
      "         -9.0300e-09, -2.6949e-08, -8.5791e-10, -1.1635e-08, -7.3959e-09,\n",
      "         -9.7975e-06,  4.5500e-07, -5.4868e-06,  4.2644e-06,  3.0767e-06,\n",
      "          4.9342e-06,  4.9311e-07,  1.9163e-05,  2.2566e-08, -1.8834e-08,\n",
      "          2.0761e-06,  2.9012e-06,  3.2295e-07, -7.7596e-08,  1.3192e-07,\n",
      "          5.7596e-06, -3.6992e-05, -3.0007e-07,  4.7662e-08, -3.7218e-07,\n",
      "         -5.0134e-06,  3.3447e-08, -4.8486e-06,  5.2302e-07,  3.9796e-06,\n",
      "          6.6157e-06,  1.0902e-06,  2.3589e-08,  1.0390e-07,  1.5728e-07,\n",
      "          1.2887e-05, -8.6768e-07, -2.0269e-07,  7.0103e-07,  1.9808e-05,\n",
      "          9.1876e-08, -1.8358e-07, -3.2414e-05, -8.2112e-06,  4.8515e-08,\n",
      "         -1.7336e-05,  3.3777e-06, -4.5661e-07,  3.0050e-06,  1.6503e-09,\n",
      "         -7.2160e-09,  7.9344e-07, -5.0495e-09,  2.4069e-07, -4.3610e-06,\n",
      "          2.7735e-06,  2.7815e-07, -2.0350e-07,  1.6112e-08, -2.0510e-06,\n",
      "         -4.1556e-06, -1.0095e-06, -4.1982e-06,  9.7768e-06,  3.5272e-08,\n",
      "          4.1931e-06,  3.6369e-07, -7.5657e-08,  1.3493e-06, -9.1426e-06,\n",
      "          1.5492e-06, -5.5606e-08,  2.8326e-10, -6.2274e-06,  1.1553e-07,\n",
      "          1.3244e-07,  1.1595e-08,  1.0701e-07, -1.7686e-10,  1.3434e-06,\n",
      "          7.6663e-07,  3.0214e-06,  3.1273e-06,  1.8162e-07, -8.7588e-09,\n",
      "         -5.3203e-08,  1.3994e-06, -3.9140e-07, -5.9820e-06,  1.8577e-06,\n",
      "         -2.9392e+00,  2.7478e-05,  1.7482e-07, -1.0052e-06,  1.1428e-06,\n",
      "          3.5725e-06,  1.6826e-08, -3.2411e-08,  1.3470e-06,  9.9076e-08,\n",
      "         -9.7098e-06, -3.4484e-06,  5.2059e-07,  9.9433e-09, -1.4315e-06,\n",
      "          1.1571e-07, -1.0162e-06, -1.9451e-06,  1.3364e-07,  2.0557e-09,\n",
      "          1.8360e-05,  2.3615e-09,  1.5429e-06, -1.3612e-05, -1.3375e-08,\n",
      "         -1.1263e-05,  6.5371e-06,  3.0841e-06,  2.1411e-09,  6.0650e-08,\n",
      "         -3.8018e-06, -2.6458e-07,  3.9600e-09, -1.6493e-09, -4.5734e-07,\n",
      "          1.6022e-09, -3.6367e-09,  8.5293e-07, -3.3017e-08,  1.1829e-05,\n",
      "         -3.8856e-08, -7.8512e-08, -1.2441e-07, -8.3563e-07,  3.5845e-07,\n",
      "         -1.1632e-06, -3.2378e-05,  5.1766e-08, -3.2791e-07, -1.0741e-08,\n",
      "         -4.8005e-06, -3.0898e-07,  2.0715e-06,  1.1634e-07,  1.9251e-07,\n",
      "         -6.8956e-08,  1.6680e-06,  3.3701e-07,  6.6928e-09, -3.8224e-08,\n",
      "          2.0501e-05,  4.8596e-07,  6.9212e-08, -3.0852e-07,  2.6557e-09,\n",
      "          1.1923e-07, -8.0101e-09,  7.0794e-08, -3.8855e-07, -1.8652e-07,\n",
      "         -2.3786e-06, -1.5048e-08,  1.0523e-08,  1.7478e-07, -1.3772e-08,\n",
      "          1.3210e-06, -1.8784e-06,  2.4997e-06,  3.5084e-07,  3.1842e-10,\n",
      "          9.0063e-09, -7.8309e-07,  5.2683e-06,  1.2784e-05, -4.2034e-08,\n",
      "         -6.3294e-09, -2.5147e-05,  2.9737e-08, -3.2164e-08, -3.1089e-06,\n",
      "         -6.6904e-08, -1.1640e-05, -8.9892e-10, -1.2145e-06,  1.5709e-06,\n",
      "          1.2708e-05,  6.3839e-08,  2.4791e-07,  7.2218e-08, -1.3138e-06,\n",
      "          1.6254e-08,  1.1444e-07, -6.5649e-06, -3.8067e-09,  6.3245e-07,\n",
      "          2.2508e-07, -3.5040e-06,  7.8615e-06,  2.3449e-08, -1.6639e-08,\n",
      "         -1.5493e-08,  1.7303e-08,  4.1829e-06,  3.4123e-07, -7.8461e-07,\n",
      "          1.5215e-07, -2.0604e-06,  6.5877e-07, -1.1104e-06,  1.2191e-06,\n",
      "          8.6716e-07, -1.7774e-08, -2.4495e-07,  1.7908e-06, -1.5804e-06,\n",
      "         -8.3130e-07, -3.4135e-07,  5.9923e-06, -4.0097e-09,  1.3151e-06,\n",
      "         -3.6523e-09, -1.2073e-06,  1.5975e-07, -4.1334e-09,  1.3149e-06,\n",
      "         -1.9978e-07,  1.8486e-07, -1.6621e-07, -1.2257e-09,  1.2581e-05,\n",
      "          3.0089e-06, -6.8363e-08, -2.2873e-09,  4.8389e-07,  7.7758e-07,\n",
      "          9.1260e-08, -1.0340e-06, -6.4681e-07, -2.7343e-06,  6.2495e-07,\n",
      "          2.1049e-06, -2.5364e-07, -4.6147e-08, -8.3495e-07,  2.7527e-08,\n",
      "         -1.7728e-06, -2.1282e-05, -9.3445e-07,  5.4204e-07, -1.6911e-08,\n",
      "         -4.9603e-09,  6.9281e-06, -2.1246e-08,  2.5717e-06,  1.0584e-08,\n",
      "         -9.4637e-08,  2.0147e-06,  2.3032e-06,  2.3860e-06, -2.1552e-08,\n",
      "         -6.6082e-09,  1.6799e-05, -2.3743e-06,  1.1095e-08,  9.3728e-07,\n",
      "         -4.6303e-09, -4.5186e-09, -6.7962e-09, -1.7106e-08, -5.3569e-07,\n",
      "          3.9490e-08,  6.3466e-09, -3.9532e-06,  1.4323e-05,  1.4552e-09,\n",
      "         -4.7709e-06,  1.8672e-06, -2.3148e-06,  4.7150e-07,  5.4807e-07,\n",
      "         -6.5502e-07,  1.8981e-07,  2.8571e-09,  4.2063e-09,  3.1712e-08,\n",
      "         -5.6101e-07, -3.3603e-07, -4.8259e-06, -1.1903e-07, -1.2574e-05,\n",
      "         -1.0291e-08,  5.6589e-06, -3.3860e-07, -1.4214e-07, -1.1464e-06,\n",
      "          1.4969e-05, -9.9780e-06,  4.2101e-08, -4.2471e-07, -1.9702e-06,\n",
      "         -1.4949e-08,  6.7702e-08, -3.3593e-08,  1.7504e-06, -4.4923e-06,\n",
      "          7.6709e-06, -3.6925e-04, -1.0527e-07, -3.6139e-07,  2.1184e-07,\n",
      "          2.2556e-07,  1.1836e-07,  2.1052e-09, -1.2651e-08, -6.2661e-06,\n",
      "         -9.0347e-06, -6.6332e-09,  2.4092e-06,  1.0344e-06, -9.1272e-08,\n",
      "          7.9553e-07,  5.8131e-08,  3.6408e-07, -6.0187e-07, -2.8670e-06,\n",
      "          2.0006e-07,  8.7373e-09, -2.4830e-06,  9.6273e-06,  6.2010e-07,\n",
      "         -9.2130e-08,  4.0188e-06,  4.9999e-08, -2.8333e-09, -7.3779e-09,\n",
      "         -1.2982e-06, -5.7213e-07, -2.5262e-06,  2.0549e-06,  1.2422e-05,\n",
      "          3.0170e-09,  6.0589e-08,  1.7087e-07, -1.6573e-06,  5.5917e-08,\n",
      "         -7.3781e-06,  6.5990e-06,  1.4642e-06, -4.5863e-07, -2.4139e-08,\n",
      "         -1.1933e-06, -3.2544e-08, -1.3277e-07, -1.0180e-07, -3.4549e-07,\n",
      "         -4.7493e-07,  3.0314e-06, -1.9502e-06,  1.6607e-07, -1.4059e-07,\n",
      "         -3.2411e-06, -1.2351e-06,  1.3018e-06, -5.1746e-06, -6.0567e-09,\n",
      "          8.0995e-09, -2.5051e-07,  6.3932e-08, -7.1051e-06,  6.2887e-06,\n",
      "          1.5063e-06, -1.3608e-08, -5.9592e-07, -1.0121e-06,  2.2344e-06,\n",
      "         -9.7173e-09,  5.5005e-06,  8.0114e-07, -1.3357e-05, -1.6829e-08,\n",
      "          5.2020e-07, -1.0246e-08, -3.6627e-07, -6.9186e-07,  1.5120e-06,\n",
      "         -1.1171e-08, -8.9179e-08,  3.1674e-06, -3.2056e-08,  7.5366e-08,\n",
      "          3.3973e-08,  7.3093e-06,  6.1867e-07, -2.1678e-07, -3.1126e-08,\n",
      "         -1.4283e-06,  8.8902e-08, -4.0483e-06, -1.8835e-05,  2.0241e-07,\n",
      "          6.6565e-08,  2.4728e-07, -4.9105e-09, -3.6491e-07,  4.1599e-09,\n",
      "         -6.8935e-08,  4.7498e-07, -1.4405e-09, -2.3634e-08, -6.7978e-06,\n",
      "          6.5777e-10, -2.2546e-07, -1.1316e-06, -2.4625e-06,  2.6516e-09,\n",
      "          4.7425e-08, -9.3632e-06,  5.2651e-06,  3.2771e-07, -4.6329e-09,\n",
      "         -1.5464e-08, -6.1605e-10,  1.9835e-06,  2.1300e-08, -2.4945e-09,\n",
      "          9.7230e-06, -5.8208e-06,  6.9670e-08, -1.6276e-06, -7.9235e-06,\n",
      "          4.2358e-07, -9.8639e-08, -4.0145e-07,  4.9376e-07, -6.1349e-09,\n",
      "          5.2211e-09,  1.1231e-06, -1.9232e-08,  8.3668e-08, -2.0639e-08,\n",
      "          3.2266e-06,  4.5991e-06,  5.6453e-07, -1.1690e-08,  3.1363e-06,\n",
      "          7.9319e-07,  6.3972e-07,  2.4431e-06,  1.3691e-07,  3.4397e-07,\n",
      "          5.9716e-07, -6.7640e-08,  4.2014e-07, -1.2256e-07, -1.9971e-05,\n",
      "          4.1802e-09,  7.2193e-07,  1.4406e-07, -1.3008e-08,  7.8154e-09,\n",
      "          1.9151e-06,  7.1253e-07,  1.8491e-07, -3.6584e-08, -7.2075e-07,\n",
      "          5.4406e-09, -2.8820e-07, -1.0357e-05, -1.2787e-05, -2.7474e-09,\n",
      "          4.3042e-08,  9.2539e-08,  1.1078e-06,  4.8793e-07,  1.0743e-08,\n",
      "         -4.2308e-07, -1.2427e-08, -1.7520e-07,  3.1959e-06,  3.0790e-08,\n",
      "          8.1962e-07,  1.5018e-06,  3.3025e-07,  1.9033e-06, -2.8258e-05,\n",
      "         -4.0832e-06,  3.2066e-06, -1.3506e-09, -1.7938e-08, -9.5366e-06,\n",
      "          4.1767e-06,  9.4760e-08,  7.0537e-09, -5.8291e-08,  4.0933e-06,\n",
      "          9.3245e-08,  1.4259e-08, -8.7811e-09,  2.0307e-08, -2.2486e-07,\n",
      "          3.6306e-06,  5.9481e-09, -6.3747e-07, -5.8849e-07, -3.4337e-08,\n",
      "          1.4729e-06,  1.7147e-08, -5.9682e-08, -1.2062e-06, -1.2547e-07,\n",
      "         -2.1901e-06, -1.6813e-06,  2.1180e-07,  6.2201e-08,  1.6180e-06,\n",
      "         -1.4157e-05,  1.2282e-05,  1.9979e-09, -1.0967e-09,  1.4225e-07,\n",
      "         -4.2317e-09, -7.5325e-06, -5.8693e-06, -4.7341e-06,  1.6854e-08,\n",
      "         -4.6850e-08,  3.3433e-09, -1.1984e-08, -1.7735e-08, -9.4798e-07,\n",
      "         -1.2299e-06, -2.9764e-08, -1.1842e-07, -6.1587e-06,  7.0409e-08,\n",
      "          6.7254e-08, -2.3478e-06, -2.6517e-07,  2.7608e-06,  2.3073e-09,\n",
      "          7.2569e-08, -7.0159e-10,  3.1021e-07,  2.9071e-08,  4.6543e-09,\n",
      "          1.9451e-07, -1.7354e-09,  1.3059e-07,  2.5021e-09, -6.0248e-08,\n",
      "          1.3928e-07, -1.6684e-07,  4.6277e-07,  1.2854e-07, -5.1473e-06,\n",
      "          1.0356e-07, -1.8853e-06,  8.8526e-10,  2.5783e-07, -3.5606e-06,\n",
      "          1.6781e-05,  3.0114e-06, -4.5087e-07, -9.2964e-08, -1.4801e-05,\n",
      "          1.2632e-07, -1.3177e-05, -5.8259e-07,  2.0617e-05,  4.9409e-07,\n",
      "          6.1154e-08,  5.1906e-06,  1.2295e-05, -5.4655e-07, -6.9090e-07,\n",
      "          3.7675e-06,  5.6017e-09, -2.4236e-08, -6.7151e-08, -8.4430e-09,\n",
      "         -7.9482e-07,  8.9041e-08,  5.2378e-08, -3.9485e-06, -1.8214e-06,\n",
      "         -1.7094e-07,  1.1340e-07, -1.3492e-06, -4.8415e-06,  1.4066e-07,\n",
      "         -4.3767e-07, -4.1373e-08,  7.2860e-08, -8.9656e-08, -4.6104e-09,\n",
      "         -7.4272e-07, -1.5907e-05,  3.6720e-06,  1.5925e-06,  5.7099e-08,\n",
      "          1.7195e-09,  2.5247e-06, -4.4720e-08,  5.2508e-08,  5.4985e-08,\n",
      "          7.8396e-08,  2.1274e-06,  3.1347e-07,  8.5338e-08, -2.8651e-07,\n",
      "         -1.0325e-07, -1.7460e-06,  3.5058e-06, -3.0202e-06,  2.8637e-07,\n",
      "         -4.5763e-07, -6.7738e-06, -3.2277e-07, -2.2951e-08, -2.5471e-07,\n",
      "         -2.1949e-09,  1.7677e-06,  6.9713e-08, -5.6668e-07, -1.1175e-07,\n",
      "          1.0015e-07,  6.1517e-07, -1.1322e-08, -2.1065e-06,  2.4826e-08,\n",
      "          4.2148e-06, -6.9230e-09, -8.9478e-08, -7.0129e-08,  3.3094e-09,\n",
      "          2.1308e-06, -1.3334e-08, -2.2185e-06,  5.5710e-08,  2.2817e-08,\n",
      "         -5.1039e-09,  5.7348e-06, -2.1602e-07,  8.8378e-09, -2.1920e-06,\n",
      "         -1.5670e-07, -7.1959e-09,  1.7429e-07,  3.3435e-07,  1.8197e-07,\n",
      "         -6.0268e-06, -4.1070e-08, -9.2149e-06, -1.2947e-07, -2.9009e-06,\n",
      "          4.0414e-06,  4.2962e-06,  1.2600e-07,  5.2677e-08, -2.4173e-05,\n",
      "         -1.6576e-06, -1.0721e-08, -5.0578e-07,  2.3554e-08, -6.2507e-06,\n",
      "          7.5529e-07, -2.0994e-05,  2.9447e-06,  6.8881e-08,  4.8714e-07,\n",
      "         -7.9799e-07,  3.2075e-07, -4.6073e-06]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 0, 14],\n",
      "         [ 2, 12],\n",
      "         [10, 13],\n",
      "         [ 3, 18],\n",
      "         [ 9,  1],\n",
      "         [ 5,  2],\n",
      "         [10,  1],\n",
      "         [ 4, 12],\n",
      "         [ 7, 14],\n",
      "         [ 9,  5],\n",
      "         [ 4, 11],\n",
      "         [ 9, 12],\n",
      "         [ 9,  6],\n",
      "         [10, 18],\n",
      "         [ 8, 18],\n",
      "         [ 2,  5],\n",
      "         [10,  2],\n",
      "         [ 9,  8],\n",
      "         [ 2, 10],\n",
      "         [ 6, 16],\n",
      "         [ 9,  4],\n",
      "         [10, 10],\n",
      "         [ 7, 13],\n",
      "         [ 0, 16],\n",
      "         [ 0, 18],\n",
      "         [ 3, 12],\n",
      "         [ 3,  8],\n",
      "         [10,  9],\n",
      "         [ 6, 10],\n",
      "         [ 3, 13],\n",
      "         [ 8, 10],\n",
      "         [ 4,  7],\n",
      "         [ 0, 12],\n",
      "         [ 6,  4],\n",
      "         [ 8,  1],\n",
      "         [ 3,  7],\n",
      "         [ 2,  4],\n",
      "         [ 2,  7],\n",
      "         [ 5, 13],\n",
      "         [ 8, 15],\n",
      "         [ 4,  9],\n",
      "         [ 8, 17],\n",
      "         [ 6, 17],\n",
      "         [ 8, 12],\n",
      "         [ 1, 15],\n",
      "         [10, 12],\n",
      "         [10,  6],\n",
      "         [ 2,  9],\n",
      "         [ 8, 11],\n",
      "         [ 8, 13],\n",
      "         [ 4,  0],\n",
      "         [ 0, 15],\n",
      "         [ 8,  0],\n",
      "         [ 8,  4],\n",
      "         [ 0,  4],\n",
      "         [ 3, 17],\n",
      "         [ 8,  2],\n",
      "         [ 3,  6],\n",
      "         [ 9, 11],\n",
      "         [ 8,  3],\n",
      "         [ 4,  6],\n",
      "         [ 4,  8],\n",
      "         [10,  3],\n",
      "         [ 7, 11],\n",
      "         [10,  8],\n",
      "         [ 4,  2],\n",
      "         [ 6, 13],\n",
      "         [ 5, 11],\n",
      "         [ 0,  9],\n",
      "         [ 0,  7],\n",
      "         [ 1, 14],\n",
      "         [ 5, 12],\n",
      "         [ 5,  5],\n",
      "         [ 1, 13],\n",
      "         [ 8,  6],\n",
      "         [ 0, 11],\n",
      "         [ 9,  0],\n",
      "         [ 0, 13],\n",
      "         [ 1, 18],\n",
      "         [ 3,  9],\n",
      "         [ 2, 16],\n",
      "         [10, 16],\n",
      "         [ 1,  8],\n",
      "         [ 6, 18],\n",
      "         [ 1,  0],\n",
      "         [ 0,  8],\n",
      "         [ 5, 14],\n",
      "         [ 5,  4],\n",
      "         [ 6,  9],\n",
      "         [ 2, 17],\n",
      "         [ 9, 13],\n",
      "         [ 7,  7],\n",
      "         [ 7, 10],\n",
      "         [ 9, 18],\n",
      "         [ 2, 13],\n",
      "         [ 7,  1],\n",
      "         [10,  4],\n",
      "         [ 1,  7],\n",
      "         [ 7,  4],\n",
      "         [ 5, 17],\n",
      "         [ 0,  0],\n",
      "         [ 1, 17],\n",
      "         [ 3,  0],\n",
      "         [ 4,  3],\n",
      "         [ 4, 16],\n",
      "         [ 9, 15],\n",
      "         [ 7,  6],\n",
      "         [ 6,  3],\n",
      "         [ 1, 10],\n",
      "         [ 8,  5],\n",
      "         [ 5,  9],\n",
      "         [ 5, 16],\n",
      "         [ 9, 16],\n",
      "         [ 4, 10],\n",
      "         [ 3, 16],\n",
      "         [ 9, 17],\n",
      "         [ 5,  1],\n",
      "         [ 6, 15],\n",
      "         [ 9,  7],\n",
      "         [ 6, 12],\n",
      "         [ 6,  2],\n",
      "         [ 4, 17],\n",
      "         [ 1, 11],\n",
      "         [ 6,  5],\n",
      "         [ 3, 11],\n",
      "         [10, 11],\n",
      "         [ 1,  9],\n",
      "         [ 2,  8],\n",
      "         [ 7,  8],\n",
      "         [10,  0],\n",
      "         [ 3,  2],\n",
      "         [ 5, 10],\n",
      "         [ 0,  3],\n",
      "         [ 3, 10],\n",
      "         [ 4, 14],\n",
      "         [ 8,  9],\n",
      "         [ 2,  6],\n",
      "         [ 1, 12],\n",
      "         [ 1,  6],\n",
      "         [ 6,  8],\n",
      "         [ 7, 16],\n",
      "         [ 0,  6],\n",
      "         [ 7,  2],\n",
      "         [ 9, 10],\n",
      "         [ 8, 16],\n",
      "         [ 2,  1],\n",
      "         [ 1,  3],\n",
      "         [ 3,  1],\n",
      "         [ 3,  3],\n",
      "         [ 5, 18],\n",
      "         [ 4, 18],\n",
      "         [ 7, 18],\n",
      "         [ 8, 14],\n",
      "         [ 2, 15],\n",
      "         [ 5,  7],\n",
      "         [ 2,  0],\n",
      "         [ 9,  2],\n",
      "         [ 4,  1],\n",
      "         [ 6,  1],\n",
      "         [ 6, 11],\n",
      "         [ 9,  3],\n",
      "         [ 7, 12],\n",
      "         [ 2,  2],\n",
      "         [ 5,  8],\n",
      "         [ 5,  6],\n",
      "         [ 4, 13],\n",
      "         [ 1,  1],\n",
      "         [ 6,  6],\n",
      "         [10,  5],\n",
      "         [ 7, 17],\n",
      "         [10, 17],\n",
      "         [ 0,  5],\n",
      "         [ 1,  4],\n",
      "         [ 7,  9],\n",
      "         [ 6,  7],\n",
      "         [ 2,  3],\n",
      "         [ 2, 14],\n",
      "         [ 0, 17],\n",
      "         [ 2, 18],\n",
      "         [ 2, 11],\n",
      "         [ 0, 10],\n",
      "         [ 6, 14],\n",
      "         [ 4,  5],\n",
      "         [10, 15],\n",
      "         [ 0,  1],\n",
      "         [ 1,  2],\n",
      "         [ 8,  8],\n",
      "         [ 5, 15],\n",
      "         [ 3, 14],\n",
      "         [ 7,  0],\n",
      "         [ 6,  0],\n",
      "         [ 4,  4],\n",
      "         [ 8,  7],\n",
      "         [ 3,  5],\n",
      "         [ 3, 15],\n",
      "         [ 0,  2],\n",
      "         [ 1,  5],\n",
      "         [ 5,  3],\n",
      "         [ 1, 16],\n",
      "         [ 7,  3],\n",
      "         [ 7, 15],\n",
      "         [10,  7],\n",
      "         [10, 14],\n",
      "         [ 9, 14],\n",
      "         [ 4, 15],\n",
      "         [ 9,  9],\n",
      "         [ 5,  0],\n",
      "         [ 3,  4],\n",
      "         [ 7,  5]]]), (11, 19)), 'cls_output': tensor([[0.7252]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/164085027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 876\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.0705e-06, -1.7521e-07,  1.5456e-07, -2.1202e-08, -2.2476e-06,\n",
      "           3.2173e-06,  6.0811e-06, -5.3703e-09, -1.9807e-07, -7.9415e-07,\n",
      "          -1.5692e-07,  3.6335e-06, -3.8573e-08, -1.0093e-05, -1.3580e-08,\n",
      "          -1.8139e-07, -2.7724e-06,  9.1703e-09, -2.7452e-06, -5.7367e-09,\n",
      "           9.5353e-07,  3.9170e-09,  8.3201e-08, -1.0664e-08,  6.3288e-07,\n",
      "           3.1788e-06, -2.6123e-06, -1.0732e-09,  2.9125e-06,  4.4989e-06,\n",
      "          -9.3056e-06,  1.0643e-06, -4.5918e-07,  2.2646e-08, -9.7590e-07,\n",
      "           1.0921e-06,  5.4508e-08, -7.6128e-08, -2.9861e-09,  2.1609e-06,\n",
      "           7.3610e-08,  8.4742e-06,  3.4496e-08, -2.6085e-07, -4.6524e-06,\n",
      "           1.3929e-08, -5.0008e-07, -2.3619e-06, -2.0132e-06, -2.7828e-06,\n",
      "           1.0898e-07, -4.3159e-06, -3.1896e-08, -1.0233e-09, -2.3536e-06,\n",
      "          -1.7547e-09, -9.5498e-06,  2.6606e-06,  2.5922e-07, -5.3656e-08,\n",
      "           1.0648e-05, -2.6302e-07, -3.8063e-07,  1.8673e-08, -7.3548e-08,\n",
      "          -4.4497e-06,  5.8488e-10,  8.2012e-07, -1.0048e-08,  4.5198e-08,\n",
      "          -5.1587e-09, -4.5402e-07, -3.1521e-08,  7.7532e-07,  2.2411e-07,\n",
      "          -3.2163e-07,  2.4312e-09,  4.7526e-08, -1.8406e-09,  1.8258e-07,\n",
      "          -1.8963e-05,  3.5911e-10, -8.7442e-08, -2.1156e-06, -1.6889e-06,\n",
      "           1.6607e-08, -2.3158e-06,  1.8894e-07,  6.2720e-06, -3.0614e-07,\n",
      "           6.4272e-07,  8.4808e-10, -8.2978e-07,  3.7205e-06, -4.5556e-07,\n",
      "          -9.0431e-09, -2.6982e-08, -9.0076e-10, -1.1646e-08, -7.4008e-09,\n",
      "          -1.0095e-05,  4.5463e-07, -5.4923e-06,  4.2628e-06,  3.0881e-06,\n",
      "           4.9144e-06,  4.8263e-07,  1.9304e-05,  2.0120e-08, -1.8777e-08,\n",
      "           2.1247e-06,  2.9010e-06,  3.2299e-07, -7.7630e-08,  1.3161e-07,\n",
      "           5.7831e-06, -3.7161e-05, -2.9974e-07,  4.7228e-08, -3.7540e-07,\n",
      "          -5.0153e-06,  3.3485e-08, -4.8561e-06,  5.2390e-07,  3.9803e-06,\n",
      "           6.6331e-06,  1.0615e-06,  2.3658e-08,  1.9803e-07,  1.5534e-07,\n",
      "           1.2872e-05, -8.7163e-07, -2.0274e-07,  8.1007e-07,  1.9812e-05,\n",
      "           9.1543e-08, -1.8288e-07, -4.4274e-06, -8.2111e-06,  4.8516e-08,\n",
      "          -1.7234e-05,  3.3590e-06, -4.5906e-07,  3.0226e-06,  1.6611e-09,\n",
      "          -1.8436e-07,  7.9059e-07, -5.0428e-09,  2.3985e-07, -4.3570e-06,\n",
      "           2.7547e-06,  2.7828e-07, -2.0589e-07,  1.6140e-08, -8.7913e-06,\n",
      "          -4.1581e-06, -1.0303e-06, -4.2579e-06,  9.7744e-06,  3.5265e-08,\n",
      "           4.3416e-06,  3.5801e-07, -7.5727e-08,  1.3483e-06, -9.2882e-06,\n",
      "           1.5881e-06, -5.5673e-08,  1.8862e-10, -6.2908e-06,  1.1524e-07,\n",
      "           1.3282e-07,  1.1588e-08,  1.0572e-07, -1.8299e-10,  1.3464e-06,\n",
      "           7.6655e-07,  3.0214e-06,  4.7245e-07,  1.8281e-07, -8.3301e-09,\n",
      "          -5.3229e-08,  1.4018e-06, -3.9204e-07, -5.9981e-06,  1.8483e-06,\n",
      "          -1.2476e+01,  2.7466e-05,  1.7483e-07, -1.0044e-06,  1.1345e-06,\n",
      "           3.5873e-06,  1.6828e-08, -3.2386e-08,  1.3231e-06,  1.1067e-07,\n",
      "          -9.7024e-06, -3.4435e-06,  5.2040e-07,  9.8994e-09,  6.7235e-07,\n",
      "           1.1420e-07, -1.0131e-06, -1.9564e-06,  1.3763e-07,  2.0571e-09,\n",
      "           1.8330e-05,  2.3529e-09,  1.5280e-06, -1.3599e-05, -1.3387e-08,\n",
      "          -1.1265e-05,  6.5373e-06,  3.0832e-06,  2.0227e-09,  5.8327e-08,\n",
      "          -3.7830e-06, -2.6543e-07,  3.6740e-09, -1.6549e-09, -4.5770e-07,\n",
      "           1.5224e-09, -3.6159e-09,  8.5259e-07, -3.2185e-08,  1.1833e-05,\n",
      "          -3.8601e-08, -7.7616e-08, -1.2406e-07, -7.7444e-07,  3.5853e-07,\n",
      "          -1.1620e-06, -3.2386e-05,  5.1563e-08, -3.2455e-07, -1.0634e-08,\n",
      "          -4.7977e-06, -2.8738e-07,  2.0620e-06,  1.1597e-07,  1.9269e-07,\n",
      "           2.6304e-08, -1.5591e-06,  3.3739e-07,  6.7261e-09, -3.7009e-08,\n",
      "           2.0499e-05,  4.8093e-07,  6.9865e-08, -3.0791e-07,  2.4737e-09,\n",
      "           1.1568e-07, -1.1909e-08,  7.0830e-08, -3.8724e-07, -1.8492e-07,\n",
      "          -2.3770e-06, -4.9863e-08,  1.4903e-08,  1.7465e-07, -1.5729e-08,\n",
      "           1.3197e-06, -1.8862e-06, -3.4125e-07,  3.8081e-07,  4.1185e-09,\n",
      "           9.0387e-09, -7.8791e-07,  5.2688e-06,  1.0472e-06, -4.5152e-08,\n",
      "          -6.3991e-09, -2.5148e-05,  2.9618e-08, -3.2158e-08, -3.1126e-06,\n",
      "          -6.7059e-08, -1.1617e-05, -9.1126e-10, -1.2096e-06,  1.5687e-06,\n",
      "           1.2704e-05,  6.3805e-08,  2.4865e-07,  4.2421e-08, -1.3085e-06,\n",
      "           1.6238e-08,  1.1387e-07, -6.5243e-06, -3.7927e-09,  6.3239e-07,\n",
      "           2.1772e-07, -3.6238e-06,  7.8510e-06,  2.3451e-08, -1.6644e-08,\n",
      "          -6.7653e-09,  1.7376e-08,  4.2325e-06,  3.4382e-07, -7.9322e-07,\n",
      "           1.5175e-07, -2.0806e-06,  6.5979e-07, -1.1127e-06,  1.2323e-06,\n",
      "           8.6577e-07, -2.1816e-08, -2.4667e-07,  1.7908e-06, -2.7271e-06,\n",
      "          -8.2854e-07, -1.1972e-06,  5.9904e-06, -4.0014e-09,  1.3214e-06,\n",
      "          -3.6247e-09, -1.3826e-06,  1.5857e-07, -4.1422e-09,  1.3100e-06,\n",
      "          -1.9948e-07,  1.7546e-07, -1.6663e-07, -1.1876e-09,  1.2469e-05,\n",
      "           2.9894e-06, -6.9129e-08, -2.3283e-09,  4.1247e-07,  7.7483e-07,\n",
      "           9.1203e-08, -9.8638e-07, -6.5233e-07, -2.7579e-06,  6.2525e-07,\n",
      "           2.1035e-06, -2.5365e-07, -4.6151e-08, -8.3807e-07,  2.7435e-07,\n",
      "          -1.7637e-06, -2.1282e-05, -1.0119e-06,  5.4634e-07, -1.6888e-08,\n",
      "          -4.9618e-09,  7.0129e-06, -2.1212e-08, -3.2276e-08,  1.0739e-08,\n",
      "          -3.8581e-07,  2.0151e-06,  2.3192e-06,  2.5386e-06, -6.3311e-09,\n",
      "          -6.5924e-09,  1.6800e-05, -2.3732e-06,  1.1052e-08,  9.3492e-07,\n",
      "          -4.6413e-09, -4.5131e-09, -1.3413e-08, -1.7090e-08, -5.3563e-07,\n",
      "           3.9143e-08,  6.3290e-09, -3.9526e-06,  1.4251e-05,  1.4609e-09,\n",
      "          -4.7690e-06,  1.8617e-06, -2.3164e-06,  4.7677e-07,  5.5473e-07,\n",
      "          -6.5424e-07,  1.9043e-07,  2.8705e-09,  4.1889e-09,  3.1675e-08,\n",
      "          -5.6701e-07, -3.3418e-07, -4.8245e-06, -1.2208e-07, -1.2579e-05,\n",
      "          -1.0292e-08,  5.6189e-06, -3.3867e-07, -1.4220e-07, -1.1529e-06,\n",
      "           1.4955e-05, -9.9755e-06,  4.2098e-08, -4.7091e-07, -1.9709e-06,\n",
      "          -1.4910e-08,  6.7365e-08, -3.3627e-08,  1.7504e-06, -4.4959e-06,\n",
      "           7.6812e-06,  1.3357e-04, -1.0540e-07, -3.6329e-07,  2.1173e-07,\n",
      "           2.2562e-07,  1.1760e-07,  2.1037e-09, -1.2845e-08, -6.2611e-06,\n",
      "          -8.4922e-06, -6.6068e-09,  2.3698e-06,  1.0364e-06, -9.1273e-08,\n",
      "           7.9910e-07,  5.8195e-08,  3.8610e-07, -2.8259e-07, -2.8695e-06,\n",
      "           2.0003e-07,  8.7001e-09,  2.7057e-06,  9.6254e-06,  6.2056e-07,\n",
      "          -9.2256e-08,  4.0103e-06,  5.0245e-08, -3.0545e-09, -7.4120e-09,\n",
      "          -1.3020e-06, -5.8061e-07, -5.0467e-07,  2.0459e-06,  1.2453e-05,\n",
      "           2.8797e-09,  5.9096e-08, -1.3650e-07, -1.7231e-06,  5.6155e-08,\n",
      "          -7.2823e-06,  2.2833e-06,  1.0018e-06, -4.8581e-07, -2.4255e-08,\n",
      "          -1.1936e-06, -3.0576e-08, -1.3276e-07,  1.8906e-08, -3.4256e-07,\n",
      "          -4.7502e-07,  3.0308e-06, -1.9504e-06,  1.6687e-07, -1.4067e-07,\n",
      "          -3.2389e-06, -1.9622e-06,  1.2840e-06, -5.1929e-06, -5.7583e-09,\n",
      "           8.1122e-09, -2.4988e-07,  6.2490e-08, -7.1051e-06,  6.2890e-06,\n",
      "           1.5064e-06, -1.2966e-08, -6.0107e-07, -1.0128e-06,  2.2346e-06,\n",
      "          -9.7218e-09,  5.6699e-06, -2.6947e-06, -1.3278e-05, -1.7069e-08,\n",
      "           5.2235e-07, -9.7624e-09, -3.6445e-07, -7.0137e-07,  1.5170e-06,\n",
      "          -1.1140e-08, -8.9419e-08,  3.2134e-06, -3.2035e-08,  7.0367e-08,\n",
      "           3.3939e-08,  7.3384e-06,  6.1663e-07, -2.0497e-07, -3.0772e-08,\n",
      "          -1.4267e-06,  8.9570e-08, -4.0509e-06, -1.8876e-05,  2.0238e-07,\n",
      "           6.6535e-08,  2.4708e-07, -4.8657e-09, -3.6573e-07,  7.8544e-09,\n",
      "          -6.8982e-08,  4.8089e-07, -1.4776e-09, -2.3595e-08, -6.8129e-06,\n",
      "           6.9381e-10, -2.3210e-07, -1.1164e-06, -2.4618e-06,  2.6487e-09,\n",
      "           4.8181e-08, -9.3347e-06,  5.3015e-06,  3.2759e-07, -4.6297e-09,\n",
      "          -1.5417e-08, -6.7973e-10,  1.9806e-06,  2.1304e-08, -2.3451e-09,\n",
      "           9.6351e-06,  8.7539e-07,  6.9686e-08, -1.6368e-06, -7.8648e-06,\n",
      "           4.2370e-07, -9.8600e-08, -4.2517e-07,  4.9696e-07, -6.1395e-09,\n",
      "           5.2236e-09,  1.1263e-06,  3.4314e-09,  8.3416e-08, -2.0653e-08,\n",
      "           3.1180e-06,  4.5942e-06,  5.5429e-07, -1.1697e-08,  3.1334e-06,\n",
      "           3.5614e-07, -2.9946e-07,  2.4542e-06,  2.1006e-07,  3.4275e-07,\n",
      "           5.9653e-07, -6.7314e-08, -2.6987e-06, -1.2318e-07, -1.9951e-05,\n",
      "           4.1791e-09,  9.0749e-08,  1.4211e-07, -1.2994e-08,  7.7482e-09,\n",
      "           2.0325e-06,  3.2872e-08,  1.8770e-07, -3.6565e-08, -4.4565e-07,\n",
      "           5.4924e-09, -2.8814e-07, -1.0286e-05, -1.2779e-05, -3.6414e-09,\n",
      "           4.2905e-08,  9.2467e-08,  1.1076e-06,  6.8068e-07,  1.0772e-08,\n",
      "          -2.5271e-07, -1.2429e-08, -1.7459e-07,  3.1883e-06,  3.0819e-08,\n",
      "           8.3064e-07,  1.4967e-06,  3.5149e-07,  1.9145e-06,  1.0433e-05,\n",
      "          -4.0918e-06,  3.2103e-06, -1.2838e-09, -1.7808e-08, -9.6167e-06,\n",
      "           4.1772e-06,  9.4751e-08,  7.0124e-09, -5.8482e-08,  4.1038e-06,\n",
      "           9.3137e-08,  1.4279e-08, -8.8191e-09,  2.0127e-08, -2.0090e-07,\n",
      "           1.7485e-06,  5.9445e-09, -6.4444e-07, -5.8941e-07, -3.4347e-08,\n",
      "           1.4494e-06,  1.6853e-08, -5.9534e-08, -1.2081e-06, -1.1556e-07,\n",
      "          -2.2007e-06, -1.6601e-06,  2.1164e-07,  6.2411e-08,  1.6189e-06,\n",
      "          -1.4095e-05,  1.2285e-05,  1.9836e-09, -1.1830e-09,  1.4245e-07,\n",
      "          -4.2388e-09, -7.5148e-06, -5.9049e-06, -4.7331e-06,  1.6933e-08,\n",
      "          -4.7904e-08,  3.3373e-09, -1.2266e-08, -1.7535e-08, -9.2677e-07,\n",
      "          -1.2172e-06, -3.0298e-08, -1.1858e-07, -6.2225e-06,  7.0328e-08,\n",
      "           6.8054e-08, -7.1502e-07, -2.6513e-07,  2.7297e-06,  2.3259e-09,\n",
      "           7.3877e-08, -7.2392e-10,  3.1065e-07,  2.8778e-08,  4.6586e-09,\n",
      "           2.0189e-07, -1.7172e-09,  1.2696e-07,  2.4887e-09, -6.3959e-08,\n",
      "           1.3936e-07, -1.6669e-07,  4.6270e-07,  1.2864e-07, -5.1439e-06,\n",
      "           1.0699e-07, -1.8113e-06,  8.5400e-10,  2.5707e-07, -3.5609e-06,\n",
      "           1.6756e-05,  3.0093e-06, -4.4207e-07, -9.3632e-08, -1.4868e-05,\n",
      "           1.2654e-07, -1.3183e-05, -1.4881e-06,  2.0645e-05,  4.9586e-07,\n",
      "           6.1311e-08,  5.1878e-06,  1.2307e-05, -5.6192e-07, -6.8642e-07,\n",
      "           3.7236e-06,  5.0237e-09, -2.3857e-08, -7.7664e-08, -8.3865e-09,\n",
      "           1.5629e-06,  2.6984e-07,  5.2005e-08, -3.9755e-06, -1.7400e-06,\n",
      "          -1.7448e-07,  1.1338e-07, -1.3464e-06, -4.8910e-06,  1.4077e-07,\n",
      "          -4.3792e-07, -4.1272e-08,  7.2853e-08, -8.9580e-08, -4.6144e-09,\n",
      "          -7.4259e-07, -1.5908e-05,  3.6718e-06,  9.4396e-06,  5.6789e-08,\n",
      "           1.7059e-09,  2.5213e-06, -4.6034e-08,  5.2949e-08,  5.4982e-08,\n",
      "          -5.4109e-08,  2.1200e-06,  3.1344e-07,  9.6897e-08, -2.8529e-07,\n",
      "          -1.0593e-07, -1.7308e-06,  2.8259e-07, -3.0128e-06,  2.8530e-07,\n",
      "          -4.6122e-07, -6.8373e-06, -3.2311e-07, -2.2906e-08, -2.5482e-07,\n",
      "          -2.2110e-09, -1.1769e-06,  7.2977e-08, -5.7473e-07, -1.1251e-07,\n",
      "           1.0031e-07,  6.1135e-07, -1.1308e-08, -2.1041e-06,  2.4877e-08,\n",
      "           4.2130e-06, -6.9241e-09, -9.0283e-08, -7.0564e-08,  3.2511e-09,\n",
      "           2.1240e-06,  2.7619e-07, -2.1908e-06,  5.6387e-08,  2.2908e-08,\n",
      "          -5.2895e-09,  5.7410e-06, -2.2186e-07,  8.8366e-09, -2.1829e-06,\n",
      "          -1.5606e-07, -7.4005e-09,  1.4464e-07,  6.0941e-08,  1.8124e-07,\n",
      "          -5.9934e-06, -4.1108e-08, -9.2321e-06, -1.2959e-07, -2.9012e-06,\n",
      "           4.0460e-06,  4.3070e-06,  1.2612e-07,  5.2592e-08, -2.4102e-05,\n",
      "           1.0350e-07, -1.0870e-08, -5.3403e-07,  2.3729e-08, -6.2486e-06,\n",
      "           6.4484e-07, -2.0996e-05,  2.8569e-06,  6.8823e-08,  4.8678e-07,\n",
      "          -8.1302e-07,  3.0096e-07, -5.7045e-07]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1839e-07, -3.4249e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9451e-07,\n",
      "           3.1838e-07, -3.4252e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9452e-07,\n",
      "           3.1838e-07, -3.4245e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9452e-07,\n",
      "           3.1839e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7536e-07,  1.5568e-07,  ..., -7.9445e-07,\n",
      "           3.1837e-07, -3.4245e-06]]], device='cuda:0'), 'cls_feats': tensor([[-2.5329e-01,  2.7298e-01, -2.7280e-01, -1.6352e-01, -2.5890e-01,\n",
      "         -2.7271e-01, -2.7103e-01, -2.7263e-01,  2.7364e-01,  2.4064e-01,\n",
      "          2.7141e-01,  2.7301e-01,  2.6473e-01, -4.2325e-06,  4.4231e-07,\n",
      "          1.5733e-01,  2.6908e-01, -2.5862e-01,  2.7277e-01, -2.7285e-01,\n",
      "          2.6162e-01,  2.7369e-01,  2.7239e-01, -2.7115e-01,  2.5504e-01,\n",
      "          2.7245e-01, -2.7290e-01,  2.7163e-01,  2.7095e-01,  2.7171e-01,\n",
      "         -2.7310e-01,  1.1716e-01,  1.7293e-01, -6.8875e-03,  2.7160e-01,\n",
      "          2.7121e-01, -2.6871e-01, -2.7221e-01, -2.6997e-01,  4.9694e-08,\n",
      "          9.2227e-03,  2.6083e-01, -2.7138e-01, -2.6946e-01,  2.7263e-01,\n",
      "         -2.7328e-01, -2.7197e-01, -2.7241e-01, -2.5311e-01,  2.7223e-01,\n",
      "          2.6809e-01, -2.7347e-01, -2.7346e-01, -2.7066e-01, -2.7207e-01,\n",
      "          2.5937e-01,  2.7125e-01,  2.7321e-01,  2.7196e-01, -2.5437e-01,\n",
      "         -2.7341e-01,  3.1951e-07,  2.7169e-01, -2.6424e-01, -2.7358e-01,\n",
      "          1.6841e-02,  1.5248e-01, -2.6188e-01,  1.5418e-01,  2.7122e-01,\n",
      "          2.7234e-01, -2.0134e-08,  2.7261e-01,  2.5505e-01, -1.6933e-01,\n",
      "         -2.7271e-01, -2.7113e-01, -2.7173e-01,  2.7162e-01, -2.7094e-01,\n",
      "          2.7282e-01,  2.7289e-01, -1.2384e-02,  2.7327e-01,  2.5079e-01,\n",
      "         -5.2214e-09,  2.7274e-01,  2.5253e-01, -2.7112e-01,  2.6295e-01,\n",
      "          7.2820e-02,  1.5179e-01, -2.6720e-01,  2.6206e-01, -2.5522e-01,\n",
      "          2.5453e-01,  2.7262e-01,  2.7172e-01, -2.5858e-01, -2.7368e-01,\n",
      "         -1.7650e-01, -2.7364e-01, -2.7761e-06, -2.7050e-01, -2.7242e-01,\n",
      "         -9.5408e-07,  2.8162e-07, -6.6777e-08,  2.7260e-01, -2.7262e-01,\n",
      "         -3.2197e-02, -2.7090e-01, -2.7245e-01,  2.5116e-01,  5.3597e-03,\n",
      "          2.6201e-01, -1.3350e-03, -9.9751e-02, -2.7307e-01,  2.6879e-06,\n",
      "          2.6327e-01, -2.5784e-01,  7.6070e-05,  2.6247e-01,  2.7262e-01,\n",
      "         -2.2627e-09,  2.4026e-01,  2.7228e-01,  2.7261e-01, -2.6966e-01,\n",
      "          9.4408e-07, -2.7287e-01, -3.0215e-06, -2.5881e-01, -4.4230e-08,\n",
      "         -2.6581e-01,  2.6156e-01,  2.7199e-01,  1.0778e-01,  2.7290e-01,\n",
      "          3.6665e-10, -6.5719e-03,  3.9806e-03, -2.6437e-01,  2.6979e-01,\n",
      "          5.7351e-03,  2.6954e-01,  2.7312e-01, -2.5813e-01,  2.6934e-01,\n",
      "         -2.7057e-01,  2.7349e-01,  2.7225e-01,  9.3279e-09, -2.7260e-01,\n",
      "         -3.3164e-05, -2.5894e-01, -2.7316e-01, -2.6113e-01, -2.7235e-01,\n",
      "          1.2625e-01, -2.7337e-01,  2.3563e-01, -2.5498e-01, -2.6821e-01,\n",
      "          2.6304e-01,  2.7362e-01, -2.5284e-01,  2.7051e-01, -2.7178e-01,\n",
      "          1.6742e-07, -2.3239e-01, -2.7683e-02, -2.6194e-01, -2.7192e-01,\n",
      "          2.4778e-07,  2.7302e-01, -2.7223e-01,  1.2483e-08,  2.5061e-01,\n",
      "         -9.8509e-02,  2.7330e-01,  2.7125e-01, -2.7161e-01,  2.7224e-01,\n",
      "         -2.6130e-01,  2.2664e-01,  1.0688e-10, -2.6499e-01,  2.7301e-01,\n",
      "          2.7192e-01,  2.6404e-01, -2.6144e-01,  2.7179e-01, -2.5356e-01,\n",
      "          2.7134e-01,  2.5808e-01,  2.7103e-01,  2.6942e-01,  2.7246e-01,\n",
      "         -2.7130e-01, -2.7259e-01,  2.7295e-01,  2.7233e-01,  3.3845e-07,\n",
      "         -2.0712e-01, -2.6847e-01,  2.7179e-01,  2.6877e-01, -2.6997e-01,\n",
      "         -2.6171e-01, -2.7193e-01,  2.0173e-01, -2.7287e-01,  2.6270e-04,\n",
      "         -2.8069e-06, -2.7149e-01,  2.6112e-01, -2.7188e-01,  2.5542e-01,\n",
      "          3.3272e-05,  1.4977e-01,  2.7326e-01, -2.5716e-01,  1.5103e-01,\n",
      "          2.6368e-01,  2.7272e-01,  2.7164e-01,  2.7301e-01,  2.7327e-01,\n",
      "          2.6694e-01, -3.0225e-08,  2.5672e-01, -2.7213e-01, -2.7201e-01,\n",
      "          2.7264e-01, -1.2435e-02, -2.6663e-01,  2.6263e-01,  2.7366e-01,\n",
      "          2.6474e-01,  2.6283e-01,  2.6809e-01,  2.7153e-01, -1.1546e-01,\n",
      "          2.7087e-01, -1.0964e-02,  2.6851e-01, -1.4769e-01,  2.7090e-01,\n",
      "          2.7092e-01,  2.7378e-01,  2.4240e-03,  2.5407e-01,  2.5658e-01,\n",
      "         -2.7265e-01, -2.5323e-03, -2.0187e-04, -2.7371e-01,  2.7115e-01,\n",
      "         -2.7124e-01, -2.7291e-01,  2.4970e-01,  5.9815e-07, -2.7289e-01,\n",
      "         -2.4732e-01,  2.7261e-01,  2.6020e-01,  2.6386e-01,  2.6247e-01,\n",
      "          2.6751e-08, -2.7202e-01,  2.3406e-01, -2.7045e-01,  2.7338e-01,\n",
      "         -2.6875e-01,  2.5824e-01,  2.7291e-01,  2.7290e-01,  2.4673e-01,\n",
      "         -2.7322e-01, -2.6165e-01,  2.6366e-01,  2.7266e-01,  3.6768e-05,\n",
      "         -2.7326e-01, -2.7268e-02, -2.6397e-01,  1.1433e-01, -2.6522e-01,\n",
      "         -2.6601e-01,  1.3108e-01, -2.7206e-01, -2.7207e-01, -1.2430e-05,\n",
      "         -2.7117e-01, -2.7116e-01, -2.5257e-01, -2.6989e-01,  2.5703e-01,\n",
      "         -2.7190e-01, -2.7168e-01,  2.7014e-01,  2.6280e-01, -2.7317e-01,\n",
      "          2.7230e-01,  7.2003e-07, -3.0523e-04,  2.7236e-01,  2.9854e-06,\n",
      "          2.1700e-01,  2.4211e-01, -1.9442e-06, -2.6873e-01,  2.7277e-01,\n",
      "         -2.7360e-01,  2.7330e-01,  2.7135e-01, -2.6965e-01, -2.2647e-02,\n",
      "          1.6124e-07,  2.7255e-01,  2.7328e-01, -2.7367e-01,  2.5577e-01,\n",
      "         -2.7231e-01, -2.6937e-01, -2.7147e-01, -1.1090e-01, -2.6133e-01,\n",
      "         -2.7257e-01, -2.6162e-01,  2.6645e-01,  2.2533e-07, -2.7141e-01,\n",
      "          2.7209e-01,  3.8620e-04,  1.6536e-02, -2.6058e-01,  2.6870e-01,\n",
      "         -2.6980e-01,  3.1584e-07, -2.7198e-01, -1.5198e-01, -2.7295e-01,\n",
      "         -2.7343e-01,  2.5247e-01,  2.5445e-01,  1.5226e-01,  2.7351e-01,\n",
      "          2.7271e-01,  2.6702e-01,  1.5340e-01,  2.7023e-01, -2.5675e-01,\n",
      "         -2.7217e-01,  2.6960e-01,  2.5103e-01, -2.7228e-01, -1.0551e-01,\n",
      "         -2.4441e-01,  2.7025e-01, -2.6249e-01,  2.7232e-01,  8.4238e-08,\n",
      "         -2.7359e-01, -2.7176e-01, -2.7291e-01, -2.7263e-01, -2.7089e-01,\n",
      "          2.3009e-02, -2.5393e-01, -2.7267e-01,  2.7359e-01,  2.7277e-01,\n",
      "          2.6673e-01, -2.7256e-01, -2.7292e-01,  2.5438e-01,  2.4872e-01,\n",
      "         -2.6349e-02,  2.7306e-01,  2.7272e-01, -2.7240e-01,  2.6908e-01,\n",
      "         -2.5230e-01,  2.7099e-01,  3.2165e-06,  2.2942e-03,  1.9737e-02,\n",
      "         -2.7261e-01,  2.5198e-01,  2.6387e-01,  2.7236e-01,  2.7049e-01,\n",
      "         -8.5241e-03, -2.7017e-01, -2.6183e-01,  2.7123e-01,  2.7288e-01,\n",
      "          1.3550e-01,  1.6019e-01,  2.7247e-01,  2.7167e-01, -2.7295e-01,\n",
      "          2.7165e-01, -2.6089e-01,  2.4908e-01, -2.7323e-01, -2.6206e-01,\n",
      "         -2.7173e-01,  2.7181e-01, -2.6966e-01,  1.2801e-06, -2.7334e-01,\n",
      "         -2.6206e-01, -2.7279e-01,  2.7226e-01,  2.7166e-01, -2.7324e-01,\n",
      "          1.2125e-08, -2.6470e-01, -2.7231e-01, -9.4538e-04, -1.6635e-01,\n",
      "         -1.3581e-01,  2.6416e-01, -1.6150e-01, -2.7024e-01, -2.7223e-01,\n",
      "         -2.3759e-01,  2.7153e-01,  2.7048e-01,  2.7280e-01,  2.5597e-01,\n",
      "          2.7314e-01, -1.6542e-01,  2.7044e-01,  2.4861e-01,  3.6537e-04,\n",
      "          2.7301e-01, -2.6170e-01, -2.7230e-01, -5.2143e-08, -2.7126e-01,\n",
      "          2.7298e-01, -1.6175e-08, -2.7138e-01,  2.7295e-01,  2.6272e-01,\n",
      "         -2.0160e-06,  2.7323e-01, -1.2852e-04,  2.7220e-01,  2.7177e-01,\n",
      "          2.7006e-01,  7.5270e-04,  2.7072e-01,  2.7324e-01, -2.2270e-01,\n",
      "          2.7274e-01, -2.7306e-01, -4.1577e-07, -2.7281e-01, -1.0313e-04,\n",
      "         -2.5432e-01, -2.7204e-01,  2.6971e-01, -2.7316e-01, -2.7034e-01,\n",
      "         -2.7253e-01,  4.1148e-07, -2.7264e-01,  2.7142e-01, -2.2444e-01,\n",
      "         -2.2670e-01,  6.9491e-07,  2.7069e-01, -2.7274e-01, -2.5658e-01,\n",
      "          2.6244e-01,  2.6912e-01, -2.6680e-01, -2.5361e-02,  2.7273e-01,\n",
      "         -2.7295e-01,  2.6339e-01,  2.4183e-01, -2.7272e-01,  2.3183e-01,\n",
      "          2.6350e-01,  5.8954e-03, -2.7357e-01, -2.6848e-01,  1.4947e-01,\n",
      "          2.7316e-01, -2.6478e-01,  2.6547e-01,  2.6885e-01, -2.7347e-01,\n",
      "         -2.7233e-01,  2.1855e-06, -3.2388e-02, -2.7295e-01,  2.5127e-01,\n",
      "         -2.8110e-02,  2.7183e-01,  2.4543e-01,  1.3275e-01,  2.7332e-01,\n",
      "         -2.6173e-01, -4.4202e-03,  2.2200e-01,  2.6853e-01, -4.4395e-04,\n",
      "         -2.7194e-01,  2.6979e-02,  2.5757e-01,  2.2338e-01, -2.7329e-01,\n",
      "          2.6293e-01,  2.4051e-01,  2.6220e-01, -2.7191e-01,  2.7227e-01,\n",
      "         -2.5946e-01,  3.5099e-02,  2.6400e-01, -2.7336e-01, -2.7260e-01,\n",
      "         -2.7285e-01,  5.2348e-04,  2.7274e-01, -2.7351e-01,  2.7193e-01,\n",
      "          1.8870e-02, -2.0834e-01,  2.5499e-01, -2.7214e-01,  6.2968e-08,\n",
      "          2.7236e-01,  2.5419e-01, -2.7341e-01,  1.5647e-04, -2.7237e-01,\n",
      "          1.3710e-03,  2.4738e-01,  2.7279e-01,  2.6177e-01, -2.7283e-01,\n",
      "          4.4240e-03,  2.6228e-01, -2.5398e-01, -2.6876e-01, -2.6492e-01,\n",
      "         -2.7282e-01,  1.0813e-08,  4.0760e-06, -2.7280e-01, -2.7259e-01,\n",
      "          2.5444e-01,  2.6198e-01,  2.7152e-01, -2.6067e-01,  2.7049e-01,\n",
      "         -2.4644e-01,  2.7136e-01, -2.7154e-01, -2.6196e-01, -2.7146e-01,\n",
      "          2.7064e-01, -2.7271e-01, -2.7218e-01, -2.7300e-01,  2.7229e-01,\n",
      "          2.6744e-01,  9.3937e-06, -1.2705e-02,  2.6767e-01,  2.6920e-01,\n",
      "          2.7285e-01, -2.7282e-01, -6.2177e-08, -2.6961e-01,  2.4165e-01,\n",
      "         -2.6665e-01,  1.3670e-03,  2.7087e-01, -2.6168e-01, -2.7303e-01,\n",
      "         -2.7342e-01,  2.5364e-01, -1.3879e-01, -2.7343e-01,  2.2210e-06,\n",
      "         -2.7116e-01, -2.6262e-01,  2.6627e-01,  2.7142e-01, -2.7356e-01,\n",
      "         -2.6524e-01,  2.7126e-01, -2.6414e-01,  2.6202e-01,  2.5281e-01,\n",
      "          2.7308e-01, -4.2828e-08, -2.2151e-07,  2.7244e-01,  2.6131e-01,\n",
      "         -2.7234e-01, -2.7266e-01, -2.7312e-01, -2.5223e-01, -2.6841e-01,\n",
      "          2.7149e-01, -1.4834e-08,  2.2038e-05,  2.7381e-01, -2.7356e-01,\n",
      "          2.7182e-01, -2.6702e-01,  2.7306e-01, -2.7175e-01, -2.7184e-01,\n",
      "         -2.7141e-01,  5.9924e-03, -1.0006e-02, -1.4745e-04, -2.5374e-01,\n",
      "          2.7147e-01,  8.8581e-06,  2.5426e-01, -1.9334e-03,  2.7121e-01,\n",
      "         -1.7211e-01,  2.4070e-02, -2.7326e-01, -2.7158e-01, -2.7188e-01,\n",
      "         -1.6470e-01,  1.1724e-08,  2.7160e-01, -3.1781e-07,  2.9881e-02,\n",
      "         -2.6133e-01,  2.7309e-01,  2.7135e-01, -1.7179e-05,  2.7253e-01,\n",
      "         -2.7346e-01, -2.5500e-01, -2.7213e-01, -2.7293e-01,  1.1335e-02,\n",
      "          2.7219e-01, -2.7200e-01, -2.3388e-01,  2.7184e-01,  2.7010e-01,\n",
      "         -2.4659e-01,  1.1877e-04,  2.6927e-01, -2.6908e-01,  2.7006e-01,\n",
      "         -2.6020e-01, -1.9822e-04,  1.0538e-01, -4.8812e-03,  2.7079e-01,\n",
      "         -2.7316e-01, -2.7206e-01, -2.7165e-01, -2.7226e-01, -4.5433e-04,\n",
      "          2.6929e-01, -2.6833e-01, -2.5352e-01, -2.7220e-01,  2.7324e-01,\n",
      "          2.7257e-01,  2.6701e-01,  2.7214e-01, -2.7124e-01,  2.7099e-01,\n",
      "         -2.7240e-01, -2.5856e-01,  2.6703e-01, -1.4720e-05,  8.5388e-09,\n",
      "          2.2639e-04,  2.7091e-01,  2.7100e-01, -2.7262e-01, -1.6317e-01,\n",
      "          2.6928e-01, -2.7154e-01, -2.7302e-01, -1.6911e-01, -2.7054e-01,\n",
      "          4.3386e-07, -2.7071e-01, -2.7054e-01,  1.3794e-01,  2.5327e-01,\n",
      "          2.7136e-01,  2.7082e-01,  2.6493e-01, -8.8703e-07, -2.7009e-01,\n",
      "          2.2928e-01,  2.6872e-01,  2.7208e-01, -2.7159e-01, -1.4761e-06,\n",
      "         -2.5449e-01, -2.7302e-01, -2.7221e-01,  2.6903e-01, -2.7323e-01,\n",
      "         -2.7337e-01, -2.5867e-01,  2.6244e-01,  2.6236e-01,  2.7024e-01,\n",
      "         -2.5688e-01, -2.7190e-01, -2.7045e-01,  1.2533e-01,  2.7131e-01,\n",
      "         -1.9350e-05,  2.7349e-01, -4.5170e-06,  2.5484e-01, -2.7258e-01,\n",
      "          2.7195e-01, -2.7275e-01,  2.3883e-01,  2.6131e-01, -2.6139e-01,\n",
      "          2.6701e-01,  2.5422e-01, -2.7042e-01,  2.5555e-01,  2.1375e-02,\n",
      "         -2.7165e-01,  2.7207e-01, -1.5697e-02,  2.7171e-01, -2.4122e-02,\n",
      "         -2.7250e-01, -2.4917e-01, -2.7290e-01, -2.7202e-01, -2.6841e-02,\n",
      "          2.6774e-01,  2.7053e-01, -2.5754e-04,  2.6161e-01,  2.7343e-01,\n",
      "          2.7171e-01,  2.7215e-01, -2.7246e-01, -2.7303e-01, -1.1138e-02,\n",
      "         -9.8367e-09,  2.7236e-01, -2.7244e-01]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0705e-06, -1.7521e-07,  1.5456e-07, -2.1202e-08, -2.2476e-06,\n",
      "          3.2173e-06,  6.0811e-06, -5.3703e-09, -1.9807e-07, -7.9415e-07,\n",
      "         -1.5692e-07,  3.6335e-06, -3.8573e-08, -1.0093e-05, -1.3580e-08,\n",
      "         -1.8139e-07, -2.7724e-06,  9.1703e-09, -2.7452e-06, -5.7367e-09,\n",
      "          9.5353e-07,  3.9170e-09,  8.3201e-08, -1.0664e-08,  6.3288e-07,\n",
      "          3.1788e-06, -2.6123e-06, -1.0732e-09,  2.9125e-06,  4.4989e-06,\n",
      "         -9.3056e-06,  1.0643e-06, -4.5918e-07,  2.2646e-08, -9.7590e-07,\n",
      "          1.0921e-06,  5.4508e-08, -7.6128e-08, -2.9861e-09,  2.1609e-06,\n",
      "          7.3610e-08,  8.4742e-06,  3.4496e-08, -2.6085e-07, -4.6524e-06,\n",
      "          1.3929e-08, -5.0008e-07, -2.3619e-06, -2.0132e-06, -2.7828e-06,\n",
      "          1.0898e-07, -4.3159e-06, -3.1896e-08, -1.0233e-09, -2.3536e-06,\n",
      "         -1.7547e-09, -9.5498e-06,  2.6606e-06,  2.5922e-07, -5.3656e-08,\n",
      "          1.0648e-05, -2.6302e-07, -3.8063e-07,  1.8673e-08, -7.3548e-08,\n",
      "         -4.4497e-06,  5.8488e-10,  8.2012e-07, -1.0048e-08,  4.5198e-08,\n",
      "         -5.1587e-09, -4.5402e-07, -3.1521e-08,  7.7532e-07,  2.2411e-07,\n",
      "         -3.2163e-07,  2.4312e-09,  4.7526e-08, -1.8406e-09,  1.8258e-07,\n",
      "         -1.8963e-05,  3.5911e-10, -8.7442e-08, -2.1156e-06, -1.6889e-06,\n",
      "          1.6607e-08, -2.3158e-06,  1.8894e-07,  6.2720e-06, -3.0614e-07,\n",
      "          6.4272e-07,  8.4808e-10, -8.2978e-07,  3.7205e-06, -4.5556e-07,\n",
      "         -9.0431e-09, -2.6982e-08, -9.0076e-10, -1.1646e-08, -7.4008e-09,\n",
      "         -1.0095e-05,  4.5463e-07, -5.4923e-06,  4.2628e-06,  3.0881e-06,\n",
      "          4.9144e-06,  4.8263e-07,  1.9304e-05,  2.0120e-08, -1.8777e-08,\n",
      "          2.1247e-06,  2.9010e-06,  3.2299e-07, -7.7630e-08,  1.3161e-07,\n",
      "          5.7831e-06, -3.7161e-05, -2.9974e-07,  4.7228e-08, -3.7540e-07,\n",
      "         -5.0153e-06,  3.3485e-08, -4.8561e-06,  5.2390e-07,  3.9803e-06,\n",
      "          6.6331e-06,  1.0615e-06,  2.3658e-08,  1.9803e-07,  1.5534e-07,\n",
      "          1.2872e-05, -8.7163e-07, -2.0274e-07,  8.1007e-07,  1.9812e-05,\n",
      "          9.1543e-08, -1.8288e-07, -4.4274e-06, -8.2111e-06,  4.8516e-08,\n",
      "         -1.7234e-05,  3.3590e-06, -4.5906e-07,  3.0226e-06,  1.6611e-09,\n",
      "         -1.8436e-07,  7.9059e-07, -5.0428e-09,  2.3985e-07, -4.3570e-06,\n",
      "          2.7547e-06,  2.7828e-07, -2.0589e-07,  1.6140e-08, -8.7913e-06,\n",
      "         -4.1581e-06, -1.0303e-06, -4.2579e-06,  9.7744e-06,  3.5265e-08,\n",
      "          4.3416e-06,  3.5801e-07, -7.5727e-08,  1.3483e-06, -9.2882e-06,\n",
      "          1.5881e-06, -5.5673e-08,  1.8862e-10, -6.2908e-06,  1.1524e-07,\n",
      "          1.3282e-07,  1.1588e-08,  1.0572e-07, -1.8299e-10,  1.3464e-06,\n",
      "          7.6655e-07,  3.0214e-06,  4.7245e-07,  1.8281e-07, -8.3301e-09,\n",
      "         -5.3229e-08,  1.4018e-06, -3.9204e-07, -5.9981e-06,  1.8483e-06,\n",
      "         -1.2476e+01,  2.7466e-05,  1.7483e-07, -1.0044e-06,  1.1345e-06,\n",
      "          3.5873e-06,  1.6828e-08, -3.2386e-08,  1.3231e-06,  1.1067e-07,\n",
      "         -9.7024e-06, -3.4435e-06,  5.2040e-07,  9.8994e-09,  6.7235e-07,\n",
      "          1.1420e-07, -1.0131e-06, -1.9564e-06,  1.3763e-07,  2.0571e-09,\n",
      "          1.8330e-05,  2.3529e-09,  1.5280e-06, -1.3599e-05, -1.3387e-08,\n",
      "         -1.1265e-05,  6.5373e-06,  3.0832e-06,  2.0227e-09,  5.8327e-08,\n",
      "         -3.7830e-06, -2.6543e-07,  3.6740e-09, -1.6549e-09, -4.5770e-07,\n",
      "          1.5224e-09, -3.6159e-09,  8.5259e-07, -3.2185e-08,  1.1833e-05,\n",
      "         -3.8601e-08, -7.7616e-08, -1.2406e-07, -7.7444e-07,  3.5853e-07,\n",
      "         -1.1620e-06, -3.2386e-05,  5.1563e-08, -3.2455e-07, -1.0634e-08,\n",
      "         -4.7977e-06, -2.8738e-07,  2.0620e-06,  1.1597e-07,  1.9269e-07,\n",
      "          2.6304e-08, -1.5591e-06,  3.3739e-07,  6.7261e-09, -3.7009e-08,\n",
      "          2.0499e-05,  4.8093e-07,  6.9865e-08, -3.0791e-07,  2.4737e-09,\n",
      "          1.1568e-07, -1.1909e-08,  7.0830e-08, -3.8724e-07, -1.8492e-07,\n",
      "         -2.3770e-06, -4.9863e-08,  1.4903e-08,  1.7465e-07, -1.5729e-08,\n",
      "          1.3197e-06, -1.8862e-06, -3.4125e-07,  3.8081e-07,  4.1185e-09,\n",
      "          9.0387e-09, -7.8791e-07,  5.2688e-06,  1.0472e-06, -4.5152e-08,\n",
      "         -6.3991e-09, -2.5148e-05,  2.9618e-08, -3.2158e-08, -3.1126e-06,\n",
      "         -6.7059e-08, -1.1617e-05, -9.1126e-10, -1.2096e-06,  1.5687e-06,\n",
      "          1.2704e-05,  6.3805e-08,  2.4865e-07,  4.2421e-08, -1.3085e-06,\n",
      "          1.6238e-08,  1.1387e-07, -6.5243e-06, -3.7927e-09,  6.3239e-07,\n",
      "          2.1772e-07, -3.6238e-06,  7.8510e-06,  2.3451e-08, -1.6644e-08,\n",
      "         -6.7653e-09,  1.7376e-08,  4.2325e-06,  3.4382e-07, -7.9322e-07,\n",
      "          1.5175e-07, -2.0806e-06,  6.5979e-07, -1.1127e-06,  1.2323e-06,\n",
      "          8.6577e-07, -2.1816e-08, -2.4667e-07,  1.7908e-06, -2.7271e-06,\n",
      "         -8.2854e-07, -1.1972e-06,  5.9904e-06, -4.0014e-09,  1.3214e-06,\n",
      "         -3.6247e-09, -1.3826e-06,  1.5857e-07, -4.1422e-09,  1.3100e-06,\n",
      "         -1.9948e-07,  1.7546e-07, -1.6663e-07, -1.1876e-09,  1.2469e-05,\n",
      "          2.9894e-06, -6.9129e-08, -2.3283e-09,  4.1247e-07,  7.7483e-07,\n",
      "          9.1203e-08, -9.8638e-07, -6.5233e-07, -2.7579e-06,  6.2525e-07,\n",
      "          2.1035e-06, -2.5365e-07, -4.6151e-08, -8.3807e-07,  2.7435e-07,\n",
      "         -1.7637e-06, -2.1282e-05, -1.0119e-06,  5.4634e-07, -1.6888e-08,\n",
      "         -4.9618e-09,  7.0129e-06, -2.1212e-08, -3.2276e-08,  1.0739e-08,\n",
      "         -3.8581e-07,  2.0151e-06,  2.3192e-06,  2.5386e-06, -6.3311e-09,\n",
      "         -6.5924e-09,  1.6800e-05, -2.3732e-06,  1.1052e-08,  9.3492e-07,\n",
      "         -4.6413e-09, -4.5131e-09, -1.3413e-08, -1.7090e-08, -5.3563e-07,\n",
      "          3.9143e-08,  6.3290e-09, -3.9526e-06,  1.4251e-05,  1.4609e-09,\n",
      "         -4.7690e-06,  1.8617e-06, -2.3164e-06,  4.7677e-07,  5.5473e-07,\n",
      "         -6.5424e-07,  1.9043e-07,  2.8705e-09,  4.1889e-09,  3.1675e-08,\n",
      "         -5.6701e-07, -3.3418e-07, -4.8245e-06, -1.2208e-07, -1.2579e-05,\n",
      "         -1.0292e-08,  5.6189e-06, -3.3867e-07, -1.4220e-07, -1.1529e-06,\n",
      "          1.4955e-05, -9.9755e-06,  4.2098e-08, -4.7091e-07, -1.9709e-06,\n",
      "         -1.4910e-08,  6.7365e-08, -3.3627e-08,  1.7504e-06, -4.4959e-06,\n",
      "          7.6812e-06,  1.3357e-04, -1.0540e-07, -3.6329e-07,  2.1173e-07,\n",
      "          2.2562e-07,  1.1760e-07,  2.1037e-09, -1.2845e-08, -6.2611e-06,\n",
      "         -8.4922e-06, -6.6068e-09,  2.3698e-06,  1.0364e-06, -9.1273e-08,\n",
      "          7.9910e-07,  5.8195e-08,  3.8610e-07, -2.8259e-07, -2.8695e-06,\n",
      "          2.0003e-07,  8.7001e-09,  2.7057e-06,  9.6254e-06,  6.2056e-07,\n",
      "         -9.2256e-08,  4.0103e-06,  5.0245e-08, -3.0545e-09, -7.4120e-09,\n",
      "         -1.3020e-06, -5.8061e-07, -5.0467e-07,  2.0459e-06,  1.2453e-05,\n",
      "          2.8797e-09,  5.9096e-08, -1.3650e-07, -1.7231e-06,  5.6155e-08,\n",
      "         -7.2823e-06,  2.2833e-06,  1.0018e-06, -4.8581e-07, -2.4255e-08,\n",
      "         -1.1936e-06, -3.0576e-08, -1.3276e-07,  1.8906e-08, -3.4256e-07,\n",
      "         -4.7502e-07,  3.0308e-06, -1.9504e-06,  1.6687e-07, -1.4067e-07,\n",
      "         -3.2389e-06, -1.9622e-06,  1.2840e-06, -5.1929e-06, -5.7583e-09,\n",
      "          8.1122e-09, -2.4988e-07,  6.2490e-08, -7.1051e-06,  6.2890e-06,\n",
      "          1.5064e-06, -1.2966e-08, -6.0107e-07, -1.0128e-06,  2.2346e-06,\n",
      "         -9.7218e-09,  5.6699e-06, -2.6947e-06, -1.3278e-05, -1.7069e-08,\n",
      "          5.2235e-07, -9.7624e-09, -3.6445e-07, -7.0137e-07,  1.5170e-06,\n",
      "         -1.1140e-08, -8.9419e-08,  3.2134e-06, -3.2035e-08,  7.0367e-08,\n",
      "          3.3939e-08,  7.3384e-06,  6.1663e-07, -2.0497e-07, -3.0772e-08,\n",
      "         -1.4267e-06,  8.9570e-08, -4.0509e-06, -1.8876e-05,  2.0238e-07,\n",
      "          6.6535e-08,  2.4708e-07, -4.8657e-09, -3.6573e-07,  7.8544e-09,\n",
      "         -6.8982e-08,  4.8089e-07, -1.4776e-09, -2.3595e-08, -6.8129e-06,\n",
      "          6.9381e-10, -2.3210e-07, -1.1164e-06, -2.4618e-06,  2.6487e-09,\n",
      "          4.8181e-08, -9.3347e-06,  5.3015e-06,  3.2759e-07, -4.6297e-09,\n",
      "         -1.5417e-08, -6.7973e-10,  1.9806e-06,  2.1304e-08, -2.3451e-09,\n",
      "          9.6351e-06,  8.7539e-07,  6.9686e-08, -1.6368e-06, -7.8648e-06,\n",
      "          4.2370e-07, -9.8600e-08, -4.2517e-07,  4.9696e-07, -6.1395e-09,\n",
      "          5.2236e-09,  1.1263e-06,  3.4314e-09,  8.3416e-08, -2.0653e-08,\n",
      "          3.1180e-06,  4.5942e-06,  5.5429e-07, -1.1697e-08,  3.1334e-06,\n",
      "          3.5614e-07, -2.9946e-07,  2.4542e-06,  2.1006e-07,  3.4275e-07,\n",
      "          5.9653e-07, -6.7314e-08, -2.6987e-06, -1.2318e-07, -1.9951e-05,\n",
      "          4.1791e-09,  9.0749e-08,  1.4211e-07, -1.2994e-08,  7.7482e-09,\n",
      "          2.0325e-06,  3.2872e-08,  1.8770e-07, -3.6565e-08, -4.4565e-07,\n",
      "          5.4924e-09, -2.8814e-07, -1.0286e-05, -1.2779e-05, -3.6414e-09,\n",
      "          4.2905e-08,  9.2467e-08,  1.1076e-06,  6.8068e-07,  1.0772e-08,\n",
      "         -2.5271e-07, -1.2429e-08, -1.7459e-07,  3.1883e-06,  3.0819e-08,\n",
      "          8.3064e-07,  1.4967e-06,  3.5149e-07,  1.9145e-06,  1.0433e-05,\n",
      "         -4.0918e-06,  3.2103e-06, -1.2838e-09, -1.7808e-08, -9.6167e-06,\n",
      "          4.1772e-06,  9.4751e-08,  7.0124e-09, -5.8482e-08,  4.1038e-06,\n",
      "          9.3137e-08,  1.4279e-08, -8.8191e-09,  2.0127e-08, -2.0090e-07,\n",
      "          1.7485e-06,  5.9445e-09, -6.4444e-07, -5.8941e-07, -3.4347e-08,\n",
      "          1.4494e-06,  1.6853e-08, -5.9534e-08, -1.2081e-06, -1.1556e-07,\n",
      "         -2.2007e-06, -1.6601e-06,  2.1164e-07,  6.2411e-08,  1.6189e-06,\n",
      "         -1.4095e-05,  1.2285e-05,  1.9836e-09, -1.1830e-09,  1.4245e-07,\n",
      "         -4.2388e-09, -7.5148e-06, -5.9049e-06, -4.7331e-06,  1.6933e-08,\n",
      "         -4.7904e-08,  3.3373e-09, -1.2266e-08, -1.7535e-08, -9.2677e-07,\n",
      "         -1.2172e-06, -3.0298e-08, -1.1858e-07, -6.2225e-06,  7.0328e-08,\n",
      "          6.8054e-08, -7.1502e-07, -2.6513e-07,  2.7297e-06,  2.3259e-09,\n",
      "          7.3877e-08, -7.2392e-10,  3.1065e-07,  2.8778e-08,  4.6586e-09,\n",
      "          2.0189e-07, -1.7172e-09,  1.2696e-07,  2.4887e-09, -6.3959e-08,\n",
      "          1.3936e-07, -1.6669e-07,  4.6270e-07,  1.2864e-07, -5.1439e-06,\n",
      "          1.0699e-07, -1.8113e-06,  8.5400e-10,  2.5707e-07, -3.5609e-06,\n",
      "          1.6756e-05,  3.0093e-06, -4.4207e-07, -9.3632e-08, -1.4868e-05,\n",
      "          1.2654e-07, -1.3183e-05, -1.4881e-06,  2.0645e-05,  4.9586e-07,\n",
      "          6.1311e-08,  5.1878e-06,  1.2307e-05, -5.6192e-07, -6.8642e-07,\n",
      "          3.7236e-06,  5.0237e-09, -2.3857e-08, -7.7664e-08, -8.3865e-09,\n",
      "          1.5629e-06,  2.6984e-07,  5.2005e-08, -3.9755e-06, -1.7400e-06,\n",
      "         -1.7448e-07,  1.1338e-07, -1.3464e-06, -4.8910e-06,  1.4077e-07,\n",
      "         -4.3792e-07, -4.1272e-08,  7.2853e-08, -8.9580e-08, -4.6144e-09,\n",
      "         -7.4259e-07, -1.5908e-05,  3.6718e-06,  9.4396e-06,  5.6789e-08,\n",
      "          1.7059e-09,  2.5213e-06, -4.6034e-08,  5.2949e-08,  5.4982e-08,\n",
      "         -5.4109e-08,  2.1200e-06,  3.1344e-07,  9.6897e-08, -2.8529e-07,\n",
      "         -1.0593e-07, -1.7308e-06,  2.8259e-07, -3.0128e-06,  2.8530e-07,\n",
      "         -4.6122e-07, -6.8373e-06, -3.2311e-07, -2.2906e-08, -2.5482e-07,\n",
      "         -2.2110e-09, -1.1769e-06,  7.2977e-08, -5.7473e-07, -1.1251e-07,\n",
      "          1.0031e-07,  6.1135e-07, -1.1308e-08, -2.1041e-06,  2.4877e-08,\n",
      "          4.2130e-06, -6.9241e-09, -9.0283e-08, -7.0564e-08,  3.2511e-09,\n",
      "          2.1240e-06,  2.7619e-07, -2.1908e-06,  5.6387e-08,  2.2908e-08,\n",
      "         -5.2895e-09,  5.7410e-06, -2.2186e-07,  8.8366e-09, -2.1829e-06,\n",
      "         -1.5606e-07, -7.4005e-09,  1.4464e-07,  6.0941e-08,  1.8124e-07,\n",
      "         -5.9934e-06, -4.1108e-08, -9.2321e-06, -1.2959e-07, -2.9012e-06,\n",
      "          4.0460e-06,  4.3070e-06,  1.2612e-07,  5.2592e-08, -2.4102e-05,\n",
      "          1.0350e-07, -1.0870e-08, -5.3403e-07,  2.3729e-08, -6.2486e-06,\n",
      "          6.4484e-07, -2.0996e-05,  2.8569e-06,  6.8823e-08,  4.8678e-07,\n",
      "         -8.1302e-07,  3.0096e-07, -5.7045e-07]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 9,  7],\n",
      "         [ 7, 10],\n",
      "         [ 4, 10],\n",
      "         [ 9,  5],\n",
      "         [ 7,  3],\n",
      "         [ 7,  7],\n",
      "         [10,  2],\n",
      "         [ 8,  1],\n",
      "         [ 9,  4],\n",
      "         [ 2,  1],\n",
      "         [ 4, 18],\n",
      "         [ 5, 10],\n",
      "         [ 8, 18],\n",
      "         [ 6,  3],\n",
      "         [ 3,  2],\n",
      "         [ 3,  0],\n",
      "         [ 6, 16],\n",
      "         [ 1,  3],\n",
      "         [ 9, 10],\n",
      "         [ 6,  2],\n",
      "         [ 2, 14],\n",
      "         [ 2,  6],\n",
      "         [ 6,  1],\n",
      "         [ 7, 16],\n",
      "         [ 3, 16],\n",
      "         [ 9,  3],\n",
      "         [ 6, 18],\n",
      "         [ 3, 18],\n",
      "         [10, 15],\n",
      "         [10,  0],\n",
      "         [ 3, 17],\n",
      "         [ 8,  0],\n",
      "         [ 4, 13],\n",
      "         [ 3, 11],\n",
      "         [ 1, 17],\n",
      "         [ 8, 11],\n",
      "         [ 0, 13],\n",
      "         [ 8,  8],\n",
      "         [ 9,  6],\n",
      "         [ 1,  6],\n",
      "         [ 1, 15],\n",
      "         [ 5,  7],\n",
      "         [ 2, 16],\n",
      "         [ 6,  6],\n",
      "         [ 1, 12],\n",
      "         [ 7, 18],\n",
      "         [ 2, 18],\n",
      "         [ 8,  2],\n",
      "         [ 0, 16],\n",
      "         [ 3,  5],\n",
      "         [ 6,  5],\n",
      "         [ 0,  7],\n",
      "         [ 4, 14],\n",
      "         [ 5, 12],\n",
      "         [ 0,  3],\n",
      "         [ 1,  7],\n",
      "         [ 2,  7],\n",
      "         [ 3,  8],\n",
      "         [10, 18],\n",
      "         [10,  8],\n",
      "         [ 9,  8],\n",
      "         [ 9, 14],\n",
      "         [ 1,  9],\n",
      "         [ 2,  0],\n",
      "         [ 4,  9],\n",
      "         [10,  1],\n",
      "         [ 5,  3],\n",
      "         [ 5,  9],\n",
      "         [ 0, 10],\n",
      "         [ 6, 17],\n",
      "         [ 7,  8],\n",
      "         [ 1, 10],\n",
      "         [ 4,  2],\n",
      "         [ 0,  5],\n",
      "         [ 6, 10],\n",
      "         [ 0,  6],\n",
      "         [ 7, 14],\n",
      "         [ 5,  5],\n",
      "         [ 5, 16],\n",
      "         [ 4, 15],\n",
      "         [ 9, 13],\n",
      "         [ 6,  9],\n",
      "         [ 5,  4],\n",
      "         [ 8,  7],\n",
      "         [ 3,  4],\n",
      "         [ 7,  1],\n",
      "         [ 5,  6],\n",
      "         [ 1,  2],\n",
      "         [10, 12],\n",
      "         [ 1, 18],\n",
      "         [10, 14],\n",
      "         [ 7,  5],\n",
      "         [ 4,  6],\n",
      "         [ 0, 11],\n",
      "         [ 9,  0],\n",
      "         [ 1,  5],\n",
      "         [ 3, 13],\n",
      "         [ 2,  3],\n",
      "         [ 6,  0],\n",
      "         [ 9, 11],\n",
      "         [ 8,  5],\n",
      "         [ 3,  9],\n",
      "         [10, 11],\n",
      "         [ 7, 15],\n",
      "         [ 7, 11],\n",
      "         [ 2, 15],\n",
      "         [ 4, 11],\n",
      "         [ 3,  7],\n",
      "         [ 6, 14],\n",
      "         [ 6, 12],\n",
      "         [ 7,  4],\n",
      "         [ 3, 12],\n",
      "         [ 8, 14],\n",
      "         [ 5, 17],\n",
      "         [ 4, 16],\n",
      "         [ 1,  8],\n",
      "         [ 9, 16],\n",
      "         [ 0,  8],\n",
      "         [ 2, 13],\n",
      "         [ 9, 15],\n",
      "         [ 9, 17],\n",
      "         [ 5,  2],\n",
      "         [ 8, 12],\n",
      "         [ 0,  4],\n",
      "         [ 4,  7],\n",
      "         [ 4, 17],\n",
      "         [ 1, 14],\n",
      "         [10, 17],\n",
      "         [ 5, 15],\n",
      "         [ 6, 11],\n",
      "         [ 4, 12],\n",
      "         [ 5, 18],\n",
      "         [ 2,  8],\n",
      "         [ 3,  1],\n",
      "         [ 4,  3],\n",
      "         [ 3,  3],\n",
      "         [ 2, 11],\n",
      "         [ 8, 15],\n",
      "         [ 4,  4],\n",
      "         [ 7, 13],\n",
      "         [ 0, 14],\n",
      "         [10,  5],\n",
      "         [ 0, 15],\n",
      "         [ 5, 13],\n",
      "         [ 6,  4],\n",
      "         [ 7, 12],\n",
      "         [ 9, 12],\n",
      "         [ 6,  8],\n",
      "         [ 2, 12],\n",
      "         [ 3, 15],\n",
      "         [ 5, 14],\n",
      "         [ 4,  5],\n",
      "         [ 0,  2],\n",
      "         [ 2, 17],\n",
      "         [ 0,  1],\n",
      "         [ 0,  9],\n",
      "         [ 9,  1],\n",
      "         [ 2, 10],\n",
      "         [ 0, 18],\n",
      "         [ 7,  0],\n",
      "         [ 1, 13],\n",
      "         [ 8,  6],\n",
      "         [ 1,  4],\n",
      "         [ 5,  8],\n",
      "         [ 5,  0],\n",
      "         [10, 16],\n",
      "         [ 4,  0],\n",
      "         [ 1, 16],\n",
      "         [ 4,  1],\n",
      "         [ 7,  6],\n",
      "         [ 6, 13],\n",
      "         [ 2,  5],\n",
      "         [ 7,  2],\n",
      "         [ 3, 10],\n",
      "         [ 5,  1],\n",
      "         [ 8, 16],\n",
      "         [10,  9],\n",
      "         [ 7, 17],\n",
      "         [ 1,  1],\n",
      "         [ 1,  0],\n",
      "         [ 6,  7],\n",
      "         [ 8, 13],\n",
      "         [ 8,  3],\n",
      "         [10, 10],\n",
      "         [10,  6],\n",
      "         [ 0,  0],\n",
      "         [ 3,  6],\n",
      "         [ 8,  4],\n",
      "         [ 6, 15],\n",
      "         [ 9, 18],\n",
      "         [ 4,  8],\n",
      "         [10,  7],\n",
      "         [ 5, 11],\n",
      "         [ 7,  9],\n",
      "         [10,  3],\n",
      "         [ 2,  4],\n",
      "         [ 2,  2],\n",
      "         [ 8, 17],\n",
      "         [ 8, 10],\n",
      "         [ 9,  9],\n",
      "         [ 1, 11],\n",
      "         [ 0, 17],\n",
      "         [ 3, 14],\n",
      "         [10, 13],\n",
      "         [ 0, 12],\n",
      "         [ 2,  9],\n",
      "         [ 8,  9],\n",
      "         [ 9,  2],\n",
      "         [10,  4]]]), (11, 19)), 'cls_output': tensor([[0.9757]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/164085027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 1817\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch_junsheng_39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29fd19f11c6b89e267402bb3227bc1208f7e2c9719aa03eba13baf7684fe5867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
