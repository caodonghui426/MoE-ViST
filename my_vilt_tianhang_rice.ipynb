{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    debug = False\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # root_path = r'E:\\\\Download\\\\xiangguan' # 存放数据的根目录\n",
    "    root_path = r'/home/junsheng/data/xiangguan' # 存放数据的根目录\n",
    "    n_fold = 5\n",
    "\n",
    "    # wandb \n",
    "    wandb_name = \"vilt|290 sensor only\"\n",
    "    \n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Sensor\n",
    "    # senser_input_num = 11 # 翔冠的传感器参数\n",
    "    senser_input_num = 19 # 天航的传感器参数\n",
    "    \n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-4 # 0.01 ->1e-4\n",
    "    decay_power = 1\n",
    "    max_epoch = 50\n",
    "    max_steps = 25000\n",
    "    warmup_steps = 2500\n",
    "    end_lr = 0\n",
    "    lr_mult = 1  # multiply lr for downstream heads\n",
    "    # T_max = 8000/train_batch_size*max_epoch \n",
    "    T_max = 1000/train_batch_size*max_epoch \n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "# config = vars(config)\n",
    "# config = dict(config)\n",
    "config\n",
    "\n",
    "if config.debug:\n",
    "    config.max_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)  # 为CPU设置随机种子\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)  # 为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有GPU设置随机种子\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "setup_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_MODE\"] = 'dryrun' # 离线模式\n",
    "try:\n",
    "    # wandb.log(key=\"*******\") # if debug\n",
    "    wandb.login() # storage in ~/.netrc file\n",
    "    anonymous = None\n",
    "except:\n",
    "    anonymous = \"must\"\n",
    "    print('\\nGet your W&B access token from here: https://wandb.ai/authorize\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pic_key</th>\n",
       "      <th>date_hour</th>\n",
       "      <th>date</th>\n",
       "      <th>co2</th>\n",
       "      <th>stemp</th>\n",
       "      <th>stemp2</th>\n",
       "      <th>stemp3</th>\n",
       "      <th>stemp4</th>\n",
       "      <th>stemp5</th>\n",
       "      <th>...</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>press</th>\n",
       "      <th>solar</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind_d</th>\n",
       "      <th>wind_sp</th>\n",
       "      <th>LAI</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>282</td>\n",
       "      <td>/789/1655496854_1655496673_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>624.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>991.1</td>\n",
       "      <td>2.52</td>\n",
       "      <td>17.26</td>\n",
       "      <td>274.3</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655496854_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283</td>\n",
       "      <td>/789/1655496854_1655496673_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>624.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>991.2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>17.18</td>\n",
       "      <td>268.7</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655496854_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>284</td>\n",
       "      <td>/789/1655504090_1655503874_4.jpg</td>\n",
       "      <td>2022-06-18 06</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>617.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>991.9</td>\n",
       "      <td>8.84</td>\n",
       "      <td>17.75</td>\n",
       "      <td>248.6</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655504090_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285</td>\n",
       "      <td>/789/1655504090_1655503874_4.jpg</td>\n",
       "      <td>2022-06-18 06</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>617.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>9.20</td>\n",
       "      <td>17.83</td>\n",
       "      <td>265.7</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655504090_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>/789/1655511249_1655511073_4.jpg</td>\n",
       "      <td>2022-06-18 08</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>604.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.1</td>\n",
       "      <td>18.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>992.6</td>\n",
       "      <td>17.75</td>\n",
       "      <td>18.98</td>\n",
       "      <td>275.4</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>/home/junsheng/data/tianhang_rice/1655511249_1...</td>\n",
       "      <td>1.626667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                           pic_key      date_hour        date    co2  \\\n",
       "0    282  /789/1655496854_1655496673_4.jpg  2022-06-18 04  2022-06-18  624.0   \n",
       "1    283  /789/1655496854_1655496673_4.jpg  2022-06-18 04  2022-06-18  624.0   \n",
       "2    284  /789/1655504090_1655503874_4.jpg  2022-06-18 06  2022-06-18  617.0   \n",
       "3    285  /789/1655504090_1655503874_4.jpg  2022-06-18 06  2022-06-18  617.0   \n",
       "4    286  /789/1655511249_1655511073_4.jpg  2022-06-18 08  2022-06-18  604.0   \n",
       "\n",
       "   stemp  stemp2  stemp3  stemp4  stemp5  ...  pm10  pm25  press  solar  \\\n",
       "0   19.8    19.6    19.8    19.3    19.1  ...   6.0   6.0  991.1   2.52   \n",
       "1   19.8    19.6    19.8    19.3    19.1  ...   7.0   7.0  991.2   5.93   \n",
       "2   19.5    19.5    19.6    19.1    19.0  ...   5.0   5.0  991.9   8.84   \n",
       "3   19.5    19.4    19.5    19.1    19.0  ...   3.0   3.0  992.0   9.20   \n",
       "4   19.3    19.2    19.4    19.1    18.9  ...   1.0   1.0  992.6  17.75   \n",
       "\n",
       "    temp wind_d wind_sp       LAI  \\\n",
       "0  17.26  274.3    3.75  1.626667   \n",
       "1  17.18  268.7    2.67  1.626667   \n",
       "2  17.75  248.6    2.07  1.626667   \n",
       "3  17.83  265.7    2.95  1.626667   \n",
       "4  18.98  275.4    3.62  1.626667   \n",
       "\n",
       "                                          image_path     label  \n",
       "0  /home/junsheng/data/tianhang_rice/1655496854_1...  1.626667  \n",
       "1  /home/junsheng/data/tianhang_rice/1655496854_1...  1.626667  \n",
       "2  /home/junsheng/data/tianhang_rice/1655504090_1...  1.626667  \n",
       "3  /home/junsheng/data/tianhang_rice/1655504090_1...  1.626667  \n",
       "4  /home/junsheng/data/tianhang_rice/1655511249_1...  1.626667  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tianhang = pd.read_csv(\"/home/junsheng/ViLT/data/290-tianhang-rice.csv\")\n",
    "df_tianhang['image_path'] = df_tianhang['pic_key'].map(lambda x:os.path.join('/home/junsheng/data/tianhang_rice',x.split('/')[-1]))\n",
    "df_tianhang['label'] = df_tianhang['LAI']\n",
    "df_tianhang = df_tianhang.dropna()\n",
    "df_tianhang = df_tianhang.reset_index()\n",
    "df_tianhang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n",
      "763\n",
      "['1662156938_1662156676_4.jpg', '1653783194_1653783073_4.jpg', '1654416840_1654416674_4.jpg', '1655259332_1655259076_4.jpg', '1653790398_1653790273_4.jpg', '1654467235_1654467074_4.jpg', '1655201643_1655201475_4.jpg', '1658304910_1658304676_4.jpg', '1657966494_1657966275_4.jpg', '1655460872_1655460672_4.jpg', '1662272091_1662271873_4.jpg', '1655266456_1655266276_4.jpg', '1654755224_1654755073_4.jpg', '1658268849_1658268671_4.jpg', '1653963189_1653963073_4.jpg', '1658283283_1658283073_4.jpg', '1653984798_1653984674_4.jpg', '1662264895_1662264673_4.jpg', '1658384087_1658383873_4.jpg', '1654935249_1654935073_4.jpg', '1662171336_1662171077_4.jpg', '1657829686_1657829472_4.jpg', '1654834532_1654834277_4.jpg', '1654553669_1654553474_4.jpg', '1653876767_1653876672_4.jpg', '1662192998_1662192677_4.jpg', '1654827275_1654827077_4.jpg', '1655280872_1655280676_4.jpg', '1651716813_1651716678_4.jpg', '1654409629_1654409476_4.jpg', '1653804793_1653804673_4.jpg', '1657844100_1657843874_4.jpg', '1658052866_1658052671_4.jpg', '1657779278_1657779072_4.jpg', '1654762414_1654762272_4.jpg', '1658175261_1658175072_4.jpg', '1653869579_1653869472_4.jpg', '1655244857_1655244673_4.jpg', '1657880094_1657879872_4.jpg', '1657793630_1657793472_4.jpg', '1655252096_1655251876_4.jpg', '1658218520_1658218276_4.jpg', '1654373600_1654373473_4.jpg', '1655453622_1655453472_4.jpg', '1657772058_1657771873_4.jpg', '1662250580_1662250277_4.jpg', '1658276093_1658275874_4.jpg', '1655187292_1655187077_4.jpg', '1653977592_1653977474_4.jpg', '1654316002_1654315873_4.jpg', '1654820080_1654819876_4.jpg', '1657865697_1657865473_4.jpg', '1654769607_1654769472_4.jpg', '1658002442_1658002270_4.jpg', '1662149570_1662149469_4.jpg', '1654913665_1654913476_4.jpg', '1654395242_1654395076_4.jpg', '1654460013_1654459871_4.jpg', '1655108049_1655107872_4.jpg', '1654640051_1654639874_4.jpg', '1655288067_1655287873_4.jpg', '1657930515_1657930275_4.jpg', '1655064891_1655064672_4.jpg', '1658376935_1658376674_4.jpg', '1658261662_1658261472_4.jpg', '1658391283_1658391073_4.jpg', '1655374456_1655374273_4.jpg', '1654748068_1654747873_4.jpg', '1654402410_1654402275_4.jpg', '1651630435_1651630278_4.jpg', '1658024117_1658023871_4.jpg', '1654856110_1654855874_4.jpg', '1655172891_1655172677_4.jpg', '1655014466_1655014276_4.jpg', '1654589713_1654589476_4.jpg', '1655446445_1655446275_4.jpg', '1657858531_1657858273_4.jpg', '1654496068_1654495874_4.jpg', '1655115228_1655115072_4.jpg', '1654560853_1654560675_4.jpg', '1651623244_1651623079_4.jpg', '1654337599_1654337475_4.jpg', '1658009676_1658009470_4.jpg', '1662286478_1662286271_4.jpg', '1651644831_1651644678_4.jpg', '1657923276_1657923072_4.jpg', '1654510458_1654510274_4.jpg', '1654654421_1654654273_4.jpg', '1654330398_1654330274_4.jpg', '1654740852_1654740675_4.jpg', '1658117667_1658117473_4.jpg', '1654244018_1654243874_4.jpg', '1658088875_1658088671_4.jpg', '1658189717_1658189475_4.jpg', '1658211281_1658211076_4.jpg', '1654157578_1654157472_4.jpg', '1654676073_1654675874_4.jpg', '1654323209_1654323074_4.jpg', '1662243234_1662243074_4.jpg', '1658225733_1658225474_4.jpg', '1655165678_1655165475_4.jpg', '1662178592_1662178276_4.jpg', '1655000043_1654999876_4.jpg', '1657872923_1657872672_4.jpg', '1657916032_1657915873_4.jpg', '1655345689_1655345476_4.jpg', '1655331267_1655331073_4.jpg', '1654978452_1654978272_4.jpg', '1657851281_1657851073_4.jpg', '1654805672_1654805473_4.jpg', '1658096092_1658095873_4.jpg', '1654063992_1654063873_4.jpg', '1654582514_1654582274_4.jpg', '1654388033_1654387876_4.jpg', '1658031304_1658031072_4.jpg', '1658448910_1658448675_4.jpg', '1654848876_1654848676_4.jpg', '1654928060_1654927876_4.jpg', '1657786528_1657786272_4.jpg', '1662164109_1662163876_4.jpg', '1658103262_1658103071_4.jpg', '1662185718_1662185474_4.jpg', '1654719269_1654719074_4.jpg', '1654135995_1654135873_4.jpg', '1655079279_1655079074_4.jpg', '1655093685_1655093475_4.jpg', '1654546450_1654546273_4.jpg', '1658398473_1658398273_4.jpg', '1655021644_1655021475_4.jpg', '1654071218_1654071073_4.jpg', '1658369662_1658369470_4.jpg', '1655072036_1655071873_4.jpg', '1654236845_1654236677_4.jpg', '1657959267_1657959073_4.jpg', '1653955972_1653955872_4.jpg', '1657764908_1657764674_4.jpg', '1655100847_1655100673_4.jpg', '1655028850_1655028673_4.jpg', '1655417662_1655417472_4.jpg', '1658132109_1658131872_4.jpg', '1654892090_1654891870_4.jpg', '1655007297_1655007077_4.jpg', '1654920876_1654920676_4.jpg', '1658456084_1658455876_4.jpg', '1654222398_1654222273_4.jpg', '1658204117_1658203876_4.jpg', '1651709638_1651709479_4.jpg', '1654308814_1654308675_4.jpg', '1657836923_1657836672_4.jpg', '1658441703_1658441473_4.jpg', '1658124904_1658124673_4.jpg', '1654726446_1654726273_4.jpg', '1657952162_1657951875_4.jpg', '1653891196_1653891072_4.jpg', '1658016892_1658016671_4.jpg', '1658312049_1658311871_4.jpg', '1658297652_1658297474_4.jpg', '1655367332_1655367076_4.jpg', '1654503252_1654503075_4.jpg', '1654380811_1654380674_4.jpg', '1654942456_1654942275_4.jpg', '1654128781_1654128673_4.jpg', '1654596904_1654596674_4.jpg', '1658434505_1658434272_4.jpg', '1658139297_1658139075_4.jpg', '1651652028_1651651878_4.jpg', '1653898396_1653898271_4.jpg', '1654150362_1654150272_4.jpg', '1655273685_1655273477_4.jpg', '1655324074_1655323872_4.jpg', '1654733643_1654733473_4.jpg', '1662279350_1662279073_4.jpg', '1657944983_1657944676_4.jpg', '1655194475_1655194274_4.jpg', '1655086461_1655086274_4.jpg', '1654474472_1654474276_4.jpg', '1654424007_1654423874_4.jpg', '1654992878_1654992675_4.jpg', '1662235975_1662235870_4.jpg', '1655237753_1655237473_4.jpg', '1654568081_1654567876_4.jpg', '1654899281_1654899073_4.jpg', '1658347963_1658347870_4.jpg', '1655360119_1655359877_4.jpg', '1662200035_1662199871_4.jpg', '1654668871_1654668675_4.jpg', '1655151293_1655151073_4.jpg', '1658355249_1658355072_4.jpg', '1658038485_1658038272_4.jpg', '1654661632_1654661473_4.jpg', '1655439257_1655439074_4.jpg', '1655410456_1655410272_4.jpg', '1654841694_1654841477_4.jpg', '1651573597_1651573421_4.jpg', '1654647264_1654647073_4.jpg', '1655338447_1655338276_4.jpg', '1654985688_1654985473_4.jpg', '1658045663_1658045471_4.jpg', '1658362490_1658362272_4.jpg', '1654632851_1654632673_4.jpg', '1653811995_1653811873_4.jpg', '1655352923_1655352676_4.jpg', '1658290521_1658290271_4.jpg', '1658182528_1658182272_4.jpg', '1654488935_1654488677_4.jpg', '1654575283_1654575076_4.jpg', '1654042417_1654042274_4.jpg', '1655424844_1655424672_4.jpg', '1654049596_1654049475_4.jpg', '1658196961_1658196676_4.jpg', '1655432051_1655431872_4.jpg', '1654812854_1654812674_4.jpg', '1657937686_1657937473_4.jpg', '1654301585_1654301472_4.jpg', '1655158478_1655158272_4.jpg', '1662257735_1662257476_4.jpg', '1654481667_1654481477_4.jpg', '1654683242_1654683073_4.jpg', '1658110492_1658110273_4.jpg', '1655180123_1655179876_4.jpg', '1654215184_1654215075_4.jpg', '1654906476_1654906276_4.jpg']\n"
     ]
    }
   ],
   "source": [
    "# 检查图片下载的全不全\n",
    "pic = df_tianhang.image_path.map(lambda x:x.split('/')[-1]).unique()\n",
    "print(len(pic))\n",
    "file_ls = os.listdir(\"/home/junsheng/data/tianhang_rice\")\n",
    "print(len(file_ls))\n",
    "ret = list(set(pic) ^ set(file_ls))\n",
    "print(ret) #差集\n",
    "# assert len(pic)==len(file_ls),\"请检查下载的图片，缺了{}个\".format(len(pic)-len(file_ls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化非object列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'pic_key',\n",
       " 'date_hour',\n",
       " 'date',\n",
       " 'co2',\n",
       " 'stemp',\n",
       " 'stemp2',\n",
       " 'stemp3',\n",
       " 'stemp4',\n",
       " 'stemp5',\n",
       " 'shumi',\n",
       " 'shumi2',\n",
       " 'shumi3',\n",
       " 'shumi4',\n",
       " 'shumi5',\n",
       " 'ts',\n",
       " 'insert_time',\n",
       " 'humi',\n",
       " 'pm10',\n",
       " 'pm25',\n",
       " 'press',\n",
       " 'solar',\n",
       " 'temp',\n",
       " 'wind_d',\n",
       " 'wind_sp',\n",
       " 'LAI',\n",
       " 'image_path',\n",
       " 'label']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_tianhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'co2', 'stemp', 'stemp2', 'stemp3', 'stemp4', 'stemp5', 'shumi', 'shumi2', 'shumi3', 'shumi4', 'shumi5', 'humi', 'pm10', 'pm25', 'press', 'solar', 'temp', 'wind_d', 'wind_sp', 'LAI', 'label']\n",
      "{'index': (282, 1493), 'co2': (0.0, 1175.0), 'stemp': (13.3, 24.0), 'stemp2': (14.1, 22.8), 'stemp3': (14.1, 23.2), 'stemp4': (14.3, 22.3), 'stemp5': (14.7, 21.8), 'shumi': (73.9, 76.9), 'shumi2': (70.4, 74.8), 'shumi3': (67.5, 69.2), 'shumi4': (72.2, 74.2), 'shumi5': (69.8, 71.8), 'humi': (31.0, 100.0), 'pm10': (0.0, 1333.0), 'pm25': (0.0, 1333.0), 'press': (981.1, 1009.0), 'solar': (0.0, 200.0), 'temp': (7.39, 32.0), 'wind_d': (0.0, 359.8), 'wind_sp': (0.0, 9.41), 'LAI': (1.3458333333333334, 2.2466666666666666), 'label': (1.3458333333333334, 2.2466666666666666)}\n"
     ]
    }
   ],
   "source": [
    "number_title = []\n",
    "recorder = {}\n",
    "for title in df_tianhang:\n",
    "    # print(df_xiangguan[title].head())\n",
    "    if title == 'raw_label':\n",
    "        continue\n",
    "    if df_tianhang[title].dtype != \"object\":\n",
    "        \n",
    "        number_title.append(title)\n",
    "        x_min = df_tianhang[title].min()\n",
    "        x_max = df_tianhang[title].max()\n",
    "        # print(x_min,x_max)\n",
    "        recorder[title] = (x_min,x_max)\n",
    "        df_tianhang[title] = df_tianhang[title].map(lambda x:(x-x_min)/(x_max - x_min))\n",
    "print(number_title)\n",
    "print(recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 19\n"
     ]
    }
   ],
   "source": [
    "# xiangguan_sensor = ['temperature', 'humidity', 'illuminance', 'soil_temperature', 'soil_humidity', 'pressure', 'wind_speed', 'photosynthetic', 'sun_exposure_time', 'COz', 'soil_ph']\n",
    "tianhang_sensor = ['co2', 'stemp', 'stemp2', 'stemp3', 'stemp4', 'stemp5', 'shumi', 'shumi2', 'shumi3', 'shumi4', 'shumi5', 'humi', 'pm10', 'pm25', 'press', 'solar', 'temp', 'wind_d', 'wind_sp']\n",
    "\n",
    "df_tianhang['sensor'] = df_tianhang[tianhang_sensor].values.tolist()\n",
    "print(\"input dim:\",len(tianhang_sensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082, 29)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_tianhang\n",
    "if config.debug:\n",
    "    df = df[:100]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0.0    217\n",
       "1.0    217\n",
       "2.0    216\n",
       "3.0    216\n",
       "4.0    216\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=config.seed)  \n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df,df.date)):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "df.groupby(['fold'])['label'].count()# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv(\"test_fold.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransforms = transforms.Compose([\n",
    "    transforms.Resize((config.img_size,config.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.7136, 0.7118, 0.6788],\n",
    "    std=[0.3338, 0.3453, 0.3020],\n",
    "    \n",
    ")\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img =  Image.open(path).convert('RGB')\n",
    "    img = myTransforms(img)\n",
    "    return img\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df         = df\n",
    "        self.label      = label\n",
    "        self.sensors = df['sensor'].tolist()\n",
    "        self.img_paths  = df['image_path'].tolist()   \n",
    "        if self.label:\n",
    "            self.labels = df['label'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path  = self.img_paths[index]\n",
    "        img = load_img(img_path)\n",
    "        sensor = self.sensors[index]\n",
    "        sensor = torch.tensor(sensor).unsqueeze(0) #[1,n]\n",
    "        if self.label:\n",
    "            label = self.labels[index]\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
    "        else:\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataloader(fold:int):\n",
    "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "\n",
    "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    print(\"train_df.shape:\",train_df.shape)\n",
    "    print(\"valid_df.shape:\",valid_df.shape)\n",
    "\n",
    "    train_data  = BuildDataset(df=train_df,label=True)\n",
    "    valid_data = BuildDataset(df=valid_df,label=True)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=config.train_batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=config.valid_batch_size,shuffle=False)\n",
    "    # test_loader = DataLoader(test_data, batch_size=config.test_batch_size,shuffle=False)\n",
    "    return train_loader,valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (865, 30)\n",
      "valid_df.shape: (217, 30)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = BuildDataset(df=df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.train_batch_size,shuffle=True)\n",
    "# valid_loader = DataLoader(train_dataset, batch_size=config.valid_batch_size,shuffle=True)\n",
    "train_loader,valid_loader = fetch_dataloader(fold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3847427/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 384, 384])\n",
      "torch.Size([32, 1, 19])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "img,sensor,label = next(iter(train_loader))\n",
    "print(img.shape)\n",
    "print(sensor.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorViLTransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorOnlyViLTransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorOnlyViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        # mask_image=False,\n",
    "        # image_token_type_idx=1,\n",
    "        # image_embeds=None,\n",
    "        # image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        # if image_embeds is None and image_masks is None:\n",
    "        #     img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "        #     (\n",
    "        #         image_embeds, # torch.Size([1, 217, 768])\n",
    "        #         image_masks, # torch.Size([1, 217])\n",
    "        #         patch_index,\n",
    "        #         image_labels,\n",
    "        #     ) = self.transformer.visual_embed(\n",
    "        #         img,\n",
    "        #         max_image_len=config.max_image_len,\n",
    "        #         mask_it=mask_image,\n",
    "        #     )\n",
    "        # else:\n",
    "        #     patch_index, image_labels = (\n",
    "        #         None,\n",
    "        #         None,\n",
    "        #     )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        # image_embeds = image_embeds + self.token_type_embeddings(\n",
    "        #         torch.full_like(image_masks, image_token_type_idx)\n",
    "        #     )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        # batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(sensor_embeds.shape[1],1).to(config.device) # 序列数量\n",
    "        # image_masks = image_masks.to(config.device)\n",
    "        # co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        # co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "        co_embeds = sensor_embeds\n",
    "        co_masks = sensor_masks\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 1, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        # sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "        #     x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "        #     x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        # )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "        #    \"sensor_feats\":sensor_feats,\n",
    "            # \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            # \"image_labels\": image_labels,\n",
    "            # \"image_masks\": image_masks,\n",
    "           \n",
    "            # \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pretrained weights exist or were found for this model. Using random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 sensorOnlyViLTransformerSS(\n",
      "  (sensor_linear): Linear(in_features=19, out_features=768, bias=True)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n",
      "1 Linear(in_features=19, out_features=768, bias=True)\n",
      "2 Embedding(2, 768)\n",
      "3 VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "4 PatchEmbed(\n",
      "  (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      ")\n",
      "5 Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "6 Dropout(p=0.1, inplace=False)\n",
      "7 ModuleList(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "8 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "9 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "10 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "11 Linear(in_features=768, out_features=2304, bias=True)\n",
      "12 Dropout(p=0.0, inplace=False)\n",
      "13 Linear(in_features=768, out_features=768, bias=True)\n",
      "14 Dropout(p=0.1, inplace=False)\n",
      "15 Identity()\n",
      "16 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "17 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "18 Linear(in_features=768, out_features=3072, bias=True)\n",
      "19 GELU(approximate=none)\n",
      "20 Linear(in_features=3072, out_features=768, bias=True)\n",
      "21 Dropout(p=0.1, inplace=False)\n",
      "22 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "23 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "24 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "25 Linear(in_features=768, out_features=2304, bias=True)\n",
      "26 Dropout(p=0.0, inplace=False)\n",
      "27 Linear(in_features=768, out_features=768, bias=True)\n",
      "28 Dropout(p=0.1, inplace=False)\n",
      "29 Identity()\n",
      "30 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "31 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "32 Linear(in_features=768, out_features=3072, bias=True)\n",
      "33 GELU(approximate=none)\n",
      "34 Linear(in_features=3072, out_features=768, bias=True)\n",
      "35 Dropout(p=0.1, inplace=False)\n",
      "36 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "37 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "38 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "39 Linear(in_features=768, out_features=2304, bias=True)\n",
      "40 Dropout(p=0.0, inplace=False)\n",
      "41 Linear(in_features=768, out_features=768, bias=True)\n",
      "42 Dropout(p=0.1, inplace=False)\n",
      "43 Identity()\n",
      "44 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "45 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "46 Linear(in_features=768, out_features=3072, bias=True)\n",
      "47 GELU(approximate=none)\n",
      "48 Linear(in_features=3072, out_features=768, bias=True)\n",
      "49 Dropout(p=0.1, inplace=False)\n",
      "50 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "51 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "52 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "53 Linear(in_features=768, out_features=2304, bias=True)\n",
      "54 Dropout(p=0.0, inplace=False)\n",
      "55 Linear(in_features=768, out_features=768, bias=True)\n",
      "56 Dropout(p=0.1, inplace=False)\n",
      "57 Identity()\n",
      "58 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "59 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "60 Linear(in_features=768, out_features=3072, bias=True)\n",
      "61 GELU(approximate=none)\n",
      "62 Linear(in_features=3072, out_features=768, bias=True)\n",
      "63 Dropout(p=0.1, inplace=False)\n",
      "64 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "65 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "66 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "67 Linear(in_features=768, out_features=2304, bias=True)\n",
      "68 Dropout(p=0.0, inplace=False)\n",
      "69 Linear(in_features=768, out_features=768, bias=True)\n",
      "70 Dropout(p=0.1, inplace=False)\n",
      "71 Identity()\n",
      "72 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "73 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "74 Linear(in_features=768, out_features=3072, bias=True)\n",
      "75 GELU(approximate=none)\n",
      "76 Linear(in_features=3072, out_features=768, bias=True)\n",
      "77 Dropout(p=0.1, inplace=False)\n",
      "78 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "79 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "80 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "81 Linear(in_features=768, out_features=2304, bias=True)\n",
      "82 Dropout(p=0.0, inplace=False)\n",
      "83 Linear(in_features=768, out_features=768, bias=True)\n",
      "84 Dropout(p=0.1, inplace=False)\n",
      "85 Identity()\n",
      "86 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "87 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "88 Linear(in_features=768, out_features=3072, bias=True)\n",
      "89 GELU(approximate=none)\n",
      "90 Linear(in_features=3072, out_features=768, bias=True)\n",
      "91 Dropout(p=0.1, inplace=False)\n",
      "92 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "93 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "94 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "95 Linear(in_features=768, out_features=2304, bias=True)\n",
      "96 Dropout(p=0.0, inplace=False)\n",
      "97 Linear(in_features=768, out_features=768, bias=True)\n",
      "98 Dropout(p=0.1, inplace=False)\n",
      "99 Identity()\n",
      "100 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "101 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "102 Linear(in_features=768, out_features=3072, bias=True)\n",
      "103 GELU(approximate=none)\n",
      "104 Linear(in_features=3072, out_features=768, bias=True)\n",
      "105 Dropout(p=0.1, inplace=False)\n",
      "106 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "107 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "108 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "109 Linear(in_features=768, out_features=2304, bias=True)\n",
      "110 Dropout(p=0.0, inplace=False)\n",
      "111 Linear(in_features=768, out_features=768, bias=True)\n",
      "112 Dropout(p=0.1, inplace=False)\n",
      "113 Identity()\n",
      "114 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "115 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "116 Linear(in_features=768, out_features=3072, bias=True)\n",
      "117 GELU(approximate=none)\n",
      "118 Linear(in_features=3072, out_features=768, bias=True)\n",
      "119 Dropout(p=0.1, inplace=False)\n",
      "120 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "121 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "122 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "123 Linear(in_features=768, out_features=2304, bias=True)\n",
      "124 Dropout(p=0.0, inplace=False)\n",
      "125 Linear(in_features=768, out_features=768, bias=True)\n",
      "126 Dropout(p=0.1, inplace=False)\n",
      "127 Identity()\n",
      "128 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "129 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "130 Linear(in_features=768, out_features=3072, bias=True)\n",
      "131 GELU(approximate=none)\n",
      "132 Linear(in_features=3072, out_features=768, bias=True)\n",
      "133 Dropout(p=0.1, inplace=False)\n",
      "134 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "135 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "136 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "137 Linear(in_features=768, out_features=2304, bias=True)\n",
      "138 Dropout(p=0.0, inplace=False)\n",
      "139 Linear(in_features=768, out_features=768, bias=True)\n",
      "140 Dropout(p=0.1, inplace=False)\n",
      "141 Identity()\n",
      "142 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "143 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "144 Linear(in_features=768, out_features=3072, bias=True)\n",
      "145 GELU(approximate=none)\n",
      "146 Linear(in_features=3072, out_features=768, bias=True)\n",
      "147 Dropout(p=0.1, inplace=False)\n",
      "148 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "149 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "150 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "151 Linear(in_features=768, out_features=2304, bias=True)\n",
      "152 Dropout(p=0.0, inplace=False)\n",
      "153 Linear(in_features=768, out_features=768, bias=True)\n",
      "154 Dropout(p=0.1, inplace=False)\n",
      "155 Identity()\n",
      "156 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "157 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "158 Linear(in_features=768, out_features=3072, bias=True)\n",
      "159 GELU(approximate=none)\n",
      "160 Linear(in_features=3072, out_features=768, bias=True)\n",
      "161 Dropout(p=0.1, inplace=False)\n",
      "162 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "163 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "164 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "165 Linear(in_features=768, out_features=2304, bias=True)\n",
      "166 Dropout(p=0.0, inplace=False)\n",
      "167 Linear(in_features=768, out_features=768, bias=True)\n",
      "168 Dropout(p=0.1, inplace=False)\n",
      "169 Identity()\n",
      "170 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "171 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "172 Linear(in_features=768, out_features=3072, bias=True)\n",
      "173 GELU(approximate=none)\n",
      "174 Linear(in_features=3072, out_features=768, bias=True)\n",
      "175 Dropout(p=0.1, inplace=False)\n",
      "176 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "177 Linear(in_features=768, out_features=768, bias=True)\n",
      "178 Tanh()\n",
      "179 Pooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "180 Linear(in_features=768, out_features=768, bias=True)\n",
      "181 Tanh()\n",
      "182 Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = sensorOnlyViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "# model = sensorViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "model.to(config.device)\n",
    "print(config.device)\n",
    "for i,m in enumerate(model.modules()):\n",
    "    print(i,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sensor = torch.rand(config.senser_input_num)\n",
    "# # sensor = torch.ones(config.senser_input_num)\n",
    "# print(sensor)\n",
    "# sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "# batch = {}\n",
    "# batch['sensor'] = sensor\n",
    "# batch['image'] = \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\"\n",
    "# model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.mse_loss #均方误差损失函数\n",
    "# criterion = F.mae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
    "    for step, (img, sensor,label) in pbar:         \n",
    "        # img = img.to(device, dtype=torch.float)\n",
    "        # sensor  = sensor.to(device, dtype=torch.float)\n",
    "        # label  = label.to(device, dtype=torch.float)\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        #一坨优化\n",
    "        optimizer.zero_grad()#每一次反向传播之前都要归零梯度\n",
    "        loss.backward()      #反向传播\n",
    "        optimizer.step()     #固定写法\n",
    "        scheduler.step()\n",
    "     \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_mem=f'{mem:0.2f} GB')\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    val_scores = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
    "    for step, (img, sensor,label) in pbar:               \n",
    "        \n",
    "        \n",
    "        batch_size = img.size(0)\n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred  = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        \n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_memory=f'{mem:0.2f} GB')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "     # init wandb\n",
    "    run = wandb.init(project=\"vilt\",\n",
    "                    config={k: v for k, v in dict(vars(config)).items() if '__' not in k},\n",
    "                    # config={k: v for k, v in dict(config).items() if '__' not in k},\n",
    "                    anonymous=anonymous,\n",
    "                    # name=f\"vilt|fold-{config.valid_fold}\",\n",
    "                    name=config.wandb_name,\n",
    "                    # group=config.wandb_group,\n",
    "                    )\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_valid_loss = 9999\n",
    "    history = defaultdict(list)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        val_loss = valid_one_epoch(model,valid_loader,device=device,optimizer=optimizer)\n",
    "        history['Train Loss'].append(train_loss)\n",
    "        history['Valid Loss'].append(val_loss)\n",
    "\n",
    "        wandb.log({\"Train Loss\": train_loss,\n",
    "                    \"Valid Loss\": val_loss,\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "                })\n",
    "        if best_valid_loss > val_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            # model_file_path = os.path.join(wandb.run.dir,\"epoch-{}-{}.bin\".format(epoch,wandb.run.id))\n",
    "            model_file_path = os.path.join(wandb.run.dir,\"epoch-best.bin\")\n",
    "            run.summary[\"Best Epoch\"] = epoch\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(\"model save to\", model_file_path)\n",
    "            \n",
    "    os.system(\"cp /home/junsheng/ViLT/my_vilt_tianhang_rice.ipynb {}\".format(wandb.run.dir))\n",
    "    run.finish()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=config.T_max, \n",
    "                                                   eta_min=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1majldq6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /home/junsheng/ViLT/wandb/offline-run-20220926_194414-1majldq6<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20220926_194414-1majldq6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1majldq6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: NVIDIA GeForce RTX 3090\n",
      "\n",
      "Epoch 1/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :   0%|          | 0/28 [00:00<?, ?it/s]/tmp/ipykernel_3847427/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
      "Train : 100%|██████████| 28/28 [00:49<00:00,  1.78s/it, gpu_mem=2.38 GB, lr=0.00100, train_loss=0.2344]\n",
      "Valid : 100%|██████████| 55/55 [00:12<00:00,  4.35it/s, gpu_memory=2.36 GB, lr=0.00100, valid_loss=0.2307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20220926_204142-13q6ro4s/files/epoch-best.bin\n",
      "Epoch 2/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :  14%|█▍        | 4/28 [00:08<00:53,  2.23s/it, gpu_mem=2.38 GB, lr=0.00100, train_loss=0.2576]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang.ipynb Cell 46\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model, history \u001b[39m=\u001b[39m run_training(model, optimizer, scheduler,device\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdevice,num_epochs\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mmax_epoch)\n",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang.ipynb Cell 46\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, scheduler, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m                                    dataloader\u001b[39m=\u001b[39;49mtrain_loader, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                                    device\u001b[39m=\u001b[39;49mdevice, epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m val_loss \u001b[39m=\u001b[39m valid_one_epoch(model,valid_loader,device\u001b[39m=\u001b[39mdevice,optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang.ipynb Cell 46\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(\u001b[39menumerate\u001b[39m(dataloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataloader), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, (img, sensor,label) \u001b[39min\u001b[39;00m pbar:         \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# img = img.to(device, dtype=torch.float)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# sensor  = sensor.to(device, dtype=torch.float)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# label  = label.to(device, dtype=torch.float)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     batch \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m:img,\u001b[39m\"\u001b[39m\u001b[39msensor\u001b[39m\u001b[39m\"\u001b[39m:sensor}\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang.ipynb Cell 46\u001b[0m in \u001b[0;36mBuildDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     img_path  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_paths[index]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     img \u001b[39m=\u001b[39m load_img(img_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     sensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msensors[index]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     sensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(sensor)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m#[1,n]\u001b[39;00m\n",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang.ipynb Cell 46\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_img\u001b[39m(path):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     img \u001b[39m=\u001b[39m  Image\u001b[39m.\u001b[39mopen(path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     img \u001b[39m=\u001b[39m myTransforms(img)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torchvision/transforms/transforms.py:349\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    342\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torchvision/transforms/functional.py:430\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    428\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    429\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 430\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, max_size\u001b[39m=\u001b[39;49mmax_size)\n\u001b[1;32m    432\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39msize, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, max_size\u001b[39m=\u001b[39mmax_size, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py:282\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], interpolation)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/PIL/Image.py:2082\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2074\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[1;32m   2075\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[1;32m   2076\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2077\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2078\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2079\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2080\u001b[0m         )\n\u001b[0;32m-> 2082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model, history = run_training(model, optimizer, scheduler,device=config.device,num_epochs=config.max_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 384, 384]) torch.Size([4, 1, 11]) tensor([0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    }
   ],
   "source": [
    "for (img,sensor,label) in valid_loader:\n",
    "    print(img.shape,sensor.shape,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'embedding_test_dict.pt')\n",
    "# print(model)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"/home/junsheng/ViLT/wandb/offline-run-20220811_120519-nzfb1xoz/files/epoch-best.bin\"))\n",
    "model.eval()\n",
    "device = config.device\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = dict()\n",
    "    batch[\"image\"] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1375, 0.8209, 0.0185, 0.3641, 0.6405, 0.4498, 0.3304, 0.3734, 0.0022,\n",
      "        0.7310, 0.2874, 0.9734, 0.3282, 0.9755, 0.5969, 0.8699, 0.7631, 0.9917,\n",
      "        0.4927])\n",
      "img.shape: torch.Size([1, 3, 352, 608])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3847427/3499233738.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensor_feats': tensor([[[ 3.0098e-06, -1.7633e-07,  1.6960e-07, -1.6755e-08, -2.2477e-06,\n",
      "           3.1875e-06,  6.5734e-06, -4.7667e-09, -3.6099e-07, -7.4075e-07,\n",
      "          -1.5870e-07,  3.6547e-06, -3.8343e-08, -9.8189e-06, -1.4985e-08,\n",
      "          -1.8155e-07, -2.6160e-06,  9.1580e-09, -2.9543e-06, -1.4204e-08,\n",
      "           9.1015e-07,  9.1730e-10,  2.2748e-08, -1.0482e-08,  5.9382e-07,\n",
      "           3.1595e-06, -2.8178e-06, -7.7881e-10,  2.9493e-06,  4.5215e-06,\n",
      "          -9.4157e-06,  1.0949e-06, -7.7766e-07,  1.1918e-08, -1.0499e-06,\n",
      "           1.1121e-06,  2.0103e-07, -7.6055e-08, -2.8639e-09,  2.2348e-06,\n",
      "           7.2907e-08,  8.4753e-06,  3.4430e-08, -2.6252e-07, -4.6850e-06,\n",
      "           1.4032e-08,  7.8041e-08, -8.3715e-08, -2.1338e-06, -2.3601e-06,\n",
      "           1.0706e-07, -4.3645e-06, -3.1781e-08, -1.7366e-09, -2.4237e-06,\n",
      "          -1.8145e-09, -1.0051e-05,  3.1867e-06,  3.3794e-07, -5.3987e-08,\n",
      "           1.0769e-05, -2.3083e-07, -3.7618e-07,  2.0922e-08, -7.3564e-08,\n",
      "          -3.3965e-06, -6.7217e-10,  8.2509e-07, -1.1047e-08,  4.3704e-08,\n",
      "          -5.3955e-09, -4.5007e-07, -3.0815e-08,  8.9620e-07,  2.2411e-07,\n",
      "          -3.2848e-07,  2.6987e-09,  1.8339e-08, -2.0817e-09,  2.4428e-07,\n",
      "          -1.8853e-05, -1.2152e-10, -9.2818e-08, -2.2570e-06, -1.4181e-06,\n",
      "           1.7269e-08, -2.2250e-06,  1.8887e-07,  6.3074e-06, -4.9686e-07,\n",
      "           1.4500e-06,  9.9305e-10, -8.2867e-07,  3.7252e-06, -7.1737e-07,\n",
      "          -9.7343e-09, -2.6843e-08, -5.3211e-10, -1.1450e-08, -7.3038e-09,\n",
      "          -1.0524e-05,  4.5044e-07, -5.4935e-06,  4.3095e-06,  2.9912e-06,\n",
      "           5.6333e-06,  3.3044e-07,  1.8394e-05, -1.3473e-08, -1.8992e-08,\n",
      "           2.1517e-06,  2.7625e-06,  2.5279e-07, -7.7173e-08,  1.5430e-07,\n",
      "           6.6173e-06, -3.5693e-05, -3.0425e-07,  4.3769e-08, -3.7723e-07,\n",
      "          -5.0205e-06,  3.3469e-08, -5.2071e-06,  5.0564e-07,  3.9897e-06,\n",
      "           6.7419e-06,  1.1888e-06,  2.3713e-08,  3.7733e-07,  1.5928e-07,\n",
      "           1.2780e-05, -8.3278e-07, -2.0194e-07,  1.0115e-06,  1.9826e-05,\n",
      "           9.3080e-08, -1.7961e-07, -2.1378e-06, -8.2230e-06,  4.8362e-08,\n",
      "          -1.6436e-05,  3.3037e-06, -4.3871e-07,  3.0447e-06,  1.6356e-09,\n",
      "          -6.3418e-07,  7.9887e-07, -5.5045e-09,  2.4994e-07, -4.2588e-06,\n",
      "           2.8839e-06,  2.7660e-07, -2.0389e-07,  1.6031e-08, -5.8021e-06,\n",
      "          -4.1197e-06, -1.0587e-06, -4.0093e-06,  9.8111e-06,  3.5794e-08,\n",
      "           4.7867e-06,  3.7998e-07, -7.6709e-08,  1.3345e-06, -9.5288e-06,\n",
      "           1.3624e-06, -5.4951e-08,  3.7034e-09, -6.2666e-06,  1.2260e-07,\n",
      "           1.3221e-07,  1.1642e-08,  1.0803e-07, -3.7666e-10,  1.4142e-06,\n",
      "           7.6693e-07,  3.0200e-06,  7.0067e-08,  2.0231e-07, -7.9244e-09,\n",
      "          -5.3173e-08,  1.5927e-06, -3.9098e-07, -6.1793e-06,  1.7040e-06,\n",
      "           9.8423e-01,  2.7452e-05,  1.7465e-07, -1.0557e-06,  8.2267e-07,\n",
      "           3.4884e-06,  1.6859e-08, -3.3922e-08,  1.4791e-06,  1.2068e-07,\n",
      "          -9.7568e-06, -3.2518e-06,  4.7711e-07,  9.9670e-09,  5.8383e-07,\n",
      "           1.2228e-07, -9.5622e-07, -2.2258e-06,  8.2155e-08,  2.1380e-09,\n",
      "           1.7513e-05,  2.3062e-09,  1.6292e-06, -1.3576e-05, -1.3346e-08,\n",
      "          -1.1403e-05,  6.5120e-06,  3.0704e-06,  2.1401e-09,  6.2560e-08,\n",
      "          -4.0647e-06, -2.6304e-07, -1.6131e-09, -1.7027e-09, -4.3810e-07,\n",
      "           1.4841e-09, -3.6633e-09,  8.5424e-07, -7.7817e-08,  1.1856e-05,\n",
      "          -4.5291e-08, -7.8146e-08, -1.2493e-07, -8.6979e-07,  3.5774e-07,\n",
      "          -1.3665e-06, -3.2400e-05,  5.3502e-08, -2.7215e-07, -1.1221e-08,\n",
      "          -4.9856e-06, -2.9023e-07,  2.1776e-06,  1.1415e-07,  1.9350e-07,\n",
      "           3.3315e-08, -4.5706e-06,  3.2977e-07,  6.2014e-09, -3.5628e-08,\n",
      "           2.0460e-05,  4.8462e-07,  8.1533e-08, -3.6662e-07,  2.0218e-09,\n",
      "           7.0092e-08,  3.0499e-08,  7.0994e-08, -3.7688e-07, -2.0100e-07,\n",
      "          -2.4224e-06, -3.0062e-07,  6.4301e-09,  1.7713e-07,  1.7732e-08,\n",
      "           1.2725e-06, -2.0990e-06, -2.3488e-06,  2.7489e-07,  4.7020e-08,\n",
      "           9.2555e-09, -7.3256e-07,  5.2418e-06,  1.0914e-06, -3.8729e-08,\n",
      "          -6.2730e-09, -2.5959e-05,  3.0323e-08, -3.1691e-08, -3.0952e-06,\n",
      "          -7.1134e-08, -1.1104e-05, -9.5722e-10, -1.2150e-06,  1.5515e-06,\n",
      "           1.2665e-05,  6.3942e-08,  2.4215e-07,  2.9230e-08, -1.2449e-06,\n",
      "           1.6332e-08,  1.0883e-07, -7.7854e-06, -3.6778e-09,  5.9814e-07,\n",
      "           1.9523e-07, -3.4544e-06,  7.8216e-06,  2.3388e-08, -1.7143e-08,\n",
      "          -1.9323e-11,  1.6839e-08,  4.3686e-06,  3.1069e-07, -7.6256e-07,\n",
      "           1.5286e-07, -2.1125e-06,  6.6289e-07, -1.1114e-06,  1.2395e-06,\n",
      "           8.5758e-07, -4.6003e-08, -1.8927e-07,  1.7908e-06, -2.7861e-06,\n",
      "          -8.5419e-07, -8.8965e-07,  6.0976e-06, -3.9715e-09,  1.3647e-06,\n",
      "          -4.1089e-09, -1.9184e-06,  1.7754e-07, -3.6437e-09,  1.3646e-06,\n",
      "          -2.1421e-07,  4.9712e-07, -1.6457e-07, -1.1139e-09,  1.1899e-05,\n",
      "           3.0686e-06, -7.0200e-08, -2.1567e-09,  5.1912e-07,  7.9358e-07,\n",
      "           9.0897e-08, -1.0681e-06, -6.9679e-07, -2.8017e-06,  5.5134e-07,\n",
      "           2.0624e-06, -2.3587e-07, -4.5886e-08, -8.4280e-07,  1.0673e-06,\n",
      "          -1.8540e-06, -2.1284e-05, -9.6752e-07,  5.3210e-07, -1.7008e-08,\n",
      "          -4.9908e-09,  5.7303e-06, -2.1034e-08, -4.9446e-07,  8.4108e-09,\n",
      "          -5.2378e-07,  1.9581e-06,  2.2143e-06,  2.0807e-06, -6.4223e-08,\n",
      "          -6.8584e-09,  1.6887e-05, -2.4014e-06,  1.1179e-08,  7.9187e-07,\n",
      "          -4.5124e-09, -5.1839e-09,  4.0340e-08, -1.7649e-08, -5.3966e-07,\n",
      "           5.3172e-08,  6.7144e-09, -3.8797e-06,  1.4236e-05,  1.8687e-09,\n",
      "          -4.7288e-06,  1.8710e-06, -2.2767e-06,  6.0807e-07,  5.2314e-07,\n",
      "          -6.8247e-07,  1.9711e-07,  2.9914e-09,  4.1123e-09,  3.1156e-08,\n",
      "          -5.2480e-07, -3.5600e-07, -4.8378e-06, -1.2834e-07, -1.2547e-05,\n",
      "          -1.0290e-08,  5.6585e-06, -3.3979e-07, -1.3877e-07, -1.1747e-06,\n",
      "           1.5072e-05, -9.8845e-06,  4.2046e-08, -7.2434e-07, -2.0548e-06,\n",
      "          -1.4187e-08,  6.4650e-08, -3.3493e-08,  1.6097e-06, -4.5861e-06,\n",
      "           7.6159e-06, -1.8508e-05, -9.9948e-08, -3.9296e-07,  1.9840e-07,\n",
      "           2.3081e-07,  1.1104e-07,  2.1113e-09, -1.1768e-08, -6.2181e-06,\n",
      "          -8.1293e-06, -7.4773e-09,  2.4156e-06,  1.0268e-06, -9.1234e-08,\n",
      "           7.1888e-07,  5.6687e-08,  4.1021e-07, -2.8901e-07, -2.8097e-06,\n",
      "           2.0052e-07,  8.4250e-09, -5.3119e-07,  9.5986e-06,  6.4821e-07,\n",
      "          -9.2822e-08,  4.2676e-06,  4.8243e-08, -1.7216e-09, -6.7168e-09,\n",
      "          -1.4064e-06, -1.8053e-07, -7.1368e-07,  2.0127e-06,  1.2534e-05,\n",
      "           5.8101e-09,  5.8645e-08,  6.8651e-08, -1.5118e-06,  6.0172e-08,\n",
      "          -5.5409e-06,  2.6458e-06,  1.0249e-06, -4.0312e-07, -2.4770e-08,\n",
      "          -1.2278e-06, -4.2247e-08, -1.3175e-07,  7.6881e-08, -2.7465e-07,\n",
      "          -4.7375e-07,  3.1335e-06, -1.9965e-06,  1.5792e-07, -1.3974e-07,\n",
      "          -3.3766e-06, -2.4819e-06,  1.2316e-06, -5.1330e-06, -7.3933e-09,\n",
      "           8.2010e-09, -2.4629e-07,  8.1276e-08, -6.3152e-06,  6.2905e-06,\n",
      "           1.4933e-06, -1.5516e-08, -4.6201e-07, -1.0308e-06,  2.2402e-06,\n",
      "          -9.8734e-09,  5.8308e-06, -1.0452e-07, -1.2605e-05, -1.6993e-08,\n",
      "           5.3595e-07, -1.2721e-08, -3.2353e-07, -5.1091e-07,  1.5968e-06,\n",
      "          -1.0109e-08, -9.8909e-08,  3.1079e-06, -3.1707e-08,  5.3091e-08,\n",
      "           3.4085e-08,  7.3934e-06,  5.8139e-07, -2.2288e-07, -2.9042e-08,\n",
      "          -1.2588e-06,  7.2942e-08, -4.0295e-06, -1.8750e-05,  2.0236e-07,\n",
      "           6.6112e-08,  2.4739e-07, -4.4595e-09, -3.4862e-07, -1.2931e-07,\n",
      "          -6.8953e-08,  4.7769e-07, -1.7555e-09, -2.3661e-08, -7.2108e-06,\n",
      "           8.3984e-10, -2.6426e-07, -1.1538e-06, -2.7866e-06,  2.5613e-09,\n",
      "           4.4275e-08, -9.1493e-06,  4.9005e-06,  3.3433e-07, -4.6391e-09,\n",
      "          -1.6358e-08,  4.8771e-10,  1.9628e-06,  2.0868e-08, -1.1452e-09,\n",
      "           1.0789e-05, -8.1685e-07,  6.9233e-08, -1.6228e-06, -8.3381e-06,\n",
      "           4.2359e-07, -9.8737e-08, -3.9379e-07,  6.2150e-07, -6.1683e-09,\n",
      "           5.1695e-09,  1.0698e-06,  2.2504e-07,  8.3911e-08, -2.0471e-08,\n",
      "           2.9494e-06,  4.5939e-06,  5.2381e-07, -1.1953e-08,  3.1559e-06,\n",
      "           4.5402e-07,  7.1209e-08,  3.8711e-06,  2.8162e-07,  3.4351e-07,\n",
      "           5.4973e-07, -6.0966e-08, -2.7282e-06, -1.2201e-07, -1.9994e-05,\n",
      "           4.2685e-09, -4.3387e-07,  9.0800e-08, -1.2749e-08,  7.6621e-09,\n",
      "           3.0819e-06,  1.2030e-06,  1.7036e-07, -3.6000e-08, -4.8558e-07,\n",
      "           5.1660e-09, -2.8738e-07, -9.7188e-06, -1.2786e-05, -3.2561e-08,\n",
      "           4.2291e-08,  9.5755e-08,  1.1112e-06,  6.5053e-07,  1.0484e-08,\n",
      "          -1.4829e-07, -1.2533e-08, -1.5670e-07,  3.1863e-06,  3.0952e-08,\n",
      "           9.6546e-07,  1.6821e-06,  3.8237e-07,  1.9186e-06,  1.1185e-05,\n",
      "          -3.9869e-06,  3.1509e-06, -1.8149e-10, -1.7585e-08, -8.7776e-06,\n",
      "           4.1415e-06,  9.4777e-08,  6.7557e-09, -5.8614e-08,  3.9178e-06,\n",
      "           9.4860e-08,  1.5651e-08, -1.1658e-08,  2.0229e-08, -1.7113e-07,\n",
      "           1.6917e-06,  5.8614e-09, -7.4751e-07, -5.5624e-07, -3.4248e-08,\n",
      "           1.4608e-06,  2.4114e-08, -5.9332e-08, -1.1684e-06, -8.9777e-08,\n",
      "          -2.2686e-06, -1.4299e-06,  2.0697e-07,  6.4309e-08,  1.5860e-06,\n",
      "          -1.4318e-05,  1.2254e-05,  2.0471e-09, -9.0062e-10,  1.3267e-07,\n",
      "          -4.2924e-09, -7.4198e-06, -6.0113e-06, -4.6402e-06,  1.5868e-08,\n",
      "          -3.1209e-08,  3.3039e-09, -1.4343e-08, -1.8774e-08, -1.2126e-06,\n",
      "          -1.1641e-06, -3.0544e-08, -1.1885e-07, -6.1869e-06,  7.0414e-08,\n",
      "           6.2661e-08, -1.2972e-06, -2.6641e-07,  2.8257e-06,  2.4926e-09,\n",
      "           1.0417e-07, -9.6318e-10,  3.1011e-07,  2.7434e-08,  4.7637e-09,\n",
      "           2.3793e-07, -1.4570e-09,  1.4453e-07,  2.3831e-09, -7.1903e-08,\n",
      "           1.3749e-07, -1.7100e-07,  4.5404e-07,  1.2744e-07, -5.2757e-06,\n",
      "           1.1934e-07, -2.1777e-06,  8.7637e-10,  2.5438e-07, -3.5818e-06,\n",
      "           1.7125e-05,  3.0114e-06, -4.0203e-07, -8.8336e-08, -1.4575e-05,\n",
      "           1.2605e-07, -1.3164e-05, -1.1944e-06,  2.0217e-05,  4.4334e-07,\n",
      "           6.0199e-08,  5.1977e-06,  1.2602e-05, -5.8742e-07, -7.8145e-07,\n",
      "           3.6207e-06,  2.0851e-09, -2.3449e-08,  1.0228e-07, -8.4813e-09,\n",
      "           1.2786e-06,  1.7770e-07,  5.7144e-08, -3.6064e-06, -2.2387e-06,\n",
      "          -2.2297e-07,  1.1394e-07, -1.4076e-06, -5.6431e-06,  1.4078e-07,\n",
      "          -4.2442e-07, -4.1291e-08,  7.5076e-08, -9.2145e-08, -4.8533e-09,\n",
      "          -7.4494e-07, -1.5917e-05,  3.6980e-06,  1.4703e-05,  5.4332e-08,\n",
      "           1.8431e-09,  2.2992e-06, -3.8852e-08,  4.8152e-08,  5.5083e-08,\n",
      "          -6.9330e-08,  1.9823e-06,  3.0828e-07,  1.0522e-07, -2.6684e-07,\n",
      "          -1.1410e-07, -1.7042e-06,  8.0030e-07, -3.0325e-06,  2.3845e-07,\n",
      "          -3.4368e-07, -6.7451e-06, -3.1775e-07, -1.3971e-08, -2.5450e-07,\n",
      "          -2.1541e-09, -6.8670e-07,  1.0022e-07, -6.7426e-07, -9.5519e-08,\n",
      "           1.0256e-07,  5.3114e-07, -1.1518e-08, -2.0892e-06,  2.4705e-08,\n",
      "           4.3903e-06, -6.9322e-09, -8.8512e-08, -6.1111e-08,  2.9449e-09,\n",
      "           1.9286e-06,  1.1984e-07, -2.4867e-06,  5.6097e-08,  2.3104e-08,\n",
      "          -3.9946e-09,  5.7752e-06, -2.1010e-07,  8.7603e-09, -2.6734e-06,\n",
      "          -1.5607e-07, -1.0976e-08,  9.3335e-08,  2.5855e-07,  1.8419e-07,\n",
      "          -6.0773e-06, -4.0655e-08, -9.0636e-06, -1.2806e-07, -2.9104e-06,\n",
      "           4.1481e-06,  4.2486e-06,  1.2737e-07,  5.1898e-08, -2.2605e-05,\n",
      "          -6.5472e-07, -1.0680e-08, -4.6648e-07,  2.4734e-08, -6.1260e-06,\n",
      "          -5.1410e-07, -2.0926e-05,  2.9158e-06,  6.9413e-08,  4.8132e-07,\n",
      "          -8.5947e-07,  3.9749e-07,  6.9227e-07]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4250e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9457e-07,\n",
      "           3.1837e-07, -3.4249e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5569e-07,  ..., -7.9452e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1836e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7536e-07,  1.5568e-07,  ..., -7.9441e-07,\n",
      "           3.1838e-07, -3.4247e-06]]], device='cuda:0'), 'cls_feats': tensor([[ 1.8375e-02, -1.9896e-02,  1.9882e-02,  1.1685e-02,  1.8806e-02,\n",
      "          1.9875e-02,  1.9744e-02,  1.9868e-02, -1.9947e-02, -1.7409e-02,\n",
      "         -1.9774e-02, -1.9898e-02, -1.9256e-02,  1.6139e-07, -2.6331e-08,\n",
      "         -1.1237e-02, -1.9593e-02,  1.8784e-02, -1.9879e-02,  1.9885e-02,\n",
      "         -1.9016e-02, -1.9950e-02, -1.9850e-02,  1.9753e-02, -1.8509e-02,\n",
      "         -1.9855e-02,  1.9890e-02, -1.9790e-02, -1.9738e-02, -1.9797e-02,\n",
      "          1.9905e-02, -8.3680e-03, -1.2369e-02,  5.3731e-04, -1.9788e-02,\n",
      "         -1.9758e-02,  1.9564e-02,  1.9836e-02,  1.9662e-02, -3.5556e-09,\n",
      "         -9.0731e-04, -1.8955e-02,  1.9771e-02,  1.9622e-02, -1.9868e-02,\n",
      "          1.9919e-02,  1.9817e-02,  1.9851e-02,  1.8362e-02, -1.9837e-02,\n",
      "         -1.9516e-02,  1.9933e-02,  1.9933e-02,  1.9715e-02,  1.9825e-02,\n",
      "         -1.8842e-02, -1.9761e-02, -1.9913e-02, -1.9816e-02,  1.8458e-02,\n",
      "          1.9929e-02, -3.4515e-08, -1.9796e-02,  1.9218e-02,  1.9942e-02,\n",
      "         -1.3053e-03, -1.0887e-02,  1.9036e-02, -1.1010e-02, -1.9759e-02,\n",
      "         -1.9846e-02,  1.5541e-09, -1.9867e-02, -1.8510e-02,  1.2107e-02,\n",
      "          1.9875e-02,  1.9752e-02,  1.9798e-02, -1.9790e-02,  1.9737e-02,\n",
      "         -1.9883e-02, -1.9888e-02,  9.3218e-04, -1.9918e-02, -1.8184e-02,\n",
      "          8.1826e-10, -1.9877e-02, -1.8317e-02,  1.9751e-02, -1.9118e-02,\n",
      "         -5.2593e-03, -1.0838e-02,  1.9447e-02, -1.9050e-02,  1.8523e-02,\n",
      "         -1.8470e-02, -1.9868e-02, -1.9798e-02,  1.8781e-02,  1.9950e-02,\n",
      "          1.2630e-02,  1.9947e-02,  1.2867e-07,  1.9703e-02,  1.9852e-02,\n",
      "          5.4472e-08, -2.6860e-08,  4.2609e-09, -1.9866e-02,  1.9868e-02,\n",
      "          2.3585e-03,  1.9734e-02,  1.9855e-02, -1.8212e-02, -4.1654e-04,\n",
      "         -1.9046e-02,  1.3664e-04,  7.1416e-03,  1.9902e-02, -2.5478e-07,\n",
      "         -1.9143e-02,  1.8725e-02, -6.9433e-06, -1.9082e-02, -1.9867e-02,\n",
      "          1.3449e-10, -1.7380e-02, -1.9841e-02, -1.9867e-02,  1.9638e-02,\n",
      "         -5.3049e-08,  1.9887e-02,  2.8675e-07,  1.8799e-02,  2.9393e-09,\n",
      "          1.9340e-02, -1.9011e-02, -1.9818e-02, -7.7050e-03, -1.9889e-02,\n",
      "         -1.9750e-10,  6.6433e-04, -4.0827e-04,  1.9228e-02, -1.9648e-02,\n",
      "         -4.7136e-04, -1.9629e-02, -1.9906e-02,  1.8747e-02, -1.9613e-02,\n",
      "          1.9709e-02, -1.9935e-02, -1.9839e-02, -8.9123e-10,  1.9866e-02,\n",
      "          3.0439e-06,  1.8809e-02,  1.9909e-02,  1.8978e-02,  1.9846e-02,\n",
      "         -9.0114e-03,  1.9926e-02, -1.7029e-02,  1.8505e-02,  1.9525e-02,\n",
      "         -1.9125e-02, -1.9946e-02,  1.8341e-02, -1.9703e-02,  1.9802e-02,\n",
      "         -1.4488e-08,  1.6784e-02,  2.2658e-03,  1.9041e-02,  1.9813e-02,\n",
      "         -1.8837e-08, -1.9899e-02,  1.9837e-02, -3.1820e-10, -1.8170e-02,\n",
      "          7.0539e-03, -1.9920e-02, -1.9761e-02,  1.9789e-02, -1.9838e-02,\n",
      "          1.8991e-02, -1.6350e-02, -7.0514e-11,  1.9276e-02, -1.9898e-02,\n",
      "         -1.9813e-02, -1.9203e-02,  1.9002e-02, -1.9803e-02,  1.8396e-02,\n",
      "         -1.9768e-02, -1.8743e-02, -1.9744e-02, -1.9619e-02, -1.9855e-02,\n",
      "          1.9765e-02,  1.9865e-02, -1.9893e-02, -1.9845e-02, -3.0110e-08,\n",
      "          1.4888e-02,  1.9545e-02, -1.9803e-02, -1.9569e-02,  1.9662e-02,\n",
      "          1.9023e-02,  1.9814e-02, -1.4487e-02,  1.9887e-02, -1.4872e-05,\n",
      "         -6.0754e-08,  1.9780e-02, -1.8978e-02,  1.9810e-02, -1.8539e-02,\n",
      "         -2.9321e-06, -1.0692e-02, -1.9917e-02,  1.8672e-02, -1.0784e-02,\n",
      "         -1.9175e-02, -1.9875e-02, -1.9792e-02, -1.9898e-02, -1.9918e-02,\n",
      "         -1.9427e-02,  2.5414e-09, -1.8638e-02,  1.9830e-02,  1.9820e-02,\n",
      "         -1.9870e-02,  1.1435e-03,  1.9403e-02, -1.9094e-02, -1.9948e-02,\n",
      "         -1.9257e-02, -1.9109e-02, -1.9516e-02, -1.9783e-02,  8.2464e-03,\n",
      "         -1.9731e-02,  1.0776e-03, -1.9549e-02,  1.0544e-02, -1.9734e-02,\n",
      "         -1.9736e-02, -1.9958e-02, -2.3719e-04, -1.8435e-02, -1.8628e-02,\n",
      "          1.9870e-02,  2.4715e-04,  2.8284e-05,  1.9952e-02, -1.9753e-02,\n",
      "          1.9760e-02,  1.9890e-02, -1.8100e-02, -4.3864e-08,  1.9888e-02,\n",
      "          1.7918e-02, -1.9867e-02, -1.8906e-02, -1.9189e-02, -1.9082e-02,\n",
      "         -1.9159e-09,  1.9821e-02, -1.6910e-02,  1.9699e-02, -1.9927e-02,\n",
      "          1.9568e-02, -1.8756e-02, -1.9890e-02, -1.9889e-02, -1.7873e-02,\n",
      "          1.9914e-02,  1.9019e-02, -1.9173e-02, -1.9870e-02, -1.5733e-06,\n",
      "          1.9917e-02,  2.2042e-03,  1.9197e-02, -8.1676e-03,  1.9294e-02,\n",
      "          1.9355e-02, -9.3546e-03,  1.9824e-02,  1.9825e-02,  4.9474e-07,\n",
      "          1.9755e-02,  1.9754e-02,  1.8320e-02,  1.9655e-02, -1.8663e-02,\n",
      "          1.9811e-02,  1.9794e-02, -1.9675e-02, -1.9107e-02,  1.9910e-02,\n",
      "         -1.9842e-02, -4.8484e-08,  4.2489e-05, -1.9847e-02, -1.7158e-07,\n",
      "         -1.5625e-02, -1.7521e-02,  1.1461e-07,  1.9565e-02, -1.9879e-02,\n",
      "          1.9944e-02, -1.9920e-02, -1.9769e-02,  1.9637e-02,  1.8816e-03,\n",
      "         -1.0979e-08, -1.9862e-02, -1.9919e-02,  1.9949e-02, -1.8566e-02,\n",
      "          1.9843e-02,  1.9615e-02,  1.9778e-02,  7.9245e-03,  1.8994e-02,\n",
      "          1.9864e-02,  1.9016e-02, -1.9389e-02, -1.3414e-08,  1.9774e-02,\n",
      "         -1.9827e-02, -2.2482e-05, -1.3721e-03,  1.8935e-02, -1.9563e-02,\n",
      "          1.9649e-02, -3.3023e-08,  1.9818e-02,  1.0852e-02,  1.9893e-02,\n",
      "          1.9931e-02, -1.8313e-02, -1.8464e-02, -1.0873e-02, -1.9937e-02,\n",
      "         -1.9875e-02, -1.9433e-02, -1.0954e-02, -1.9682e-02,  1.8641e-02,\n",
      "          1.9833e-02, -1.9633e-02, -1.8202e-02,  1.9841e-02,  7.5454e-03,\n",
      "          1.7696e-02, -1.9683e-02,  1.9083e-02, -1.9844e-02, -8.0115e-09,\n",
      "          1.9943e-02,  1.9801e-02,  1.9890e-02,  1.9868e-02,  1.9733e-02,\n",
      "         -1.9469e-03,  1.8424e-02,  1.9871e-02, -1.9943e-02, -1.9879e-02,\n",
      "         -1.9411e-02,  1.9863e-02,  1.9891e-02, -1.8459e-02, -1.8025e-02,\n",
      "          2.2501e-03, -1.9902e-02, -1.9876e-02,  1.9851e-02, -1.9593e-02,\n",
      "          1.8299e-02, -1.9741e-02, -3.0573e-07, -2.1609e-04, -1.4607e-03,\n",
      "          1.9867e-02, -1.8275e-02, -1.9190e-02, -1.9847e-02, -1.9702e-02,\n",
      "          8.6277e-04,  1.9677e-02,  1.9032e-02, -1.9760e-02, -1.9887e-02,\n",
      "         -9.6713e-03, -1.1444e-02, -1.9856e-02, -1.9794e-02,  1.9893e-02,\n",
      "         -1.9792e-02,  1.8959e-02, -1.8053e-02,  1.9915e-02,  1.9049e-02,\n",
      "          1.9799e-02, -1.9805e-02,  1.9638e-02, -1.2300e-07,  1.9924e-02,\n",
      "          1.9050e-02,  1.9881e-02, -1.9840e-02, -1.9793e-02,  1.9916e-02,\n",
      "         -9.3192e-10,  1.9254e-02,  1.9843e-02,  4.0002e-05,  1.1890e-02,\n",
      "          9.6922e-03, -1.9212e-02,  1.1539e-02,  1.9683e-02,  1.9837e-02,\n",
      "          1.7177e-02, -1.9783e-02, -1.9701e-02, -1.9882e-02, -1.8581e-02,\n",
      "         -1.9908e-02,  1.1823e-02, -1.9698e-02, -1.8017e-02, -3.1291e-05,\n",
      "         -1.9898e-02,  1.9022e-02,  1.9843e-02,  4.5555e-09,  1.9762e-02,\n",
      "         -1.9896e-02,  7.8232e-10,  1.9771e-02, -1.9893e-02, -1.9100e-02,\n",
      "          9.0414e-08, -1.9915e-02,  5.1428e-06, -1.9835e-02, -1.9801e-02,\n",
      "         -1.9669e-02, -7.3486e-05, -1.9720e-02, -1.9916e-02,  1.6053e-02,\n",
      "         -1.9877e-02,  1.9902e-02,  1.9900e-08,  1.9883e-02,  8.8717e-06,\n",
      "          1.8454e-02,  1.9823e-02, -1.9641e-02,  1.9909e-02,  1.9690e-02,\n",
      "          1.9860e-02, -4.1663e-08,  1.9869e-02, -1.9774e-02,  1.6184e-02,\n",
      "          1.6354e-02, -7.3380e-08, -1.9718e-02,  1.9877e-02,  1.8627e-02,\n",
      "         -1.9079e-02, -1.9596e-02,  1.9416e-02,  1.8644e-03, -1.9876e-02,\n",
      "          1.9893e-02, -1.9153e-02, -1.7500e-02,  1.9876e-02, -1.6741e-02,\n",
      "         -1.9161e-02, -5.9334e-04,  1.9942e-02,  1.9547e-02, -1.0672e-02,\n",
      "         -1.9909e-02,  1.9260e-02, -1.9313e-02, -1.9575e-02,  1.9933e-02,\n",
      "          1.9845e-02, -9.6498e-08,  2.3724e-03,  1.9893e-02, -1.8221e-02,\n",
      "          2.0611e-03, -1.9806e-02, -1.7774e-02, -9.4739e-03, -1.9922e-02,\n",
      "          1.9024e-02,  2.0540e-04, -1.6000e-02, -1.9550e-02,  4.1196e-05,\n",
      "          1.9815e-02, -2.1812e-03, -1.8704e-02, -1.6104e-02,  1.9920e-02,\n",
      "         -1.9117e-02, -1.7400e-02, -1.9061e-02,  1.9812e-02, -1.9840e-02,\n",
      "          1.8850e-02, -2.8229e-03, -1.9199e-02,  1.9925e-02,  1.9866e-02,\n",
      "          1.9886e-02, -5.3838e-05, -1.9877e-02,  1.9937e-02, -1.9814e-02,\n",
      "         -1.7339e-03,  1.4978e-02, -1.8506e-02,  1.9830e-02, -4.8950e-09,\n",
      "         -1.9848e-02, -1.8444e-02,  1.9929e-02, -6.0126e-06,  1.9848e-02,\n",
      "         -6.0171e-05, -1.7923e-02, -1.9881e-02, -1.9027e-02,  1.9884e-02,\n",
      "         -3.5663e-04, -1.9066e-02,  1.8428e-02,  1.9568e-02,  1.9271e-02,\n",
      "          1.9883e-02, -1.0966e-09, -1.3818e-07,  1.9881e-02,  1.9865e-02,\n",
      "         -1.8464e-02, -1.9043e-02, -1.9782e-02,  1.8942e-02, -1.9702e-02,\n",
      "          1.7851e-02, -1.9770e-02,  1.9784e-02,  1.9042e-02,  1.9777e-02,\n",
      "         -1.9714e-02,  1.9875e-02,  1.9833e-02,  1.9897e-02, -1.9842e-02,\n",
      "         -1.9466e-02, -4.5984e-07,  9.6755e-04, -1.9484e-02, -1.9602e-02,\n",
      "         -1.9886e-02,  1.9883e-02,  4.3255e-09,  1.9634e-02, -1.7486e-02,\n",
      "          1.9405e-02, -1.3436e-04, -1.9732e-02,  1.9020e-02,  1.9900e-02,\n",
      "          1.9930e-02, -1.8402e-02,  9.9047e-03,  1.9931e-02, -1.1045e-07,\n",
      "          1.9754e-02,  1.9093e-02, -1.9375e-02, -1.9774e-02,  1.9941e-02,\n",
      "          1.9295e-02, -1.9762e-02,  1.9211e-02, -1.9047e-02, -1.8338e-02,\n",
      "         -1.9904e-02,  4.1862e-09,  1.6167e-08, -1.9854e-02, -1.8992e-02,\n",
      "          1.9845e-02,  1.9870e-02,  1.9906e-02,  1.8294e-02,  1.9541e-02,\n",
      "         -1.9779e-02,  4.2682e-10, -1.9673e-06, -1.9960e-02,  1.9941e-02,\n",
      "         -1.9805e-02,  1.9433e-02, -1.9902e-02,  1.9800e-02,  1.9807e-02,\n",
      "          1.9774e-02, -4.5988e-04,  9.3662e-04,  1.2669e-05,  1.8410e-02,\n",
      "         -1.9778e-02, -8.2972e-07, -1.8450e-02,  1.8433e-04, -1.9758e-02,\n",
      "          1.2310e-02, -1.8250e-03,  1.9917e-02,  1.9786e-02,  1.9810e-02,\n",
      "          1.1771e-02, -1.8798e-09, -1.9789e-02,  3.3225e-08, -2.1899e-03,\n",
      "          1.8993e-02, -1.9904e-02, -1.9769e-02,  9.8064e-07, -1.9861e-02,\n",
      "          1.9933e-02,  1.8507e-02,  1.9829e-02,  1.9892e-02, -1.1040e-03,\n",
      "         -1.9834e-02,  1.9819e-02,  1.6897e-02, -1.9807e-02, -1.9672e-02,\n",
      "          1.7863e-02, -1.1389e-05, -1.9607e-02,  1.9593e-02, -1.9669e-02,\n",
      "          1.8906e-02,  1.2442e-05, -7.5363e-03,  4.0337e-04, -1.9725e-02,\n",
      "          1.9909e-02,  1.9824e-02,  1.9792e-02,  1.9840e-02,  1.8956e-05,\n",
      "         -1.9609e-02,  1.9535e-02,  1.8393e-02,  1.9835e-02, -1.9916e-02,\n",
      "         -1.9864e-02, -1.9433e-02, -1.9830e-02,  1.9760e-02, -1.9741e-02,\n",
      "          1.9851e-02,  1.8780e-02, -1.9434e-02,  1.4775e-06, -9.7197e-10,\n",
      "         -9.3135e-06, -1.9735e-02, -1.9742e-02,  1.9867e-02,  1.1660e-02,\n",
      "         -1.9608e-02,  1.9784e-02,  1.9899e-02,  1.2091e-02,  1.9706e-02,\n",
      "         -4.2411e-08,  1.9719e-02,  1.9706e-02, -9.8449e-03, -1.8373e-02,\n",
      "         -1.9770e-02, -1.9728e-02, -1.9272e-02,  9.5901e-08,  1.9671e-02,\n",
      "         -1.6549e-02, -1.9565e-02, -1.9826e-02,  1.9788e-02,  1.4196e-07,\n",
      "          1.8468e-02,  1.9898e-02,  1.9835e-02, -1.9589e-02,  1.9915e-02,\n",
      "          1.9926e-02,  1.8788e-02, -1.9079e-02, -1.9073e-02, -1.9683e-02,\n",
      "          1.8651e-02,  1.9812e-02,  1.9699e-02, -8.9467e-03, -1.9765e-02,\n",
      "          9.7066e-07, -1.9935e-02,  1.6708e-07, -1.8494e-02,  1.9865e-02,\n",
      "         -1.9816e-02,  1.9877e-02, -1.7271e-02, -1.8992e-02,  1.8998e-02,\n",
      "         -1.9432e-02, -1.8447e-02,  1.9697e-02, -1.8549e-02, -1.6336e-03,\n",
      "          1.9792e-02, -1.9825e-02,  1.1727e-03, -1.9797e-02,  1.7753e-03,\n",
      "          1.9858e-02,  1.8060e-02,  1.9889e-02,  1.9821e-02,  1.9710e-03,\n",
      "         -1.9489e-02, -1.9705e-02,  2.7182e-05, -1.9015e-02, -1.9930e-02,\n",
      "         -1.9797e-02, -1.9831e-02,  1.9855e-02,  1.9900e-02,  8.4308e-04,\n",
      "          7.6579e-10, -1.9847e-02,  1.9853e-02]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0098e-06, -1.7633e-07,  1.6960e-07, -1.6755e-08, -2.2477e-06,\n",
      "          3.1875e-06,  6.5734e-06, -4.7667e-09, -3.6099e-07, -7.4075e-07,\n",
      "         -1.5870e-07,  3.6547e-06, -3.8343e-08, -9.8189e-06, -1.4985e-08,\n",
      "         -1.8155e-07, -2.6160e-06,  9.1580e-09, -2.9543e-06, -1.4204e-08,\n",
      "          9.1015e-07,  9.1730e-10,  2.2748e-08, -1.0482e-08,  5.9382e-07,\n",
      "          3.1595e-06, -2.8178e-06, -7.7881e-10,  2.9493e-06,  4.5215e-06,\n",
      "         -9.4157e-06,  1.0949e-06, -7.7766e-07,  1.1918e-08, -1.0499e-06,\n",
      "          1.1121e-06,  2.0103e-07, -7.6055e-08, -2.8639e-09,  2.2348e-06,\n",
      "          7.2907e-08,  8.4753e-06,  3.4430e-08, -2.6252e-07, -4.6850e-06,\n",
      "          1.4032e-08,  7.8041e-08, -8.3715e-08, -2.1338e-06, -2.3601e-06,\n",
      "          1.0706e-07, -4.3645e-06, -3.1781e-08, -1.7366e-09, -2.4237e-06,\n",
      "         -1.8145e-09, -1.0051e-05,  3.1867e-06,  3.3794e-07, -5.3987e-08,\n",
      "          1.0769e-05, -2.3083e-07, -3.7618e-07,  2.0922e-08, -7.3564e-08,\n",
      "         -3.3965e-06, -6.7217e-10,  8.2509e-07, -1.1047e-08,  4.3704e-08,\n",
      "         -5.3955e-09, -4.5007e-07, -3.0815e-08,  8.9620e-07,  2.2411e-07,\n",
      "         -3.2848e-07,  2.6987e-09,  1.8339e-08, -2.0817e-09,  2.4428e-07,\n",
      "         -1.8853e-05, -1.2152e-10, -9.2818e-08, -2.2570e-06, -1.4181e-06,\n",
      "          1.7269e-08, -2.2250e-06,  1.8887e-07,  6.3074e-06, -4.9686e-07,\n",
      "          1.4500e-06,  9.9305e-10, -8.2867e-07,  3.7252e-06, -7.1737e-07,\n",
      "         -9.7343e-09, -2.6843e-08, -5.3211e-10, -1.1450e-08, -7.3038e-09,\n",
      "         -1.0524e-05,  4.5044e-07, -5.4935e-06,  4.3095e-06,  2.9912e-06,\n",
      "          5.6333e-06,  3.3044e-07,  1.8394e-05, -1.3473e-08, -1.8992e-08,\n",
      "          2.1517e-06,  2.7625e-06,  2.5279e-07, -7.7173e-08,  1.5430e-07,\n",
      "          6.6173e-06, -3.5693e-05, -3.0425e-07,  4.3769e-08, -3.7723e-07,\n",
      "         -5.0205e-06,  3.3469e-08, -5.2071e-06,  5.0564e-07,  3.9897e-06,\n",
      "          6.7419e-06,  1.1888e-06,  2.3713e-08,  3.7733e-07,  1.5928e-07,\n",
      "          1.2780e-05, -8.3278e-07, -2.0194e-07,  1.0115e-06,  1.9826e-05,\n",
      "          9.3080e-08, -1.7961e-07, -2.1378e-06, -8.2230e-06,  4.8362e-08,\n",
      "         -1.6436e-05,  3.3037e-06, -4.3871e-07,  3.0447e-06,  1.6356e-09,\n",
      "         -6.3418e-07,  7.9887e-07, -5.5045e-09,  2.4994e-07, -4.2588e-06,\n",
      "          2.8839e-06,  2.7660e-07, -2.0389e-07,  1.6031e-08, -5.8021e-06,\n",
      "         -4.1197e-06, -1.0587e-06, -4.0093e-06,  9.8111e-06,  3.5794e-08,\n",
      "          4.7867e-06,  3.7998e-07, -7.6709e-08,  1.3345e-06, -9.5288e-06,\n",
      "          1.3624e-06, -5.4951e-08,  3.7034e-09, -6.2666e-06,  1.2260e-07,\n",
      "          1.3221e-07,  1.1642e-08,  1.0803e-07, -3.7666e-10,  1.4142e-06,\n",
      "          7.6693e-07,  3.0200e-06,  7.0067e-08,  2.0231e-07, -7.9244e-09,\n",
      "         -5.3173e-08,  1.5927e-06, -3.9098e-07, -6.1793e-06,  1.7040e-06,\n",
      "          9.8423e-01,  2.7452e-05,  1.7465e-07, -1.0557e-06,  8.2267e-07,\n",
      "          3.4884e-06,  1.6859e-08, -3.3922e-08,  1.4791e-06,  1.2068e-07,\n",
      "         -9.7568e-06, -3.2518e-06,  4.7711e-07,  9.9670e-09,  5.8383e-07,\n",
      "          1.2228e-07, -9.5622e-07, -2.2258e-06,  8.2155e-08,  2.1380e-09,\n",
      "          1.7513e-05,  2.3062e-09,  1.6292e-06, -1.3576e-05, -1.3346e-08,\n",
      "         -1.1403e-05,  6.5120e-06,  3.0704e-06,  2.1401e-09,  6.2560e-08,\n",
      "         -4.0647e-06, -2.6304e-07, -1.6131e-09, -1.7027e-09, -4.3810e-07,\n",
      "          1.4841e-09, -3.6633e-09,  8.5424e-07, -7.7817e-08,  1.1856e-05,\n",
      "         -4.5291e-08, -7.8146e-08, -1.2493e-07, -8.6979e-07,  3.5774e-07,\n",
      "         -1.3665e-06, -3.2400e-05,  5.3502e-08, -2.7215e-07, -1.1221e-08,\n",
      "         -4.9856e-06, -2.9023e-07,  2.1776e-06,  1.1415e-07,  1.9350e-07,\n",
      "          3.3315e-08, -4.5706e-06,  3.2977e-07,  6.2014e-09, -3.5628e-08,\n",
      "          2.0460e-05,  4.8462e-07,  8.1533e-08, -3.6662e-07,  2.0218e-09,\n",
      "          7.0092e-08,  3.0499e-08,  7.0994e-08, -3.7688e-07, -2.0100e-07,\n",
      "         -2.4224e-06, -3.0062e-07,  6.4301e-09,  1.7713e-07,  1.7732e-08,\n",
      "          1.2725e-06, -2.0990e-06, -2.3488e-06,  2.7489e-07,  4.7020e-08,\n",
      "          9.2555e-09, -7.3256e-07,  5.2418e-06,  1.0914e-06, -3.8729e-08,\n",
      "         -6.2730e-09, -2.5959e-05,  3.0323e-08, -3.1691e-08, -3.0952e-06,\n",
      "         -7.1134e-08, -1.1104e-05, -9.5722e-10, -1.2150e-06,  1.5515e-06,\n",
      "          1.2665e-05,  6.3942e-08,  2.4215e-07,  2.9230e-08, -1.2449e-06,\n",
      "          1.6332e-08,  1.0883e-07, -7.7854e-06, -3.6778e-09,  5.9814e-07,\n",
      "          1.9523e-07, -3.4544e-06,  7.8216e-06,  2.3388e-08, -1.7143e-08,\n",
      "         -1.9323e-11,  1.6839e-08,  4.3686e-06,  3.1069e-07, -7.6256e-07,\n",
      "          1.5286e-07, -2.1125e-06,  6.6289e-07, -1.1114e-06,  1.2395e-06,\n",
      "          8.5758e-07, -4.6003e-08, -1.8927e-07,  1.7908e-06, -2.7861e-06,\n",
      "         -8.5419e-07, -8.8965e-07,  6.0976e-06, -3.9715e-09,  1.3647e-06,\n",
      "         -4.1089e-09, -1.9184e-06,  1.7754e-07, -3.6437e-09,  1.3646e-06,\n",
      "         -2.1421e-07,  4.9712e-07, -1.6457e-07, -1.1139e-09,  1.1899e-05,\n",
      "          3.0686e-06, -7.0200e-08, -2.1567e-09,  5.1912e-07,  7.9358e-07,\n",
      "          9.0897e-08, -1.0681e-06, -6.9679e-07, -2.8017e-06,  5.5134e-07,\n",
      "          2.0624e-06, -2.3587e-07, -4.5886e-08, -8.4280e-07,  1.0673e-06,\n",
      "         -1.8540e-06, -2.1284e-05, -9.6752e-07,  5.3210e-07, -1.7008e-08,\n",
      "         -4.9908e-09,  5.7303e-06, -2.1034e-08, -4.9446e-07,  8.4108e-09,\n",
      "         -5.2378e-07,  1.9581e-06,  2.2143e-06,  2.0807e-06, -6.4223e-08,\n",
      "         -6.8584e-09,  1.6887e-05, -2.4014e-06,  1.1179e-08,  7.9187e-07,\n",
      "         -4.5124e-09, -5.1839e-09,  4.0340e-08, -1.7649e-08, -5.3966e-07,\n",
      "          5.3172e-08,  6.7144e-09, -3.8797e-06,  1.4236e-05,  1.8687e-09,\n",
      "         -4.7288e-06,  1.8710e-06, -2.2767e-06,  6.0807e-07,  5.2314e-07,\n",
      "         -6.8247e-07,  1.9711e-07,  2.9914e-09,  4.1123e-09,  3.1156e-08,\n",
      "         -5.2480e-07, -3.5600e-07, -4.8378e-06, -1.2834e-07, -1.2547e-05,\n",
      "         -1.0290e-08,  5.6585e-06, -3.3979e-07, -1.3877e-07, -1.1747e-06,\n",
      "          1.5072e-05, -9.8845e-06,  4.2046e-08, -7.2434e-07, -2.0548e-06,\n",
      "         -1.4187e-08,  6.4650e-08, -3.3493e-08,  1.6097e-06, -4.5861e-06,\n",
      "          7.6159e-06, -1.8508e-05, -9.9948e-08, -3.9296e-07,  1.9840e-07,\n",
      "          2.3081e-07,  1.1104e-07,  2.1113e-09, -1.1768e-08, -6.2181e-06,\n",
      "         -8.1293e-06, -7.4773e-09,  2.4156e-06,  1.0268e-06, -9.1234e-08,\n",
      "          7.1888e-07,  5.6687e-08,  4.1021e-07, -2.8901e-07, -2.8097e-06,\n",
      "          2.0052e-07,  8.4250e-09, -5.3119e-07,  9.5986e-06,  6.4821e-07,\n",
      "         -9.2822e-08,  4.2676e-06,  4.8243e-08, -1.7216e-09, -6.7168e-09,\n",
      "         -1.4064e-06, -1.8053e-07, -7.1368e-07,  2.0127e-06,  1.2534e-05,\n",
      "          5.8101e-09,  5.8645e-08,  6.8651e-08, -1.5118e-06,  6.0172e-08,\n",
      "         -5.5409e-06,  2.6458e-06,  1.0249e-06, -4.0312e-07, -2.4770e-08,\n",
      "         -1.2278e-06, -4.2247e-08, -1.3175e-07,  7.6881e-08, -2.7465e-07,\n",
      "         -4.7375e-07,  3.1335e-06, -1.9965e-06,  1.5792e-07, -1.3974e-07,\n",
      "         -3.3766e-06, -2.4819e-06,  1.2316e-06, -5.1330e-06, -7.3933e-09,\n",
      "          8.2010e-09, -2.4629e-07,  8.1276e-08, -6.3152e-06,  6.2905e-06,\n",
      "          1.4933e-06, -1.5516e-08, -4.6201e-07, -1.0308e-06,  2.2402e-06,\n",
      "         -9.8734e-09,  5.8308e-06, -1.0452e-07, -1.2605e-05, -1.6993e-08,\n",
      "          5.3595e-07, -1.2721e-08, -3.2353e-07, -5.1091e-07,  1.5968e-06,\n",
      "         -1.0109e-08, -9.8909e-08,  3.1079e-06, -3.1707e-08,  5.3091e-08,\n",
      "          3.4085e-08,  7.3934e-06,  5.8139e-07, -2.2288e-07, -2.9042e-08,\n",
      "         -1.2588e-06,  7.2942e-08, -4.0295e-06, -1.8750e-05,  2.0236e-07,\n",
      "          6.6112e-08,  2.4739e-07, -4.4595e-09, -3.4862e-07, -1.2931e-07,\n",
      "         -6.8953e-08,  4.7769e-07, -1.7555e-09, -2.3661e-08, -7.2108e-06,\n",
      "          8.3984e-10, -2.6426e-07, -1.1538e-06, -2.7866e-06,  2.5613e-09,\n",
      "          4.4275e-08, -9.1493e-06,  4.9005e-06,  3.3433e-07, -4.6391e-09,\n",
      "         -1.6358e-08,  4.8771e-10,  1.9628e-06,  2.0868e-08, -1.1452e-09,\n",
      "          1.0789e-05, -8.1685e-07,  6.9233e-08, -1.6228e-06, -8.3381e-06,\n",
      "          4.2359e-07, -9.8737e-08, -3.9379e-07,  6.2150e-07, -6.1683e-09,\n",
      "          5.1695e-09,  1.0698e-06,  2.2504e-07,  8.3911e-08, -2.0471e-08,\n",
      "          2.9494e-06,  4.5939e-06,  5.2381e-07, -1.1953e-08,  3.1559e-06,\n",
      "          4.5402e-07,  7.1209e-08,  3.8711e-06,  2.8162e-07,  3.4351e-07,\n",
      "          5.4973e-07, -6.0966e-08, -2.7282e-06, -1.2201e-07, -1.9994e-05,\n",
      "          4.2685e-09, -4.3387e-07,  9.0800e-08, -1.2749e-08,  7.6621e-09,\n",
      "          3.0819e-06,  1.2030e-06,  1.7036e-07, -3.6000e-08, -4.8558e-07,\n",
      "          5.1660e-09, -2.8738e-07, -9.7188e-06, -1.2786e-05, -3.2561e-08,\n",
      "          4.2291e-08,  9.5755e-08,  1.1112e-06,  6.5053e-07,  1.0484e-08,\n",
      "         -1.4829e-07, -1.2533e-08, -1.5670e-07,  3.1863e-06,  3.0952e-08,\n",
      "          9.6546e-07,  1.6821e-06,  3.8237e-07,  1.9186e-06,  1.1185e-05,\n",
      "         -3.9869e-06,  3.1509e-06, -1.8149e-10, -1.7585e-08, -8.7776e-06,\n",
      "          4.1415e-06,  9.4777e-08,  6.7557e-09, -5.8614e-08,  3.9178e-06,\n",
      "          9.4860e-08,  1.5651e-08, -1.1658e-08,  2.0229e-08, -1.7113e-07,\n",
      "          1.6917e-06,  5.8614e-09, -7.4751e-07, -5.5624e-07, -3.4248e-08,\n",
      "          1.4608e-06,  2.4114e-08, -5.9332e-08, -1.1684e-06, -8.9777e-08,\n",
      "         -2.2686e-06, -1.4299e-06,  2.0697e-07,  6.4309e-08,  1.5860e-06,\n",
      "         -1.4318e-05,  1.2254e-05,  2.0471e-09, -9.0062e-10,  1.3267e-07,\n",
      "         -4.2924e-09, -7.4198e-06, -6.0113e-06, -4.6402e-06,  1.5868e-08,\n",
      "         -3.1209e-08,  3.3039e-09, -1.4343e-08, -1.8774e-08, -1.2126e-06,\n",
      "         -1.1641e-06, -3.0544e-08, -1.1885e-07, -6.1869e-06,  7.0414e-08,\n",
      "          6.2661e-08, -1.2972e-06, -2.6641e-07,  2.8257e-06,  2.4926e-09,\n",
      "          1.0417e-07, -9.6318e-10,  3.1011e-07,  2.7434e-08,  4.7637e-09,\n",
      "          2.3793e-07, -1.4570e-09,  1.4453e-07,  2.3831e-09, -7.1903e-08,\n",
      "          1.3749e-07, -1.7100e-07,  4.5404e-07,  1.2744e-07, -5.2757e-06,\n",
      "          1.1934e-07, -2.1777e-06,  8.7637e-10,  2.5438e-07, -3.5818e-06,\n",
      "          1.7125e-05,  3.0114e-06, -4.0203e-07, -8.8336e-08, -1.4575e-05,\n",
      "          1.2605e-07, -1.3164e-05, -1.1944e-06,  2.0217e-05,  4.4334e-07,\n",
      "          6.0199e-08,  5.1977e-06,  1.2602e-05, -5.8742e-07, -7.8145e-07,\n",
      "          3.6207e-06,  2.0851e-09, -2.3449e-08,  1.0228e-07, -8.4813e-09,\n",
      "          1.2786e-06,  1.7770e-07,  5.7144e-08, -3.6064e-06, -2.2387e-06,\n",
      "         -2.2297e-07,  1.1394e-07, -1.4076e-06, -5.6431e-06,  1.4078e-07,\n",
      "         -4.2442e-07, -4.1291e-08,  7.5076e-08, -9.2145e-08, -4.8533e-09,\n",
      "         -7.4494e-07, -1.5917e-05,  3.6980e-06,  1.4703e-05,  5.4332e-08,\n",
      "          1.8431e-09,  2.2992e-06, -3.8852e-08,  4.8152e-08,  5.5083e-08,\n",
      "         -6.9330e-08,  1.9823e-06,  3.0828e-07,  1.0522e-07, -2.6684e-07,\n",
      "         -1.1410e-07, -1.7042e-06,  8.0030e-07, -3.0325e-06,  2.3845e-07,\n",
      "         -3.4368e-07, -6.7451e-06, -3.1775e-07, -1.3971e-08, -2.5450e-07,\n",
      "         -2.1541e-09, -6.8670e-07,  1.0022e-07, -6.7426e-07, -9.5519e-08,\n",
      "          1.0256e-07,  5.3114e-07, -1.1518e-08, -2.0892e-06,  2.4705e-08,\n",
      "          4.3903e-06, -6.9322e-09, -8.8512e-08, -6.1111e-08,  2.9449e-09,\n",
      "          1.9286e-06,  1.1984e-07, -2.4867e-06,  5.6097e-08,  2.3104e-08,\n",
      "         -3.9946e-09,  5.7752e-06, -2.1010e-07,  8.7603e-09, -2.6734e-06,\n",
      "         -1.5607e-07, -1.0976e-08,  9.3335e-08,  2.5855e-07,  1.8419e-07,\n",
      "         -6.0773e-06, -4.0655e-08, -9.0636e-06, -1.2806e-07, -2.9104e-06,\n",
      "          4.1481e-06,  4.2486e-06,  1.2737e-07,  5.1898e-08, -2.2605e-05,\n",
      "         -6.5472e-07, -1.0680e-08, -4.6648e-07,  2.4734e-08, -6.1260e-06,\n",
      "         -5.1410e-07, -2.0926e-05,  2.9158e-06,  6.9413e-08,  4.8132e-07,\n",
      "         -8.5947e-07,  3.9749e-07,  6.9227e-07]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 6, 11],\n",
      "         [ 6,  2],\n",
      "         [ 5,  8],\n",
      "         [ 8, 13],\n",
      "         [ 5,  9],\n",
      "         [ 2, 10],\n",
      "         [ 4, 12],\n",
      "         [ 2,  7],\n",
      "         [ 8, 15],\n",
      "         [ 2, 17],\n",
      "         [ 7, 11],\n",
      "         [ 9, 11],\n",
      "         [ 4, 10],\n",
      "         [ 5,  5],\n",
      "         [ 0,  3],\n",
      "         [ 1, 11],\n",
      "         [ 1,  4],\n",
      "         [ 0,  9],\n",
      "         [ 3, 13],\n",
      "         [ 0,  8],\n",
      "         [ 3, 17],\n",
      "         [ 4,  8],\n",
      "         [10, 11],\n",
      "         [ 2, 16],\n",
      "         [ 3, 16],\n",
      "         [ 1, 15],\n",
      "         [ 9, 14],\n",
      "         [ 5, 11],\n",
      "         [ 9,  2],\n",
      "         [ 3, 12],\n",
      "         [10, 15],\n",
      "         [ 2,  8],\n",
      "         [ 1,  1],\n",
      "         [ 3,  2],\n",
      "         [ 3,  9],\n",
      "         [ 5,  1],\n",
      "         [ 8, 16],\n",
      "         [ 1,  2],\n",
      "         [ 0,  5],\n",
      "         [ 3,  6],\n",
      "         [ 4, 11],\n",
      "         [ 2, 11],\n",
      "         [10,  9],\n",
      "         [ 7,  1],\n",
      "         [ 1,  5],\n",
      "         [ 5,  4],\n",
      "         [ 4, 13],\n",
      "         [ 7, 12],\n",
      "         [ 0,  6],\n",
      "         [ 0,  1],\n",
      "         [ 8,  1],\n",
      "         [10,  6],\n",
      "         [ 8, 18],\n",
      "         [ 4,  4],\n",
      "         [10,  4],\n",
      "         [ 6, 18],\n",
      "         [ 6, 15],\n",
      "         [ 8,  3],\n",
      "         [ 9,  4],\n",
      "         [ 6,  7],\n",
      "         [10, 14],\n",
      "         [ 1,  6],\n",
      "         [ 8, 10],\n",
      "         [ 3,  0],\n",
      "         [ 5, 13],\n",
      "         [ 4,  5],\n",
      "         [ 7, 10],\n",
      "         [ 5, 18],\n",
      "         [10,  8],\n",
      "         [10, 16],\n",
      "         [ 2,  3],\n",
      "         [ 9, 13],\n",
      "         [10, 17],\n",
      "         [ 2,  2],\n",
      "         [ 6, 14],\n",
      "         [ 8,  5],\n",
      "         [ 5,  2],\n",
      "         [ 9, 17],\n",
      "         [ 7,  0],\n",
      "         [ 9,  0],\n",
      "         [ 7,  5],\n",
      "         [ 4,  6],\n",
      "         [ 7, 17],\n",
      "         [ 5, 17],\n",
      "         [ 3, 15],\n",
      "         [ 8,  6],\n",
      "         [ 7, 13],\n",
      "         [ 8, 14],\n",
      "         [ 5, 12],\n",
      "         [ 7,  9],\n",
      "         [ 6,  4],\n",
      "         [ 7,  4],\n",
      "         [ 6,  3],\n",
      "         [ 4,  2],\n",
      "         [ 9, 10],\n",
      "         [ 2,  4],\n",
      "         [ 5,  7],\n",
      "         [ 3,  4],\n",
      "         [ 4,  1],\n",
      "         [ 2,  6],\n",
      "         [ 0,  0],\n",
      "         [ 2, 15],\n",
      "         [ 9,  8],\n",
      "         [ 0, 18],\n",
      "         [ 1,  9],\n",
      "         [ 9,  1],\n",
      "         [ 9, 15],\n",
      "         [10,  5],\n",
      "         [10,  1],\n",
      "         [ 2,  5],\n",
      "         [ 7, 15],\n",
      "         [ 8,  2],\n",
      "         [ 2,  0],\n",
      "         [ 1,  7],\n",
      "         [ 0, 17],\n",
      "         [ 5,  0],\n",
      "         [10, 12],\n",
      "         [ 3,  1],\n",
      "         [ 6, 17],\n",
      "         [ 5, 16],\n",
      "         [ 4, 18],\n",
      "         [ 0,  7],\n",
      "         [ 2,  1],\n",
      "         [10, 10],\n",
      "         [ 0, 16],\n",
      "         [ 0, 12],\n",
      "         [10,  7],\n",
      "         [ 6,  6],\n",
      "         [10,  0],\n",
      "         [ 1, 10],\n",
      "         [ 4, 14],\n",
      "         [ 4, 17],\n",
      "         [ 1, 18],\n",
      "         [ 2, 12],\n",
      "         [ 3,  7],\n",
      "         [ 1, 13],\n",
      "         [ 2, 13],\n",
      "         [ 4,  9],\n",
      "         [ 7,  6],\n",
      "         [ 9, 16],\n",
      "         [ 5, 15],\n",
      "         [ 7, 16],\n",
      "         [ 6,  8],\n",
      "         [ 6,  5],\n",
      "         [ 0,  4],\n",
      "         [ 3, 10],\n",
      "         [ 1, 16],\n",
      "         [10,  2],\n",
      "         [ 1, 14],\n",
      "         [ 8,  7],\n",
      "         [ 7,  3],\n",
      "         [ 8,  4],\n",
      "         [ 0, 13],\n",
      "         [ 6, 10],\n",
      "         [ 3, 14],\n",
      "         [ 6,  1],\n",
      "         [10, 13],\n",
      "         [ 0, 14],\n",
      "         [ 9,  7],\n",
      "         [ 0, 11],\n",
      "         [10, 18],\n",
      "         [ 4,  3],\n",
      "         [ 0,  2],\n",
      "         [ 7, 18],\n",
      "         [ 3, 18],\n",
      "         [ 4, 15],\n",
      "         [ 1,  3],\n",
      "         [ 5,  6],\n",
      "         [ 4,  7],\n",
      "         [ 9,  5],\n",
      "         [ 8,  8],\n",
      "         [ 6, 16],\n",
      "         [ 3, 11],\n",
      "         [ 7,  7],\n",
      "         [ 1, 12],\n",
      "         [ 2, 14],\n",
      "         [ 2, 18],\n",
      "         [ 9,  9],\n",
      "         [ 6,  0],\n",
      "         [ 3,  3],\n",
      "         [ 8,  9],\n",
      "         [ 8, 12],\n",
      "         [ 2,  9],\n",
      "         [ 6,  9],\n",
      "         [ 0, 10],\n",
      "         [ 9, 12],\n",
      "         [ 8, 11],\n",
      "         [ 9,  6],\n",
      "         [ 7, 14],\n",
      "         [ 7,  2],\n",
      "         [ 9, 18],\n",
      "         [ 1, 17],\n",
      "         [ 5, 10],\n",
      "         [ 6, 13],\n",
      "         [ 8,  0],\n",
      "         [ 9,  3],\n",
      "         [ 4,  0],\n",
      "         [ 3,  8],\n",
      "         [ 3,  5],\n",
      "         [ 1,  0],\n",
      "         [ 6, 12],\n",
      "         [ 5,  3],\n",
      "         [ 0, 15],\n",
      "         [ 4, 16],\n",
      "         [ 8, 17],\n",
      "         [ 1,  8],\n",
      "         [ 7,  8],\n",
      "         [ 5, 14],\n",
      "         [10,  3]]]), (11, 19)), 'cls_output': tensor([[0.4540]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junsheng/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples=[\n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\", #0\n",
    "            \n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-07-18-04-22-30-preset-18.jpeg\", # 3\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "n = 1\n",
    "sensor = torch.rand(config.senser_input_num)\n",
    "# sensor = torch.ones(config.senser_input_num)\n",
    "print(sensor)\n",
    "sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "out = infer(examples[0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.4540]], device='cuda:0')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4539965\n"
     ]
    }
   ],
   "source": [
    "print(out[0].cpu().numpy()[0][0])\n",
    "#0.00031266143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test by valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择三组生长期不同的数据去验证训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.query(\"fold==0\").reset_index(drop=True)\n",
    "df_test.to_csv(\"test_by_valid.csv\",index=False)\n",
    "sensor_test_list = df_test.sensor.tolist()\n",
    "image_test_list = df_test.image_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang.ipynb Cell 57\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m sensor \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mtensor(sensor_test_list[idx])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m out \u001b[39m=\u001b[39m infer(image_test_list[idx],sensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'infer' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 64\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.0827e-06, -1.7530e-07,  1.5452e-07, -2.2674e-08, -2.2496e-06,\n",
      "           3.2150e-06,  6.0465e-06, -5.3801e-09, -1.8847e-07, -7.7690e-07,\n",
      "          -1.5788e-07,  3.6213e-06, -3.8558e-08, -1.0063e-05, -1.4680e-08,\n",
      "          -1.8126e-07, -2.7730e-06,  9.1713e-09, -2.7553e-06, -5.3852e-09,\n",
      "           9.5103e-07,  3.8168e-09,  7.2278e-08, -1.0654e-08,  6.3138e-07,\n",
      "           3.1689e-06, -2.6612e-06, -1.0689e-09,  2.8926e-06,  4.4980e-06,\n",
      "          -9.2651e-06,  1.0652e-06, -4.1821e-07,  2.2096e-08, -9.7153e-07,\n",
      "           1.0851e-06,  7.0076e-08, -7.6146e-08, -2.9167e-09,  2.1692e-06,\n",
      "           7.3493e-08,  8.4667e-06,  3.4533e-08, -2.6099e-07, -4.6151e-06,\n",
      "           1.3888e-08,  3.3025e-07, -5.3568e-06, -2.0778e-06, -2.7470e-06,\n",
      "           1.0874e-07, -4.3026e-06, -3.1775e-08, -1.3380e-09, -2.3559e-06,\n",
      "          -1.7084e-09, -9.4255e-06,  2.6611e-06,  7.8697e-07, -5.3748e-08,\n",
      "           1.0649e-05, -2.6270e-07, -3.8041e-07,  1.9315e-08, -7.3593e-08,\n",
      "          -4.4816e-06,  5.6049e-10,  8.2062e-07, -1.0063e-08,  4.5128e-08,\n",
      "          -5.1747e-09, -4.5403e-07, -2.9219e-08,  7.7394e-07,  2.2411e-07,\n",
      "          -3.3084e-07,  2.4516e-09,  3.5821e-10, -1.8521e-09,  1.8354e-07,\n",
      "          -1.8954e-05,  2.7999e-10, -8.5999e-08, -2.1377e-06, -1.6716e-06,\n",
      "           1.6572e-08, -2.3315e-06,  1.8895e-07,  6.3496e-06, -3.1833e-07,\n",
      "           6.2023e-07,  8.4298e-10, -8.2967e-07,  3.7212e-06,  1.1039e-06,\n",
      "          -9.0300e-09, -2.6949e-08, -8.5791e-10, -1.1635e-08, -7.3959e-09,\n",
      "          -9.7975e-06,  4.5500e-07, -5.4868e-06,  4.2644e-06,  3.0767e-06,\n",
      "           4.9342e-06,  4.9311e-07,  1.9163e-05,  2.2566e-08, -1.8834e-08,\n",
      "           2.0761e-06,  2.9012e-06,  3.2295e-07, -7.7596e-08,  1.3192e-07,\n",
      "           5.7596e-06, -3.6992e-05, -3.0007e-07,  4.7662e-08, -3.7218e-07,\n",
      "          -5.0134e-06,  3.3447e-08, -4.8486e-06,  5.2302e-07,  3.9796e-06,\n",
      "           6.6157e-06,  1.0902e-06,  2.3589e-08,  1.0390e-07,  1.5728e-07,\n",
      "           1.2887e-05, -8.6768e-07, -2.0269e-07,  7.0103e-07,  1.9808e-05,\n",
      "           9.1876e-08, -1.8358e-07, -3.2414e-05, -8.2112e-06,  4.8515e-08,\n",
      "          -1.7336e-05,  3.3777e-06, -4.5661e-07,  3.0050e-06,  1.6503e-09,\n",
      "          -7.2160e-09,  7.9344e-07, -5.0495e-09,  2.4069e-07, -4.3610e-06,\n",
      "           2.7735e-06,  2.7815e-07, -2.0350e-07,  1.6112e-08, -2.0510e-06,\n",
      "          -4.1556e-06, -1.0095e-06, -4.1982e-06,  9.7768e-06,  3.5272e-08,\n",
      "           4.1931e-06,  3.6369e-07, -7.5657e-08,  1.3493e-06, -9.1426e-06,\n",
      "           1.5492e-06, -5.5606e-08,  2.8326e-10, -6.2274e-06,  1.1553e-07,\n",
      "           1.3244e-07,  1.1595e-08,  1.0701e-07, -1.7686e-10,  1.3434e-06,\n",
      "           7.6663e-07,  3.0214e-06,  3.1273e-06,  1.8162e-07, -8.7588e-09,\n",
      "          -5.3203e-08,  1.3994e-06, -3.9140e-07, -5.9820e-06,  1.8577e-06,\n",
      "          -2.9392e+00,  2.7478e-05,  1.7482e-07, -1.0052e-06,  1.1428e-06,\n",
      "           3.5725e-06,  1.6826e-08, -3.2411e-08,  1.3470e-06,  9.9076e-08,\n",
      "          -9.7098e-06, -3.4484e-06,  5.2059e-07,  9.9433e-09, -1.4315e-06,\n",
      "           1.1571e-07, -1.0162e-06, -1.9451e-06,  1.3364e-07,  2.0557e-09,\n",
      "           1.8360e-05,  2.3615e-09,  1.5429e-06, -1.3612e-05, -1.3375e-08,\n",
      "          -1.1263e-05,  6.5371e-06,  3.0841e-06,  2.1411e-09,  6.0650e-08,\n",
      "          -3.8018e-06, -2.6458e-07,  3.9600e-09, -1.6493e-09, -4.5734e-07,\n",
      "           1.6022e-09, -3.6367e-09,  8.5293e-07, -3.3017e-08,  1.1829e-05,\n",
      "          -3.8856e-08, -7.8512e-08, -1.2441e-07, -8.3563e-07,  3.5845e-07,\n",
      "          -1.1632e-06, -3.2378e-05,  5.1766e-08, -3.2791e-07, -1.0741e-08,\n",
      "          -4.8005e-06, -3.0898e-07,  2.0715e-06,  1.1634e-07,  1.9251e-07,\n",
      "          -6.8956e-08,  1.6680e-06,  3.3701e-07,  6.6928e-09, -3.8224e-08,\n",
      "           2.0501e-05,  4.8596e-07,  6.9212e-08, -3.0852e-07,  2.6557e-09,\n",
      "           1.1923e-07, -8.0101e-09,  7.0794e-08, -3.8855e-07, -1.8652e-07,\n",
      "          -2.3786e-06, -1.5048e-08,  1.0523e-08,  1.7478e-07, -1.3772e-08,\n",
      "           1.3210e-06, -1.8784e-06,  2.4997e-06,  3.5084e-07,  3.1842e-10,\n",
      "           9.0063e-09, -7.8309e-07,  5.2683e-06,  1.2784e-05, -4.2034e-08,\n",
      "          -6.3294e-09, -2.5147e-05,  2.9737e-08, -3.2164e-08, -3.1089e-06,\n",
      "          -6.6904e-08, -1.1640e-05, -8.9892e-10, -1.2145e-06,  1.5709e-06,\n",
      "           1.2708e-05,  6.3839e-08,  2.4791e-07,  7.2218e-08, -1.3138e-06,\n",
      "           1.6254e-08,  1.1444e-07, -6.5649e-06, -3.8067e-09,  6.3245e-07,\n",
      "           2.2508e-07, -3.5040e-06,  7.8615e-06,  2.3449e-08, -1.6639e-08,\n",
      "          -1.5493e-08,  1.7303e-08,  4.1829e-06,  3.4123e-07, -7.8461e-07,\n",
      "           1.5215e-07, -2.0604e-06,  6.5877e-07, -1.1104e-06,  1.2191e-06,\n",
      "           8.6716e-07, -1.7774e-08, -2.4495e-07,  1.7908e-06, -1.5804e-06,\n",
      "          -8.3130e-07, -3.4135e-07,  5.9923e-06, -4.0097e-09,  1.3151e-06,\n",
      "          -3.6523e-09, -1.2073e-06,  1.5975e-07, -4.1334e-09,  1.3149e-06,\n",
      "          -1.9978e-07,  1.8486e-07, -1.6621e-07, -1.2257e-09,  1.2581e-05,\n",
      "           3.0089e-06, -6.8363e-08, -2.2873e-09,  4.8389e-07,  7.7758e-07,\n",
      "           9.1260e-08, -1.0340e-06, -6.4681e-07, -2.7343e-06,  6.2495e-07,\n",
      "           2.1049e-06, -2.5364e-07, -4.6147e-08, -8.3495e-07,  2.7527e-08,\n",
      "          -1.7728e-06, -2.1282e-05, -9.3445e-07,  5.4204e-07, -1.6911e-08,\n",
      "          -4.9603e-09,  6.9281e-06, -2.1246e-08,  2.5717e-06,  1.0584e-08,\n",
      "          -9.4637e-08,  2.0147e-06,  2.3032e-06,  2.3860e-06, -2.1552e-08,\n",
      "          -6.6082e-09,  1.6799e-05, -2.3743e-06,  1.1095e-08,  9.3728e-07,\n",
      "          -4.6303e-09, -4.5186e-09, -6.7962e-09, -1.7106e-08, -5.3569e-07,\n",
      "           3.9490e-08,  6.3466e-09, -3.9532e-06,  1.4323e-05,  1.4552e-09,\n",
      "          -4.7709e-06,  1.8672e-06, -2.3148e-06,  4.7150e-07,  5.4807e-07,\n",
      "          -6.5502e-07,  1.8981e-07,  2.8571e-09,  4.2063e-09,  3.1712e-08,\n",
      "          -5.6101e-07, -3.3603e-07, -4.8259e-06, -1.1903e-07, -1.2574e-05,\n",
      "          -1.0291e-08,  5.6589e-06, -3.3860e-07, -1.4214e-07, -1.1464e-06,\n",
      "           1.4969e-05, -9.9780e-06,  4.2101e-08, -4.2471e-07, -1.9702e-06,\n",
      "          -1.4949e-08,  6.7702e-08, -3.3593e-08,  1.7504e-06, -4.4923e-06,\n",
      "           7.6709e-06, -3.6925e-04, -1.0527e-07, -3.6139e-07,  2.1184e-07,\n",
      "           2.2556e-07,  1.1836e-07,  2.1052e-09, -1.2651e-08, -6.2661e-06,\n",
      "          -9.0347e-06, -6.6332e-09,  2.4092e-06,  1.0344e-06, -9.1272e-08,\n",
      "           7.9553e-07,  5.8131e-08,  3.6408e-07, -6.0187e-07, -2.8670e-06,\n",
      "           2.0006e-07,  8.7373e-09, -2.4830e-06,  9.6273e-06,  6.2010e-07,\n",
      "          -9.2130e-08,  4.0188e-06,  4.9999e-08, -2.8333e-09, -7.3779e-09,\n",
      "          -1.2982e-06, -5.7213e-07, -2.5262e-06,  2.0549e-06,  1.2422e-05,\n",
      "           3.0170e-09,  6.0589e-08,  1.7087e-07, -1.6573e-06,  5.5917e-08,\n",
      "          -7.3781e-06,  6.5990e-06,  1.4642e-06, -4.5863e-07, -2.4139e-08,\n",
      "          -1.1933e-06, -3.2544e-08, -1.3277e-07, -1.0180e-07, -3.4549e-07,\n",
      "          -4.7493e-07,  3.0314e-06, -1.9502e-06,  1.6607e-07, -1.4059e-07,\n",
      "          -3.2411e-06, -1.2351e-06,  1.3018e-06, -5.1746e-06, -6.0567e-09,\n",
      "           8.0995e-09, -2.5051e-07,  6.3932e-08, -7.1051e-06,  6.2887e-06,\n",
      "           1.5063e-06, -1.3608e-08, -5.9592e-07, -1.0121e-06,  2.2344e-06,\n",
      "          -9.7173e-09,  5.5005e-06,  8.0114e-07, -1.3357e-05, -1.6829e-08,\n",
      "           5.2020e-07, -1.0246e-08, -3.6627e-07, -6.9186e-07,  1.5120e-06,\n",
      "          -1.1171e-08, -8.9179e-08,  3.1674e-06, -3.2056e-08,  7.5366e-08,\n",
      "           3.3973e-08,  7.3093e-06,  6.1867e-07, -2.1678e-07, -3.1126e-08,\n",
      "          -1.4283e-06,  8.8902e-08, -4.0483e-06, -1.8835e-05,  2.0241e-07,\n",
      "           6.6565e-08,  2.4728e-07, -4.9105e-09, -3.6491e-07,  4.1599e-09,\n",
      "          -6.8935e-08,  4.7498e-07, -1.4405e-09, -2.3634e-08, -6.7978e-06,\n",
      "           6.5777e-10, -2.2546e-07, -1.1316e-06, -2.4625e-06,  2.6516e-09,\n",
      "           4.7425e-08, -9.3632e-06,  5.2651e-06,  3.2771e-07, -4.6329e-09,\n",
      "          -1.5464e-08, -6.1605e-10,  1.9835e-06,  2.1300e-08, -2.4945e-09,\n",
      "           9.7230e-06, -5.8208e-06,  6.9670e-08, -1.6276e-06, -7.9235e-06,\n",
      "           4.2358e-07, -9.8639e-08, -4.0145e-07,  4.9376e-07, -6.1349e-09,\n",
      "           5.2211e-09,  1.1231e-06, -1.9232e-08,  8.3668e-08, -2.0639e-08,\n",
      "           3.2266e-06,  4.5991e-06,  5.6453e-07, -1.1690e-08,  3.1363e-06,\n",
      "           7.9319e-07,  6.3972e-07,  2.4431e-06,  1.3691e-07,  3.4397e-07,\n",
      "           5.9716e-07, -6.7640e-08,  4.2014e-07, -1.2256e-07, -1.9971e-05,\n",
      "           4.1802e-09,  7.2193e-07,  1.4406e-07, -1.3008e-08,  7.8154e-09,\n",
      "           1.9151e-06,  7.1253e-07,  1.8491e-07, -3.6584e-08, -7.2075e-07,\n",
      "           5.4406e-09, -2.8820e-07, -1.0357e-05, -1.2787e-05, -2.7474e-09,\n",
      "           4.3042e-08,  9.2539e-08,  1.1078e-06,  4.8793e-07,  1.0743e-08,\n",
      "          -4.2308e-07, -1.2427e-08, -1.7520e-07,  3.1959e-06,  3.0790e-08,\n",
      "           8.1962e-07,  1.5018e-06,  3.3025e-07,  1.9033e-06, -2.8258e-05,\n",
      "          -4.0832e-06,  3.2066e-06, -1.3506e-09, -1.7938e-08, -9.5366e-06,\n",
      "           4.1767e-06,  9.4760e-08,  7.0537e-09, -5.8291e-08,  4.0933e-06,\n",
      "           9.3245e-08,  1.4259e-08, -8.7811e-09,  2.0307e-08, -2.2486e-07,\n",
      "           3.6306e-06,  5.9481e-09, -6.3747e-07, -5.8849e-07, -3.4337e-08,\n",
      "           1.4729e-06,  1.7147e-08, -5.9682e-08, -1.2062e-06, -1.2547e-07,\n",
      "          -2.1901e-06, -1.6813e-06,  2.1180e-07,  6.2201e-08,  1.6180e-06,\n",
      "          -1.4157e-05,  1.2282e-05,  1.9979e-09, -1.0967e-09,  1.4225e-07,\n",
      "          -4.2317e-09, -7.5325e-06, -5.8693e-06, -4.7341e-06,  1.6854e-08,\n",
      "          -4.6850e-08,  3.3433e-09, -1.1984e-08, -1.7735e-08, -9.4798e-07,\n",
      "          -1.2299e-06, -2.9764e-08, -1.1842e-07, -6.1587e-06,  7.0409e-08,\n",
      "           6.7254e-08, -2.3478e-06, -2.6517e-07,  2.7608e-06,  2.3073e-09,\n",
      "           7.2569e-08, -7.0159e-10,  3.1021e-07,  2.9071e-08,  4.6543e-09,\n",
      "           1.9451e-07, -1.7354e-09,  1.3059e-07,  2.5021e-09, -6.0248e-08,\n",
      "           1.3928e-07, -1.6684e-07,  4.6277e-07,  1.2854e-07, -5.1473e-06,\n",
      "           1.0356e-07, -1.8853e-06,  8.8526e-10,  2.5783e-07, -3.5606e-06,\n",
      "           1.6781e-05,  3.0114e-06, -4.5087e-07, -9.2964e-08, -1.4801e-05,\n",
      "           1.2632e-07, -1.3177e-05, -5.8259e-07,  2.0617e-05,  4.9409e-07,\n",
      "           6.1154e-08,  5.1906e-06,  1.2295e-05, -5.4655e-07, -6.9090e-07,\n",
      "           3.7675e-06,  5.6017e-09, -2.4236e-08, -6.7151e-08, -8.4430e-09,\n",
      "          -7.9482e-07,  8.9041e-08,  5.2378e-08, -3.9485e-06, -1.8214e-06,\n",
      "          -1.7094e-07,  1.1340e-07, -1.3492e-06, -4.8415e-06,  1.4066e-07,\n",
      "          -4.3767e-07, -4.1373e-08,  7.2860e-08, -8.9656e-08, -4.6104e-09,\n",
      "          -7.4272e-07, -1.5907e-05,  3.6720e-06,  1.5925e-06,  5.7099e-08,\n",
      "           1.7195e-09,  2.5247e-06, -4.4720e-08,  5.2508e-08,  5.4985e-08,\n",
      "           7.8396e-08,  2.1274e-06,  3.1347e-07,  8.5338e-08, -2.8651e-07,\n",
      "          -1.0325e-07, -1.7460e-06,  3.5058e-06, -3.0202e-06,  2.8637e-07,\n",
      "          -4.5763e-07, -6.7738e-06, -3.2277e-07, -2.2951e-08, -2.5471e-07,\n",
      "          -2.1949e-09,  1.7677e-06,  6.9713e-08, -5.6668e-07, -1.1175e-07,\n",
      "           1.0015e-07,  6.1517e-07, -1.1322e-08, -2.1065e-06,  2.4826e-08,\n",
      "           4.2148e-06, -6.9230e-09, -8.9478e-08, -7.0129e-08,  3.3094e-09,\n",
      "           2.1308e-06, -1.3334e-08, -2.2185e-06,  5.5710e-08,  2.2817e-08,\n",
      "          -5.1039e-09,  5.7348e-06, -2.1602e-07,  8.8378e-09, -2.1920e-06,\n",
      "          -1.5670e-07, -7.1959e-09,  1.7429e-07,  3.3435e-07,  1.8197e-07,\n",
      "          -6.0268e-06, -4.1070e-08, -9.2149e-06, -1.2947e-07, -2.9009e-06,\n",
      "           4.0414e-06,  4.2962e-06,  1.2600e-07,  5.2677e-08, -2.4173e-05,\n",
      "          -1.6576e-06, -1.0721e-08, -5.0578e-07,  2.3554e-08, -6.2507e-06,\n",
      "           7.5529e-07, -2.0994e-05,  2.9447e-06,  6.8881e-08,  4.8714e-07,\n",
      "          -7.9799e-07,  3.2075e-07, -4.6073e-06]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1839e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9462e-07,\n",
      "           3.1838e-07, -3.4247e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5569e-07,  ..., -7.9456e-07,\n",
      "           3.1840e-07, -3.4248e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5567e-07,  ..., -7.9454e-07,\n",
      "           3.1838e-07, -3.4248e-06]]], device='cuda:0'), 'cls_feats': tensor([[-6.2369e-02,  6.7438e-02, -6.7392e-02, -3.9794e-02, -6.3808e-02,\n",
      "         -6.7367e-02, -6.6934e-02, -6.7346e-02,  6.7608e-02,  5.9141e-02,\n",
      "          6.7032e-02,  6.7446e-02,  6.5307e-02, -1.1194e-06,  1.1023e-07,\n",
      "          3.8261e-02,  6.6431e-02, -6.3736e-02,  6.7383e-02, -6.7404e-02,\n",
      "          6.4508e-02,  6.7620e-02,  6.7284e-02, -6.6964e-02,  6.2817e-02,\n",
      "          6.7301e-02, -6.7417e-02,  6.7087e-02,  6.6913e-02,  6.7109e-02,\n",
      "         -6.7469e-02,  2.8371e-02,  4.2128e-02, -1.6269e-03,  6.7079e-02,\n",
      "          6.6979e-02, -6.6333e-02, -6.7239e-02, -6.6659e-02,  1.1936e-08,\n",
      "          2.0455e-03,  6.4305e-02, -6.7023e-02, -6.6528e-02,  6.7345e-02,\n",
      "         -6.7515e-02, -6.7175e-02, -6.7290e-02, -6.2325e-02,  6.7243e-02,\n",
      "          6.6173e-02, -6.7563e-02, -6.7561e-02, -6.6837e-02, -6.7201e-02,\n",
      "          6.3929e-02,  6.6990e-02,  6.7497e-02,  6.7174e-02, -6.2646e-02,\n",
      "         -6.7547e-02,  6.8752e-08,  6.7104e-02, -6.5181e-02, -6.7592e-02,\n",
      "          3.9844e-03,  3.7063e-02, -6.4575e-02,  3.7483e-02,  6.6982e-02,\n",
      "          6.7271e-02, -4.6332e-09,  6.7342e-02,  6.2820e-02, -4.1235e-02,\n",
      "         -6.7368e-02, -6.6958e-02, -6.7113e-02,  6.7086e-02, -6.6908e-02,\n",
      "          6.7395e-02,  6.7414e-02, -2.9495e-03,  6.7512e-02,  6.1731e-02,\n",
      "         -2.2837e-09,  6.7376e-02,  6.2175e-02, -6.6956e-02,  6.4848e-02,\n",
      "          1.7535e-02,  3.6893e-02, -6.5943e-02,  6.4621e-02, -6.2865e-02,\n",
      "          6.2686e-02,  6.7344e-02,  6.7112e-02, -6.3725e-02, -6.7618e-02,\n",
      "         -4.3014e-02, -6.7607e-02, -7.1798e-07, -6.6797e-02, -6.7291e-02,\n",
      "         -2.3954e-07,  6.3011e-08, -1.6364e-08,  6.7339e-02, -6.7345e-02,\n",
      "         -7.7168e-03, -6.6899e-02, -6.7301e-02,  6.1825e-02,  1.2671e-03,\n",
      "          6.4608e-02, -2.9232e-04, -2.4108e-02, -6.7459e-02,  6.0292e-07,\n",
      "          6.4932e-02, -6.3537e-02,  1.7254e-05,  6.4727e-02,  6.7343e-02,\n",
      "         -7.2030e-10,  5.9043e-02,  6.7256e-02,  6.7341e-02, -6.6580e-02,\n",
      "          2.3766e-07, -6.7408e-02, -6.7751e-07, -6.3785e-02, -9.5706e-09,\n",
      "         -6.5586e-02,  6.4491e-02,  6.7181e-02,  2.6073e-02,  6.7416e-02,\n",
      "         -7.2806e-11, -1.4449e-03,  8.7101e-04, -6.5215e-02,  6.6613e-02,\n",
      "          1.3377e-03,  6.6549e-02,  6.7473e-02, -6.3610e-02,  6.6497e-02,\n",
      "         -6.6815e-02,  6.7569e-02,  6.7249e-02,  2.1300e-09, -6.7339e-02,\n",
      "         -7.5100e-06, -6.3818e-02, -6.7483e-02, -6.4382e-02, -6.7273e-02,\n",
      "          3.0603e-02, -6.7539e-02,  5.7865e-02, -6.2802e-02, -6.6204e-02,\n",
      "          6.4873e-02,  6.7604e-02, -6.2256e-02,  6.6797e-02, -6.7126e-02,\n",
      "          3.8525e-08, -5.7044e-02, -6.4657e-03, -6.4590e-02, -6.7163e-02,\n",
      "          5.8586e-08,  6.7447e-02, -6.7242e-02,  3.4424e-09,  6.1683e-02,\n",
      "         -2.3804e-02,  6.7520e-02,  6.6989e-02, -6.7084e-02,  6.7247e-02,\n",
      "         -6.4425e-02,  5.5586e-02, -7.8754e-13, -6.5376e-02,  6.7446e-02,\n",
      "          6.7162e-02,  6.5131e-02, -6.4461e-02,  6.7130e-02, -6.2440e-02,\n",
      "          6.7013e-02,  6.3599e-02,  6.6932e-02,  6.6517e-02,  6.7303e-02,\n",
      "         -6.7004e-02, -6.7336e-02,  6.7429e-02,  6.7268e-02,  7.7366e-08,\n",
      "         -5.0665e-02, -6.6271e-02,  6.7131e-02,  6.6349e-02, -6.6659e-02,\n",
      "         -6.4530e-02, -6.7165e-02,  4.9314e-02, -6.7408e-02,  6.6034e-05,\n",
      "         -8.6171e-07, -6.7051e-02,  6.4380e-02, -6.7154e-02,  6.2915e-02,\n",
      "          7.6205e-06,  3.6394e-02,  6.7509e-02, -6.3362e-02,  3.6703e-02,\n",
      "          6.5037e-02,  6.7369e-02,  6.7092e-02,  6.7445e-02,  6.7513e-02,\n",
      "          6.5877e-02, -6.9070e-09,  6.3248e-02, -6.7217e-02, -6.7186e-02,\n",
      "          6.7350e-02, -2.8146e-03, -6.5798e-02,  6.4767e-02,  6.7612e-02,\n",
      "          6.5310e-02,  6.4819e-02,  6.6173e-02,  6.7063e-02, -2.7954e-02,\n",
      "          6.6891e-02, -2.4323e-03,  6.6283e-02, -3.5880e-02,  6.6900e-02,\n",
      "          6.6905e-02,  6.7645e-02,  5.3848e-04,  6.2569e-02,  6.3214e-02,\n",
      "         -6.7352e-02, -5.6301e-04, -3.8800e-05, -6.7625e-02,  6.6964e-02,\n",
      "         -6.6986e-02, -6.7418e-02,  6.1452e-02,  1.4334e-07, -6.7413e-02,\n",
      "         -6.0843e-02,  6.7342e-02,  6.4143e-02,  6.5083e-02,  6.4727e-02,\n",
      "          6.6866e-09, -6.7189e-02,  5.7467e-02, -6.6783e-02,  6.7541e-02,\n",
      "         -6.6345e-02,  6.3640e-02,  6.7420e-02,  6.7416e-02,  6.0693e-02,\n",
      "         -6.7499e-02, -6.4516e-02,  6.5031e-02,  6.7353e-02,  9.6023e-06,\n",
      "         -6.7508e-02, -6.3883e-03, -6.5111e-02,  2.7676e-02, -6.5434e-02,\n",
      "         -6.5638e-02,  3.1789e-02, -6.7198e-02, -6.7201e-02, -3.2726e-06,\n",
      "         -6.6969e-02, -6.6967e-02, -6.2186e-02, -6.6638e-02,  6.3329e-02,\n",
      "         -6.7157e-02, -6.7101e-02,  6.6704e-02,  6.4811e-02, -6.7486e-02,\n",
      "          6.7260e-02,  1.7546e-07, -5.8865e-05,  6.7277e-02,  7.4857e-07,\n",
      "          5.3150e-02,  5.9514e-02, -4.8562e-07, -6.6338e-02,  6.7382e-02,\n",
      "         -6.7597e-02,  6.7519e-02,  6.7017e-02, -6.6577e-02, -5.2690e-03,\n",
      "          3.9099e-08,  6.7325e-02,  6.7514e-02, -6.7615e-02,  6.3006e-02,\n",
      "         -6.7263e-02, -6.6503e-02, -6.7047e-02, -2.6836e-02, -6.4434e-02,\n",
      "         -6.7331e-02, -6.4508e-02,  6.5750e-02,  5.6114e-08, -6.7032e-02,\n",
      "          6.7208e-02,  9.6642e-05,  3.8482e-03, -6.4239e-02,  6.6331e-02,\n",
      "         -6.6616e-02,  6.8670e-08, -6.7178e-02, -3.6938e-02, -6.7429e-02,\n",
      "         -6.7554e-02,  6.2161e-02,  6.2668e-02,  3.7008e-02,  6.7574e-02,\n",
      "          6.7368e-02,  6.5898e-02,  3.7291e-02,  6.6727e-02, -6.3257e-02,\n",
      "         -6.7228e-02,  6.6562e-02,  6.1792e-02, -6.7257e-02, -2.5518e-02,\n",
      "         -6.0100e-02,  6.6731e-02, -6.4731e-02,  6.7266e-02,  1.8924e-08,\n",
      "         -6.7595e-02, -6.7123e-02, -6.7418e-02, -6.7345e-02, -6.6897e-02,\n",
      "          5.3282e-03, -6.2533e-02, -6.7356e-02,  6.7596e-02,  6.7384e-02,\n",
      "          6.5824e-02, -6.7330e-02, -6.7421e-02,  6.2648e-02,  6.1200e-02,\n",
      "         -6.0875e-03,  6.7457e-02,  6.7371e-02, -6.7288e-02,  6.6429e-02,\n",
      "         -6.2117e-02,  6.6923e-02,  7.2089e-07,  5.1562e-04,  4.7186e-03,\n",
      "         -6.7341e-02,  6.2035e-02,  6.5086e-02,  6.7276e-02,  6.6795e-02,\n",
      "         -1.8734e-03, -6.6711e-02, -6.4561e-02,  6.6985e-02,  6.7410e-02,\n",
      "          3.2877e-02,  3.8970e-02,  6.7305e-02,  6.7098e-02, -6.7430e-02,\n",
      "          6.7094e-02, -6.4319e-02,  6.1292e-02, -6.7501e-02, -6.4619e-02,\n",
      "         -6.7115e-02,  6.7135e-02, -6.6579e-02,  2.8509e-07, -6.7531e-02,\n",
      "         -6.4620e-02, -6.7388e-02,  6.7252e-02,  6.7096e-02, -6.7506e-02,\n",
      "          2.8633e-09, -6.5300e-02, -6.7263e-02, -2.4722e-04, -4.0494e-02,\n",
      "         -3.2954e-02,  6.5160e-02, -3.9293e-02, -6.6729e-02, -6.7242e-02,\n",
      "         -5.8362e-02,  6.7063e-02,  6.6790e-02,  6.7391e-02,  6.3057e-02,\n",
      "          6.7479e-02, -4.0264e-02,  6.6780e-02,  6.1174e-02,  8.4328e-05,\n",
      "          6.7444e-02, -6.4527e-02, -6.7261e-02, -1.2016e-08, -6.6994e-02,\n",
      "          6.7437e-02, -4.0952e-09, -6.7023e-02,  6.7428e-02,  6.4790e-02,\n",
      "         -5.2360e-07,  6.7501e-02, -3.3817e-05,  6.7234e-02,  6.7123e-02,\n",
      "          6.6683e-02,  1.6733e-04,  6.6854e-02,  6.7504e-02, -5.4591e-02,\n",
      "          6.7375e-02, -6.7458e-02, -1.0713e-07, -6.7394e-02, -2.3773e-05,\n",
      "         -6.2634e-02, -6.7195e-02,  6.6591e-02, -6.7483e-02, -6.6754e-02,\n",
      "         -6.7320e-02,  9.0393e-08, -6.7350e-02,  6.7033e-02, -5.5030e-02,\n",
      "         -5.5602e-02,  1.5041e-07,  6.6845e-02, -6.7374e-02, -6.3212e-02,\n",
      "          6.4717e-02,  6.6441e-02, -6.5841e-02, -6.0728e-03,  6.7371e-02,\n",
      "         -6.7430e-02,  6.4964e-02,  5.9442e-02, -6.7371e-02,  5.6901e-02,\n",
      "          6.4991e-02,  1.2980e-03, -6.7591e-02, -6.6276e-02,  3.6318e-02,\n",
      "          6.7483e-02, -6.5320e-02,  6.5499e-02,  6.6371e-02, -6.7563e-02,\n",
      "         -6.7270e-02,  5.6841e-07, -7.7628e-03, -6.7428e-02,  6.1854e-02,\n",
      "         -6.7351e-03,  6.7140e-02,  6.0360e-02,  3.2200e-02,  6.7526e-02,\n",
      "         -6.4536e-02, -1.1429e-03,  5.4412e-02,  6.6289e-02, -1.0021e-04,\n",
      "         -6.7169e-02,  6.3203e-03,  6.3468e-02,  5.4763e-02, -6.7518e-02,\n",
      "          6.4844e-02,  5.9107e-02,  6.4657e-02, -6.7161e-02,  6.7252e-02,\n",
      "         -6.3953e-02,  8.2347e-03,  6.5119e-02, -6.7534e-02, -6.7339e-02,\n",
      "         -6.7405e-02,  1.1444e-04,  6.7375e-02, -6.7574e-02,  6.7164e-02,\n",
      "          4.2723e-03, -5.0971e-02,  6.2805e-02, -6.7219e-02,  1.4702e-08,\n",
      "          6.7277e-02,  6.2599e-02, -6.7550e-02,  4.1347e-05, -6.7279e-02,\n",
      "          3.5698e-04,  6.0857e-02,  6.7389e-02,  6.4545e-02, -6.7400e-02,\n",
      "          1.0368e-03,  6.4676e-02, -6.2545e-02, -6.6346e-02, -6.5358e-02,\n",
      "         -6.7396e-02,  1.5532e-09,  1.0901e-06, -6.7390e-02, -6.7336e-02,\n",
      "          6.2665e-02,  6.4600e-02,  6.7060e-02, -6.4262e-02,  6.6794e-02,\n",
      "         -6.0619e-02,  6.7019e-02, -6.7066e-02, -6.4595e-02, -6.7044e-02,\n",
      "          6.6832e-02, -6.7367e-02, -6.7229e-02, -6.7443e-02,  6.7259e-02,\n",
      "          6.6007e-02,  2.4126e-06, -3.0179e-03,  6.6067e-02,  6.6460e-02,\n",
      "          6.7404e-02, -6.7397e-02, -1.5102e-08, -6.6566e-02,  5.9396e-02,\n",
      "         -6.5803e-02,  3.0325e-04,  6.6892e-02, -6.4522e-02, -6.7451e-02,\n",
      "         -6.7552e-02,  6.2458e-02, -3.3686e-02, -6.7554e-02,  5.6933e-07,\n",
      "         -6.6965e-02, -6.4766e-02,  6.5704e-02,  6.7033e-02, -6.7588e-02,\n",
      "         -6.5439e-02,  6.6992e-02, -6.5156e-02,  6.4610e-02,  6.2246e-02,\n",
      "          6.7464e-02, -1.0333e-08, -5.2990e-08,  6.7298e-02,  6.4427e-02,\n",
      "         -6.7270e-02, -6.7353e-02, -6.7473e-02, -6.2098e-02, -6.6256e-02,\n",
      "          6.7051e-02, -3.9792e-09,  5.0297e-06,  6.7651e-02, -6.7588e-02,\n",
      "          6.7137e-02, -6.5897e-02,  6.7459e-02, -6.7119e-02, -6.7142e-02,\n",
      "         -6.7032e-02,  1.4208e-03, -2.2531e-03, -3.4002e-05, -6.2485e-02,\n",
      "          6.7047e-02,  1.9950e-06,  6.2619e-02, -4.3295e-04,  6.6979e-02,\n",
      "         -4.1925e-02,  5.7241e-03, -6.7509e-02, -6.7074e-02, -6.7152e-02,\n",
      "         -4.0087e-02,  1.9418e-09,  6.7081e-02, -6.9224e-08,  7.1605e-03,\n",
      "         -6.4432e-02,  6.7467e-02,  6.7016e-02, -4.3124e-06,  6.7321e-02,\n",
      "         -6.7562e-02, -6.2809e-02, -6.7216e-02, -6.7424e-02,  2.5220e-03,\n",
      "          6.7233e-02, -6.7184e-02, -5.7422e-02,  6.7142e-02,  6.6692e-02,\n",
      "         -6.0657e-02,  2.6549e-05,  6.6478e-02, -6.6431e-02,  6.6683e-02,\n",
      "         -6.4142e-02, -4.8963e-05,  2.5485e-02, -1.1370e-03,  6.6871e-02,\n",
      "         -6.7483e-02, -6.7199e-02, -6.7092e-02, -6.7251e-02, -1.1900e-04,\n",
      "          6.6485e-02, -6.6237e-02, -6.2429e-02, -6.7234e-02,  6.7504e-02,\n",
      "          6.7331e-02,  6.5897e-02,  6.7221e-02, -6.6987e-02,  6.6922e-02,\n",
      "         -6.7288e-02, -6.3721e-02,  6.5902e-02, -3.2436e-06,  1.7997e-09,\n",
      "          5.9391e-05,  6.6901e-02,  6.6926e-02, -6.7343e-02, -3.9708e-02,\n",
      "          6.6481e-02, -6.7066e-02, -6.7447e-02, -4.1179e-02, -6.6806e-02,\n",
      "          9.8530e-08, -6.6850e-02, -6.6807e-02,  3.3475e-02,  6.2364e-02,\n",
      "          6.7018e-02,  6.6879e-02,  6.5360e-02, -1.9073e-07, -6.6690e-02,\n",
      "          5.6255e-02,  6.6337e-02,  6.7206e-02, -6.7079e-02, -3.2966e-07,\n",
      "         -6.2678e-02, -6.7446e-02, -6.7237e-02,  6.6416e-02, -6.7501e-02,\n",
      "         -6.7539e-02, -6.3749e-02,  6.4719e-02,  6.4697e-02,  6.6729e-02,\n",
      "         -6.3290e-02, -6.7158e-02, -6.6782e-02,  3.0375e-02,  6.7004e-02,\n",
      "         -4.9525e-06,  6.7568e-02, -1.1982e-06,  6.2767e-02, -6.7334e-02,\n",
      "          6.7172e-02, -6.7377e-02,  5.8678e-02,  6.4428e-02, -6.4449e-02,\n",
      "          6.5896e-02,  6.2609e-02, -6.6776e-02,  6.2950e-02,  5.0737e-03,\n",
      "         -6.7092e-02,  6.7201e-02, -3.7448e-03,  6.7109e-02, -5.7745e-03,\n",
      "         -6.7313e-02, -6.1316e-02, -6.7416e-02, -6.7189e-02, -6.4288e-03,\n",
      "          6.6084e-02,  6.6805e-02, -5.5808e-05,  6.4505e-02,  6.7553e-02,\n",
      "          6.7108e-02,  6.7221e-02, -6.7303e-02, -6.7451e-02, -2.6494e-03,\n",
      "         -2.3323e-09,  6.7276e-02, -6.7297e-02]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0827e-06, -1.7530e-07,  1.5452e-07, -2.2674e-08, -2.2496e-06,\n",
      "          3.2150e-06,  6.0465e-06, -5.3801e-09, -1.8847e-07, -7.7690e-07,\n",
      "         -1.5788e-07,  3.6213e-06, -3.8558e-08, -1.0063e-05, -1.4680e-08,\n",
      "         -1.8126e-07, -2.7730e-06,  9.1713e-09, -2.7553e-06, -5.3852e-09,\n",
      "          9.5103e-07,  3.8168e-09,  7.2278e-08, -1.0654e-08,  6.3138e-07,\n",
      "          3.1689e-06, -2.6612e-06, -1.0689e-09,  2.8926e-06,  4.4980e-06,\n",
      "         -9.2651e-06,  1.0652e-06, -4.1821e-07,  2.2096e-08, -9.7153e-07,\n",
      "          1.0851e-06,  7.0076e-08, -7.6146e-08, -2.9167e-09,  2.1692e-06,\n",
      "          7.3493e-08,  8.4667e-06,  3.4533e-08, -2.6099e-07, -4.6151e-06,\n",
      "          1.3888e-08,  3.3025e-07, -5.3568e-06, -2.0778e-06, -2.7470e-06,\n",
      "          1.0874e-07, -4.3026e-06, -3.1775e-08, -1.3380e-09, -2.3559e-06,\n",
      "         -1.7084e-09, -9.4255e-06,  2.6611e-06,  7.8697e-07, -5.3748e-08,\n",
      "          1.0649e-05, -2.6270e-07, -3.8041e-07,  1.9315e-08, -7.3593e-08,\n",
      "         -4.4816e-06,  5.6049e-10,  8.2062e-07, -1.0063e-08,  4.5128e-08,\n",
      "         -5.1747e-09, -4.5403e-07, -2.9219e-08,  7.7394e-07,  2.2411e-07,\n",
      "         -3.3084e-07,  2.4516e-09,  3.5821e-10, -1.8521e-09,  1.8354e-07,\n",
      "         -1.8954e-05,  2.7999e-10, -8.5999e-08, -2.1377e-06, -1.6716e-06,\n",
      "          1.6572e-08, -2.3315e-06,  1.8895e-07,  6.3496e-06, -3.1833e-07,\n",
      "          6.2023e-07,  8.4298e-10, -8.2967e-07,  3.7212e-06,  1.1039e-06,\n",
      "         -9.0300e-09, -2.6949e-08, -8.5791e-10, -1.1635e-08, -7.3959e-09,\n",
      "         -9.7975e-06,  4.5500e-07, -5.4868e-06,  4.2644e-06,  3.0767e-06,\n",
      "          4.9342e-06,  4.9311e-07,  1.9163e-05,  2.2566e-08, -1.8834e-08,\n",
      "          2.0761e-06,  2.9012e-06,  3.2295e-07, -7.7596e-08,  1.3192e-07,\n",
      "          5.7596e-06, -3.6992e-05, -3.0007e-07,  4.7662e-08, -3.7218e-07,\n",
      "         -5.0134e-06,  3.3447e-08, -4.8486e-06,  5.2302e-07,  3.9796e-06,\n",
      "          6.6157e-06,  1.0902e-06,  2.3589e-08,  1.0390e-07,  1.5728e-07,\n",
      "          1.2887e-05, -8.6768e-07, -2.0269e-07,  7.0103e-07,  1.9808e-05,\n",
      "          9.1876e-08, -1.8358e-07, -3.2414e-05, -8.2112e-06,  4.8515e-08,\n",
      "         -1.7336e-05,  3.3777e-06, -4.5661e-07,  3.0050e-06,  1.6503e-09,\n",
      "         -7.2160e-09,  7.9344e-07, -5.0495e-09,  2.4069e-07, -4.3610e-06,\n",
      "          2.7735e-06,  2.7815e-07, -2.0350e-07,  1.6112e-08, -2.0510e-06,\n",
      "         -4.1556e-06, -1.0095e-06, -4.1982e-06,  9.7768e-06,  3.5272e-08,\n",
      "          4.1931e-06,  3.6369e-07, -7.5657e-08,  1.3493e-06, -9.1426e-06,\n",
      "          1.5492e-06, -5.5606e-08,  2.8326e-10, -6.2274e-06,  1.1553e-07,\n",
      "          1.3244e-07,  1.1595e-08,  1.0701e-07, -1.7686e-10,  1.3434e-06,\n",
      "          7.6663e-07,  3.0214e-06,  3.1273e-06,  1.8162e-07, -8.7588e-09,\n",
      "         -5.3203e-08,  1.3994e-06, -3.9140e-07, -5.9820e-06,  1.8577e-06,\n",
      "         -2.9392e+00,  2.7478e-05,  1.7482e-07, -1.0052e-06,  1.1428e-06,\n",
      "          3.5725e-06,  1.6826e-08, -3.2411e-08,  1.3470e-06,  9.9076e-08,\n",
      "         -9.7098e-06, -3.4484e-06,  5.2059e-07,  9.9433e-09, -1.4315e-06,\n",
      "          1.1571e-07, -1.0162e-06, -1.9451e-06,  1.3364e-07,  2.0557e-09,\n",
      "          1.8360e-05,  2.3615e-09,  1.5429e-06, -1.3612e-05, -1.3375e-08,\n",
      "         -1.1263e-05,  6.5371e-06,  3.0841e-06,  2.1411e-09,  6.0650e-08,\n",
      "         -3.8018e-06, -2.6458e-07,  3.9600e-09, -1.6493e-09, -4.5734e-07,\n",
      "          1.6022e-09, -3.6367e-09,  8.5293e-07, -3.3017e-08,  1.1829e-05,\n",
      "         -3.8856e-08, -7.8512e-08, -1.2441e-07, -8.3563e-07,  3.5845e-07,\n",
      "         -1.1632e-06, -3.2378e-05,  5.1766e-08, -3.2791e-07, -1.0741e-08,\n",
      "         -4.8005e-06, -3.0898e-07,  2.0715e-06,  1.1634e-07,  1.9251e-07,\n",
      "         -6.8956e-08,  1.6680e-06,  3.3701e-07,  6.6928e-09, -3.8224e-08,\n",
      "          2.0501e-05,  4.8596e-07,  6.9212e-08, -3.0852e-07,  2.6557e-09,\n",
      "          1.1923e-07, -8.0101e-09,  7.0794e-08, -3.8855e-07, -1.8652e-07,\n",
      "         -2.3786e-06, -1.5048e-08,  1.0523e-08,  1.7478e-07, -1.3772e-08,\n",
      "          1.3210e-06, -1.8784e-06,  2.4997e-06,  3.5084e-07,  3.1842e-10,\n",
      "          9.0063e-09, -7.8309e-07,  5.2683e-06,  1.2784e-05, -4.2034e-08,\n",
      "         -6.3294e-09, -2.5147e-05,  2.9737e-08, -3.2164e-08, -3.1089e-06,\n",
      "         -6.6904e-08, -1.1640e-05, -8.9892e-10, -1.2145e-06,  1.5709e-06,\n",
      "          1.2708e-05,  6.3839e-08,  2.4791e-07,  7.2218e-08, -1.3138e-06,\n",
      "          1.6254e-08,  1.1444e-07, -6.5649e-06, -3.8067e-09,  6.3245e-07,\n",
      "          2.2508e-07, -3.5040e-06,  7.8615e-06,  2.3449e-08, -1.6639e-08,\n",
      "         -1.5493e-08,  1.7303e-08,  4.1829e-06,  3.4123e-07, -7.8461e-07,\n",
      "          1.5215e-07, -2.0604e-06,  6.5877e-07, -1.1104e-06,  1.2191e-06,\n",
      "          8.6716e-07, -1.7774e-08, -2.4495e-07,  1.7908e-06, -1.5804e-06,\n",
      "         -8.3130e-07, -3.4135e-07,  5.9923e-06, -4.0097e-09,  1.3151e-06,\n",
      "         -3.6523e-09, -1.2073e-06,  1.5975e-07, -4.1334e-09,  1.3149e-06,\n",
      "         -1.9978e-07,  1.8486e-07, -1.6621e-07, -1.2257e-09,  1.2581e-05,\n",
      "          3.0089e-06, -6.8363e-08, -2.2873e-09,  4.8389e-07,  7.7758e-07,\n",
      "          9.1260e-08, -1.0340e-06, -6.4681e-07, -2.7343e-06,  6.2495e-07,\n",
      "          2.1049e-06, -2.5364e-07, -4.6147e-08, -8.3495e-07,  2.7527e-08,\n",
      "         -1.7728e-06, -2.1282e-05, -9.3445e-07,  5.4204e-07, -1.6911e-08,\n",
      "         -4.9603e-09,  6.9281e-06, -2.1246e-08,  2.5717e-06,  1.0584e-08,\n",
      "         -9.4637e-08,  2.0147e-06,  2.3032e-06,  2.3860e-06, -2.1552e-08,\n",
      "         -6.6082e-09,  1.6799e-05, -2.3743e-06,  1.1095e-08,  9.3728e-07,\n",
      "         -4.6303e-09, -4.5186e-09, -6.7962e-09, -1.7106e-08, -5.3569e-07,\n",
      "          3.9490e-08,  6.3466e-09, -3.9532e-06,  1.4323e-05,  1.4552e-09,\n",
      "         -4.7709e-06,  1.8672e-06, -2.3148e-06,  4.7150e-07,  5.4807e-07,\n",
      "         -6.5502e-07,  1.8981e-07,  2.8571e-09,  4.2063e-09,  3.1712e-08,\n",
      "         -5.6101e-07, -3.3603e-07, -4.8259e-06, -1.1903e-07, -1.2574e-05,\n",
      "         -1.0291e-08,  5.6589e-06, -3.3860e-07, -1.4214e-07, -1.1464e-06,\n",
      "          1.4969e-05, -9.9780e-06,  4.2101e-08, -4.2471e-07, -1.9702e-06,\n",
      "         -1.4949e-08,  6.7702e-08, -3.3593e-08,  1.7504e-06, -4.4923e-06,\n",
      "          7.6709e-06, -3.6925e-04, -1.0527e-07, -3.6139e-07,  2.1184e-07,\n",
      "          2.2556e-07,  1.1836e-07,  2.1052e-09, -1.2651e-08, -6.2661e-06,\n",
      "         -9.0347e-06, -6.6332e-09,  2.4092e-06,  1.0344e-06, -9.1272e-08,\n",
      "          7.9553e-07,  5.8131e-08,  3.6408e-07, -6.0187e-07, -2.8670e-06,\n",
      "          2.0006e-07,  8.7373e-09, -2.4830e-06,  9.6273e-06,  6.2010e-07,\n",
      "         -9.2130e-08,  4.0188e-06,  4.9999e-08, -2.8333e-09, -7.3779e-09,\n",
      "         -1.2982e-06, -5.7213e-07, -2.5262e-06,  2.0549e-06,  1.2422e-05,\n",
      "          3.0170e-09,  6.0589e-08,  1.7087e-07, -1.6573e-06,  5.5917e-08,\n",
      "         -7.3781e-06,  6.5990e-06,  1.4642e-06, -4.5863e-07, -2.4139e-08,\n",
      "         -1.1933e-06, -3.2544e-08, -1.3277e-07, -1.0180e-07, -3.4549e-07,\n",
      "         -4.7493e-07,  3.0314e-06, -1.9502e-06,  1.6607e-07, -1.4059e-07,\n",
      "         -3.2411e-06, -1.2351e-06,  1.3018e-06, -5.1746e-06, -6.0567e-09,\n",
      "          8.0995e-09, -2.5051e-07,  6.3932e-08, -7.1051e-06,  6.2887e-06,\n",
      "          1.5063e-06, -1.3608e-08, -5.9592e-07, -1.0121e-06,  2.2344e-06,\n",
      "         -9.7173e-09,  5.5005e-06,  8.0114e-07, -1.3357e-05, -1.6829e-08,\n",
      "          5.2020e-07, -1.0246e-08, -3.6627e-07, -6.9186e-07,  1.5120e-06,\n",
      "         -1.1171e-08, -8.9179e-08,  3.1674e-06, -3.2056e-08,  7.5366e-08,\n",
      "          3.3973e-08,  7.3093e-06,  6.1867e-07, -2.1678e-07, -3.1126e-08,\n",
      "         -1.4283e-06,  8.8902e-08, -4.0483e-06, -1.8835e-05,  2.0241e-07,\n",
      "          6.6565e-08,  2.4728e-07, -4.9105e-09, -3.6491e-07,  4.1599e-09,\n",
      "         -6.8935e-08,  4.7498e-07, -1.4405e-09, -2.3634e-08, -6.7978e-06,\n",
      "          6.5777e-10, -2.2546e-07, -1.1316e-06, -2.4625e-06,  2.6516e-09,\n",
      "          4.7425e-08, -9.3632e-06,  5.2651e-06,  3.2771e-07, -4.6329e-09,\n",
      "         -1.5464e-08, -6.1605e-10,  1.9835e-06,  2.1300e-08, -2.4945e-09,\n",
      "          9.7230e-06, -5.8208e-06,  6.9670e-08, -1.6276e-06, -7.9235e-06,\n",
      "          4.2358e-07, -9.8639e-08, -4.0145e-07,  4.9376e-07, -6.1349e-09,\n",
      "          5.2211e-09,  1.1231e-06, -1.9232e-08,  8.3668e-08, -2.0639e-08,\n",
      "          3.2266e-06,  4.5991e-06,  5.6453e-07, -1.1690e-08,  3.1363e-06,\n",
      "          7.9319e-07,  6.3972e-07,  2.4431e-06,  1.3691e-07,  3.4397e-07,\n",
      "          5.9716e-07, -6.7640e-08,  4.2014e-07, -1.2256e-07, -1.9971e-05,\n",
      "          4.1802e-09,  7.2193e-07,  1.4406e-07, -1.3008e-08,  7.8154e-09,\n",
      "          1.9151e-06,  7.1253e-07,  1.8491e-07, -3.6584e-08, -7.2075e-07,\n",
      "          5.4406e-09, -2.8820e-07, -1.0357e-05, -1.2787e-05, -2.7474e-09,\n",
      "          4.3042e-08,  9.2539e-08,  1.1078e-06,  4.8793e-07,  1.0743e-08,\n",
      "         -4.2308e-07, -1.2427e-08, -1.7520e-07,  3.1959e-06,  3.0790e-08,\n",
      "          8.1962e-07,  1.5018e-06,  3.3025e-07,  1.9033e-06, -2.8258e-05,\n",
      "         -4.0832e-06,  3.2066e-06, -1.3506e-09, -1.7938e-08, -9.5366e-06,\n",
      "          4.1767e-06,  9.4760e-08,  7.0537e-09, -5.8291e-08,  4.0933e-06,\n",
      "          9.3245e-08,  1.4259e-08, -8.7811e-09,  2.0307e-08, -2.2486e-07,\n",
      "          3.6306e-06,  5.9481e-09, -6.3747e-07, -5.8849e-07, -3.4337e-08,\n",
      "          1.4729e-06,  1.7147e-08, -5.9682e-08, -1.2062e-06, -1.2547e-07,\n",
      "         -2.1901e-06, -1.6813e-06,  2.1180e-07,  6.2201e-08,  1.6180e-06,\n",
      "         -1.4157e-05,  1.2282e-05,  1.9979e-09, -1.0967e-09,  1.4225e-07,\n",
      "         -4.2317e-09, -7.5325e-06, -5.8693e-06, -4.7341e-06,  1.6854e-08,\n",
      "         -4.6850e-08,  3.3433e-09, -1.1984e-08, -1.7735e-08, -9.4798e-07,\n",
      "         -1.2299e-06, -2.9764e-08, -1.1842e-07, -6.1587e-06,  7.0409e-08,\n",
      "          6.7254e-08, -2.3478e-06, -2.6517e-07,  2.7608e-06,  2.3073e-09,\n",
      "          7.2569e-08, -7.0159e-10,  3.1021e-07,  2.9071e-08,  4.6543e-09,\n",
      "          1.9451e-07, -1.7354e-09,  1.3059e-07,  2.5021e-09, -6.0248e-08,\n",
      "          1.3928e-07, -1.6684e-07,  4.6277e-07,  1.2854e-07, -5.1473e-06,\n",
      "          1.0356e-07, -1.8853e-06,  8.8526e-10,  2.5783e-07, -3.5606e-06,\n",
      "          1.6781e-05,  3.0114e-06, -4.5087e-07, -9.2964e-08, -1.4801e-05,\n",
      "          1.2632e-07, -1.3177e-05, -5.8259e-07,  2.0617e-05,  4.9409e-07,\n",
      "          6.1154e-08,  5.1906e-06,  1.2295e-05, -5.4655e-07, -6.9090e-07,\n",
      "          3.7675e-06,  5.6017e-09, -2.4236e-08, -6.7151e-08, -8.4430e-09,\n",
      "         -7.9482e-07,  8.9041e-08,  5.2378e-08, -3.9485e-06, -1.8214e-06,\n",
      "         -1.7094e-07,  1.1340e-07, -1.3492e-06, -4.8415e-06,  1.4066e-07,\n",
      "         -4.3767e-07, -4.1373e-08,  7.2860e-08, -8.9656e-08, -4.6104e-09,\n",
      "         -7.4272e-07, -1.5907e-05,  3.6720e-06,  1.5925e-06,  5.7099e-08,\n",
      "          1.7195e-09,  2.5247e-06, -4.4720e-08,  5.2508e-08,  5.4985e-08,\n",
      "          7.8396e-08,  2.1274e-06,  3.1347e-07,  8.5338e-08, -2.8651e-07,\n",
      "         -1.0325e-07, -1.7460e-06,  3.5058e-06, -3.0202e-06,  2.8637e-07,\n",
      "         -4.5763e-07, -6.7738e-06, -3.2277e-07, -2.2951e-08, -2.5471e-07,\n",
      "         -2.1949e-09,  1.7677e-06,  6.9713e-08, -5.6668e-07, -1.1175e-07,\n",
      "          1.0015e-07,  6.1517e-07, -1.1322e-08, -2.1065e-06,  2.4826e-08,\n",
      "          4.2148e-06, -6.9230e-09, -8.9478e-08, -7.0129e-08,  3.3094e-09,\n",
      "          2.1308e-06, -1.3334e-08, -2.2185e-06,  5.5710e-08,  2.2817e-08,\n",
      "         -5.1039e-09,  5.7348e-06, -2.1602e-07,  8.8378e-09, -2.1920e-06,\n",
      "         -1.5670e-07, -7.1959e-09,  1.7429e-07,  3.3435e-07,  1.8197e-07,\n",
      "         -6.0268e-06, -4.1070e-08, -9.2149e-06, -1.2947e-07, -2.9009e-06,\n",
      "          4.0414e-06,  4.2962e-06,  1.2600e-07,  5.2677e-08, -2.4173e-05,\n",
      "         -1.6576e-06, -1.0721e-08, -5.0578e-07,  2.3554e-08, -6.2507e-06,\n",
      "          7.5529e-07, -2.0994e-05,  2.9447e-06,  6.8881e-08,  4.8714e-07,\n",
      "         -7.9799e-07,  3.2075e-07, -4.6073e-06]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 0, 14],\n",
      "         [ 2, 12],\n",
      "         [10, 13],\n",
      "         [ 3, 18],\n",
      "         [ 9,  1],\n",
      "         [ 5,  2],\n",
      "         [10,  1],\n",
      "         [ 4, 12],\n",
      "         [ 7, 14],\n",
      "         [ 9,  5],\n",
      "         [ 4, 11],\n",
      "         [ 9, 12],\n",
      "         [ 9,  6],\n",
      "         [10, 18],\n",
      "         [ 8, 18],\n",
      "         [ 2,  5],\n",
      "         [10,  2],\n",
      "         [ 9,  8],\n",
      "         [ 2, 10],\n",
      "         [ 6, 16],\n",
      "         [ 9,  4],\n",
      "         [10, 10],\n",
      "         [ 7, 13],\n",
      "         [ 0, 16],\n",
      "         [ 0, 18],\n",
      "         [ 3, 12],\n",
      "         [ 3,  8],\n",
      "         [10,  9],\n",
      "         [ 6, 10],\n",
      "         [ 3, 13],\n",
      "         [ 8, 10],\n",
      "         [ 4,  7],\n",
      "         [ 0, 12],\n",
      "         [ 6,  4],\n",
      "         [ 8,  1],\n",
      "         [ 3,  7],\n",
      "         [ 2,  4],\n",
      "         [ 2,  7],\n",
      "         [ 5, 13],\n",
      "         [ 8, 15],\n",
      "         [ 4,  9],\n",
      "         [ 8, 17],\n",
      "         [ 6, 17],\n",
      "         [ 8, 12],\n",
      "         [ 1, 15],\n",
      "         [10, 12],\n",
      "         [10,  6],\n",
      "         [ 2,  9],\n",
      "         [ 8, 11],\n",
      "         [ 8, 13],\n",
      "         [ 4,  0],\n",
      "         [ 0, 15],\n",
      "         [ 8,  0],\n",
      "         [ 8,  4],\n",
      "         [ 0,  4],\n",
      "         [ 3, 17],\n",
      "         [ 8,  2],\n",
      "         [ 3,  6],\n",
      "         [ 9, 11],\n",
      "         [ 8,  3],\n",
      "         [ 4,  6],\n",
      "         [ 4,  8],\n",
      "         [10,  3],\n",
      "         [ 7, 11],\n",
      "         [10,  8],\n",
      "         [ 4,  2],\n",
      "         [ 6, 13],\n",
      "         [ 5, 11],\n",
      "         [ 0,  9],\n",
      "         [ 0,  7],\n",
      "         [ 1, 14],\n",
      "         [ 5, 12],\n",
      "         [ 5,  5],\n",
      "         [ 1, 13],\n",
      "         [ 8,  6],\n",
      "         [ 0, 11],\n",
      "         [ 9,  0],\n",
      "         [ 0, 13],\n",
      "         [ 1, 18],\n",
      "         [ 3,  9],\n",
      "         [ 2, 16],\n",
      "         [10, 16],\n",
      "         [ 1,  8],\n",
      "         [ 6, 18],\n",
      "         [ 1,  0],\n",
      "         [ 0,  8],\n",
      "         [ 5, 14],\n",
      "         [ 5,  4],\n",
      "         [ 6,  9],\n",
      "         [ 2, 17],\n",
      "         [ 9, 13],\n",
      "         [ 7,  7],\n",
      "         [ 7, 10],\n",
      "         [ 9, 18],\n",
      "         [ 2, 13],\n",
      "         [ 7,  1],\n",
      "         [10,  4],\n",
      "         [ 1,  7],\n",
      "         [ 7,  4],\n",
      "         [ 5, 17],\n",
      "         [ 0,  0],\n",
      "         [ 1, 17],\n",
      "         [ 3,  0],\n",
      "         [ 4,  3],\n",
      "         [ 4, 16],\n",
      "         [ 9, 15],\n",
      "         [ 7,  6],\n",
      "         [ 6,  3],\n",
      "         [ 1, 10],\n",
      "         [ 8,  5],\n",
      "         [ 5,  9],\n",
      "         [ 5, 16],\n",
      "         [ 9, 16],\n",
      "         [ 4, 10],\n",
      "         [ 3, 16],\n",
      "         [ 9, 17],\n",
      "         [ 5,  1],\n",
      "         [ 6, 15],\n",
      "         [ 9,  7],\n",
      "         [ 6, 12],\n",
      "         [ 6,  2],\n",
      "         [ 4, 17],\n",
      "         [ 1, 11],\n",
      "         [ 6,  5],\n",
      "         [ 3, 11],\n",
      "         [10, 11],\n",
      "         [ 1,  9],\n",
      "         [ 2,  8],\n",
      "         [ 7,  8],\n",
      "         [10,  0],\n",
      "         [ 3,  2],\n",
      "         [ 5, 10],\n",
      "         [ 0,  3],\n",
      "         [ 3, 10],\n",
      "         [ 4, 14],\n",
      "         [ 8,  9],\n",
      "         [ 2,  6],\n",
      "         [ 1, 12],\n",
      "         [ 1,  6],\n",
      "         [ 6,  8],\n",
      "         [ 7, 16],\n",
      "         [ 0,  6],\n",
      "         [ 7,  2],\n",
      "         [ 9, 10],\n",
      "         [ 8, 16],\n",
      "         [ 2,  1],\n",
      "         [ 1,  3],\n",
      "         [ 3,  1],\n",
      "         [ 3,  3],\n",
      "         [ 5, 18],\n",
      "         [ 4, 18],\n",
      "         [ 7, 18],\n",
      "         [ 8, 14],\n",
      "         [ 2, 15],\n",
      "         [ 5,  7],\n",
      "         [ 2,  0],\n",
      "         [ 9,  2],\n",
      "         [ 4,  1],\n",
      "         [ 6,  1],\n",
      "         [ 6, 11],\n",
      "         [ 9,  3],\n",
      "         [ 7, 12],\n",
      "         [ 2,  2],\n",
      "         [ 5,  8],\n",
      "         [ 5,  6],\n",
      "         [ 4, 13],\n",
      "         [ 1,  1],\n",
      "         [ 6,  6],\n",
      "         [10,  5],\n",
      "         [ 7, 17],\n",
      "         [10, 17],\n",
      "         [ 0,  5],\n",
      "         [ 1,  4],\n",
      "         [ 7,  9],\n",
      "         [ 6,  7],\n",
      "         [ 2,  3],\n",
      "         [ 2, 14],\n",
      "         [ 0, 17],\n",
      "         [ 2, 18],\n",
      "         [ 2, 11],\n",
      "         [ 0, 10],\n",
      "         [ 6, 14],\n",
      "         [ 4,  5],\n",
      "         [10, 15],\n",
      "         [ 0,  1],\n",
      "         [ 1,  2],\n",
      "         [ 8,  8],\n",
      "         [ 5, 15],\n",
      "         [ 3, 14],\n",
      "         [ 7,  0],\n",
      "         [ 6,  0],\n",
      "         [ 4,  4],\n",
      "         [ 8,  7],\n",
      "         [ 3,  5],\n",
      "         [ 3, 15],\n",
      "         [ 0,  2],\n",
      "         [ 1,  5],\n",
      "         [ 5,  3],\n",
      "         [ 1, 16],\n",
      "         [ 7,  3],\n",
      "         [ 7, 15],\n",
      "         [10,  7],\n",
      "         [10, 14],\n",
      "         [ 9, 14],\n",
      "         [ 4, 15],\n",
      "         [ 9,  9],\n",
      "         [ 5,  0],\n",
      "         [ 3,  4],\n",
      "         [ 7,  5]]]), (11, 19)), 'cls_output': tensor([[0.7252]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/164085027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 876\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.0705e-06, -1.7521e-07,  1.5456e-07, -2.1202e-08, -2.2476e-06,\n",
      "           3.2173e-06,  6.0811e-06, -5.3703e-09, -1.9807e-07, -7.9415e-07,\n",
      "          -1.5692e-07,  3.6335e-06, -3.8573e-08, -1.0093e-05, -1.3580e-08,\n",
      "          -1.8139e-07, -2.7724e-06,  9.1703e-09, -2.7452e-06, -5.7367e-09,\n",
      "           9.5353e-07,  3.9170e-09,  8.3201e-08, -1.0664e-08,  6.3288e-07,\n",
      "           3.1788e-06, -2.6123e-06, -1.0732e-09,  2.9125e-06,  4.4989e-06,\n",
      "          -9.3056e-06,  1.0643e-06, -4.5918e-07,  2.2646e-08, -9.7590e-07,\n",
      "           1.0921e-06,  5.4508e-08, -7.6128e-08, -2.9861e-09,  2.1609e-06,\n",
      "           7.3610e-08,  8.4742e-06,  3.4496e-08, -2.6085e-07, -4.6524e-06,\n",
      "           1.3929e-08, -5.0008e-07, -2.3619e-06, -2.0132e-06, -2.7828e-06,\n",
      "           1.0898e-07, -4.3159e-06, -3.1896e-08, -1.0233e-09, -2.3536e-06,\n",
      "          -1.7547e-09, -9.5498e-06,  2.6606e-06,  2.5922e-07, -5.3656e-08,\n",
      "           1.0648e-05, -2.6302e-07, -3.8063e-07,  1.8673e-08, -7.3548e-08,\n",
      "          -4.4497e-06,  5.8488e-10,  8.2012e-07, -1.0048e-08,  4.5198e-08,\n",
      "          -5.1587e-09, -4.5402e-07, -3.1521e-08,  7.7532e-07,  2.2411e-07,\n",
      "          -3.2163e-07,  2.4312e-09,  4.7526e-08, -1.8406e-09,  1.8258e-07,\n",
      "          -1.8963e-05,  3.5911e-10, -8.7442e-08, -2.1156e-06, -1.6889e-06,\n",
      "           1.6607e-08, -2.3158e-06,  1.8894e-07,  6.2720e-06, -3.0614e-07,\n",
      "           6.4272e-07,  8.4808e-10, -8.2978e-07,  3.7205e-06, -4.5556e-07,\n",
      "          -9.0431e-09, -2.6982e-08, -9.0076e-10, -1.1646e-08, -7.4008e-09,\n",
      "          -1.0095e-05,  4.5463e-07, -5.4923e-06,  4.2628e-06,  3.0881e-06,\n",
      "           4.9144e-06,  4.8263e-07,  1.9304e-05,  2.0120e-08, -1.8777e-08,\n",
      "           2.1247e-06,  2.9010e-06,  3.2299e-07, -7.7630e-08,  1.3161e-07,\n",
      "           5.7831e-06, -3.7161e-05, -2.9974e-07,  4.7228e-08, -3.7540e-07,\n",
      "          -5.0153e-06,  3.3485e-08, -4.8561e-06,  5.2390e-07,  3.9803e-06,\n",
      "           6.6331e-06,  1.0615e-06,  2.3658e-08,  1.9803e-07,  1.5534e-07,\n",
      "           1.2872e-05, -8.7163e-07, -2.0274e-07,  8.1007e-07,  1.9812e-05,\n",
      "           9.1543e-08, -1.8288e-07, -4.4274e-06, -8.2111e-06,  4.8516e-08,\n",
      "          -1.7234e-05,  3.3590e-06, -4.5906e-07,  3.0226e-06,  1.6611e-09,\n",
      "          -1.8436e-07,  7.9059e-07, -5.0428e-09,  2.3985e-07, -4.3570e-06,\n",
      "           2.7547e-06,  2.7828e-07, -2.0589e-07,  1.6140e-08, -8.7913e-06,\n",
      "          -4.1581e-06, -1.0303e-06, -4.2579e-06,  9.7744e-06,  3.5265e-08,\n",
      "           4.3416e-06,  3.5801e-07, -7.5727e-08,  1.3483e-06, -9.2882e-06,\n",
      "           1.5881e-06, -5.5673e-08,  1.8862e-10, -6.2908e-06,  1.1524e-07,\n",
      "           1.3282e-07,  1.1588e-08,  1.0572e-07, -1.8299e-10,  1.3464e-06,\n",
      "           7.6655e-07,  3.0214e-06,  4.7245e-07,  1.8281e-07, -8.3301e-09,\n",
      "          -5.3229e-08,  1.4018e-06, -3.9204e-07, -5.9981e-06,  1.8483e-06,\n",
      "          -1.2476e+01,  2.7466e-05,  1.7483e-07, -1.0044e-06,  1.1345e-06,\n",
      "           3.5873e-06,  1.6828e-08, -3.2386e-08,  1.3231e-06,  1.1067e-07,\n",
      "          -9.7024e-06, -3.4435e-06,  5.2040e-07,  9.8994e-09,  6.7235e-07,\n",
      "           1.1420e-07, -1.0131e-06, -1.9564e-06,  1.3763e-07,  2.0571e-09,\n",
      "           1.8330e-05,  2.3529e-09,  1.5280e-06, -1.3599e-05, -1.3387e-08,\n",
      "          -1.1265e-05,  6.5373e-06,  3.0832e-06,  2.0227e-09,  5.8327e-08,\n",
      "          -3.7830e-06, -2.6543e-07,  3.6740e-09, -1.6549e-09, -4.5770e-07,\n",
      "           1.5224e-09, -3.6159e-09,  8.5259e-07, -3.2185e-08,  1.1833e-05,\n",
      "          -3.8601e-08, -7.7616e-08, -1.2406e-07, -7.7444e-07,  3.5853e-07,\n",
      "          -1.1620e-06, -3.2386e-05,  5.1563e-08, -3.2455e-07, -1.0634e-08,\n",
      "          -4.7977e-06, -2.8738e-07,  2.0620e-06,  1.1597e-07,  1.9269e-07,\n",
      "           2.6304e-08, -1.5591e-06,  3.3739e-07,  6.7261e-09, -3.7009e-08,\n",
      "           2.0499e-05,  4.8093e-07,  6.9865e-08, -3.0791e-07,  2.4737e-09,\n",
      "           1.1568e-07, -1.1909e-08,  7.0830e-08, -3.8724e-07, -1.8492e-07,\n",
      "          -2.3770e-06, -4.9863e-08,  1.4903e-08,  1.7465e-07, -1.5729e-08,\n",
      "           1.3197e-06, -1.8862e-06, -3.4125e-07,  3.8081e-07,  4.1185e-09,\n",
      "           9.0387e-09, -7.8791e-07,  5.2688e-06,  1.0472e-06, -4.5152e-08,\n",
      "          -6.3991e-09, -2.5148e-05,  2.9618e-08, -3.2158e-08, -3.1126e-06,\n",
      "          -6.7059e-08, -1.1617e-05, -9.1126e-10, -1.2096e-06,  1.5687e-06,\n",
      "           1.2704e-05,  6.3805e-08,  2.4865e-07,  4.2421e-08, -1.3085e-06,\n",
      "           1.6238e-08,  1.1387e-07, -6.5243e-06, -3.7927e-09,  6.3239e-07,\n",
      "           2.1772e-07, -3.6238e-06,  7.8510e-06,  2.3451e-08, -1.6644e-08,\n",
      "          -6.7653e-09,  1.7376e-08,  4.2325e-06,  3.4382e-07, -7.9322e-07,\n",
      "           1.5175e-07, -2.0806e-06,  6.5979e-07, -1.1127e-06,  1.2323e-06,\n",
      "           8.6577e-07, -2.1816e-08, -2.4667e-07,  1.7908e-06, -2.7271e-06,\n",
      "          -8.2854e-07, -1.1972e-06,  5.9904e-06, -4.0014e-09,  1.3214e-06,\n",
      "          -3.6247e-09, -1.3826e-06,  1.5857e-07, -4.1422e-09,  1.3100e-06,\n",
      "          -1.9948e-07,  1.7546e-07, -1.6663e-07, -1.1876e-09,  1.2469e-05,\n",
      "           2.9894e-06, -6.9129e-08, -2.3283e-09,  4.1247e-07,  7.7483e-07,\n",
      "           9.1203e-08, -9.8638e-07, -6.5233e-07, -2.7579e-06,  6.2525e-07,\n",
      "           2.1035e-06, -2.5365e-07, -4.6151e-08, -8.3807e-07,  2.7435e-07,\n",
      "          -1.7637e-06, -2.1282e-05, -1.0119e-06,  5.4634e-07, -1.6888e-08,\n",
      "          -4.9618e-09,  7.0129e-06, -2.1212e-08, -3.2276e-08,  1.0739e-08,\n",
      "          -3.8581e-07,  2.0151e-06,  2.3192e-06,  2.5386e-06, -6.3311e-09,\n",
      "          -6.5924e-09,  1.6800e-05, -2.3732e-06,  1.1052e-08,  9.3492e-07,\n",
      "          -4.6413e-09, -4.5131e-09, -1.3413e-08, -1.7090e-08, -5.3563e-07,\n",
      "           3.9143e-08,  6.3290e-09, -3.9526e-06,  1.4251e-05,  1.4609e-09,\n",
      "          -4.7690e-06,  1.8617e-06, -2.3164e-06,  4.7677e-07,  5.5473e-07,\n",
      "          -6.5424e-07,  1.9043e-07,  2.8705e-09,  4.1889e-09,  3.1675e-08,\n",
      "          -5.6701e-07, -3.3418e-07, -4.8245e-06, -1.2208e-07, -1.2579e-05,\n",
      "          -1.0292e-08,  5.6189e-06, -3.3867e-07, -1.4220e-07, -1.1529e-06,\n",
      "           1.4955e-05, -9.9755e-06,  4.2098e-08, -4.7091e-07, -1.9709e-06,\n",
      "          -1.4910e-08,  6.7365e-08, -3.3627e-08,  1.7504e-06, -4.4959e-06,\n",
      "           7.6812e-06,  1.3357e-04, -1.0540e-07, -3.6329e-07,  2.1173e-07,\n",
      "           2.2562e-07,  1.1760e-07,  2.1037e-09, -1.2845e-08, -6.2611e-06,\n",
      "          -8.4922e-06, -6.6068e-09,  2.3698e-06,  1.0364e-06, -9.1273e-08,\n",
      "           7.9910e-07,  5.8195e-08,  3.8610e-07, -2.8259e-07, -2.8695e-06,\n",
      "           2.0003e-07,  8.7001e-09,  2.7057e-06,  9.6254e-06,  6.2056e-07,\n",
      "          -9.2256e-08,  4.0103e-06,  5.0245e-08, -3.0545e-09, -7.4120e-09,\n",
      "          -1.3020e-06, -5.8061e-07, -5.0467e-07,  2.0459e-06,  1.2453e-05,\n",
      "           2.8797e-09,  5.9096e-08, -1.3650e-07, -1.7231e-06,  5.6155e-08,\n",
      "          -7.2823e-06,  2.2833e-06,  1.0018e-06, -4.8581e-07, -2.4255e-08,\n",
      "          -1.1936e-06, -3.0576e-08, -1.3276e-07,  1.8906e-08, -3.4256e-07,\n",
      "          -4.7502e-07,  3.0308e-06, -1.9504e-06,  1.6687e-07, -1.4067e-07,\n",
      "          -3.2389e-06, -1.9622e-06,  1.2840e-06, -5.1929e-06, -5.7583e-09,\n",
      "           8.1122e-09, -2.4988e-07,  6.2490e-08, -7.1051e-06,  6.2890e-06,\n",
      "           1.5064e-06, -1.2966e-08, -6.0107e-07, -1.0128e-06,  2.2346e-06,\n",
      "          -9.7218e-09,  5.6699e-06, -2.6947e-06, -1.3278e-05, -1.7069e-08,\n",
      "           5.2235e-07, -9.7624e-09, -3.6445e-07, -7.0137e-07,  1.5170e-06,\n",
      "          -1.1140e-08, -8.9419e-08,  3.2134e-06, -3.2035e-08,  7.0367e-08,\n",
      "           3.3939e-08,  7.3384e-06,  6.1663e-07, -2.0497e-07, -3.0772e-08,\n",
      "          -1.4267e-06,  8.9570e-08, -4.0509e-06, -1.8876e-05,  2.0238e-07,\n",
      "           6.6535e-08,  2.4708e-07, -4.8657e-09, -3.6573e-07,  7.8544e-09,\n",
      "          -6.8982e-08,  4.8089e-07, -1.4776e-09, -2.3595e-08, -6.8129e-06,\n",
      "           6.9381e-10, -2.3210e-07, -1.1164e-06, -2.4618e-06,  2.6487e-09,\n",
      "           4.8181e-08, -9.3347e-06,  5.3015e-06,  3.2759e-07, -4.6297e-09,\n",
      "          -1.5417e-08, -6.7973e-10,  1.9806e-06,  2.1304e-08, -2.3451e-09,\n",
      "           9.6351e-06,  8.7539e-07,  6.9686e-08, -1.6368e-06, -7.8648e-06,\n",
      "           4.2370e-07, -9.8600e-08, -4.2517e-07,  4.9696e-07, -6.1395e-09,\n",
      "           5.2236e-09,  1.1263e-06,  3.4314e-09,  8.3416e-08, -2.0653e-08,\n",
      "           3.1180e-06,  4.5942e-06,  5.5429e-07, -1.1697e-08,  3.1334e-06,\n",
      "           3.5614e-07, -2.9946e-07,  2.4542e-06,  2.1006e-07,  3.4275e-07,\n",
      "           5.9653e-07, -6.7314e-08, -2.6987e-06, -1.2318e-07, -1.9951e-05,\n",
      "           4.1791e-09,  9.0749e-08,  1.4211e-07, -1.2994e-08,  7.7482e-09,\n",
      "           2.0325e-06,  3.2872e-08,  1.8770e-07, -3.6565e-08, -4.4565e-07,\n",
      "           5.4924e-09, -2.8814e-07, -1.0286e-05, -1.2779e-05, -3.6414e-09,\n",
      "           4.2905e-08,  9.2467e-08,  1.1076e-06,  6.8068e-07,  1.0772e-08,\n",
      "          -2.5271e-07, -1.2429e-08, -1.7459e-07,  3.1883e-06,  3.0819e-08,\n",
      "           8.3064e-07,  1.4967e-06,  3.5149e-07,  1.9145e-06,  1.0433e-05,\n",
      "          -4.0918e-06,  3.2103e-06, -1.2838e-09, -1.7808e-08, -9.6167e-06,\n",
      "           4.1772e-06,  9.4751e-08,  7.0124e-09, -5.8482e-08,  4.1038e-06,\n",
      "           9.3137e-08,  1.4279e-08, -8.8191e-09,  2.0127e-08, -2.0090e-07,\n",
      "           1.7485e-06,  5.9445e-09, -6.4444e-07, -5.8941e-07, -3.4347e-08,\n",
      "           1.4494e-06,  1.6853e-08, -5.9534e-08, -1.2081e-06, -1.1556e-07,\n",
      "          -2.2007e-06, -1.6601e-06,  2.1164e-07,  6.2411e-08,  1.6189e-06,\n",
      "          -1.4095e-05,  1.2285e-05,  1.9836e-09, -1.1830e-09,  1.4245e-07,\n",
      "          -4.2388e-09, -7.5148e-06, -5.9049e-06, -4.7331e-06,  1.6933e-08,\n",
      "          -4.7904e-08,  3.3373e-09, -1.2266e-08, -1.7535e-08, -9.2677e-07,\n",
      "          -1.2172e-06, -3.0298e-08, -1.1858e-07, -6.2225e-06,  7.0328e-08,\n",
      "           6.8054e-08, -7.1502e-07, -2.6513e-07,  2.7297e-06,  2.3259e-09,\n",
      "           7.3877e-08, -7.2392e-10,  3.1065e-07,  2.8778e-08,  4.6586e-09,\n",
      "           2.0189e-07, -1.7172e-09,  1.2696e-07,  2.4887e-09, -6.3959e-08,\n",
      "           1.3936e-07, -1.6669e-07,  4.6270e-07,  1.2864e-07, -5.1439e-06,\n",
      "           1.0699e-07, -1.8113e-06,  8.5400e-10,  2.5707e-07, -3.5609e-06,\n",
      "           1.6756e-05,  3.0093e-06, -4.4207e-07, -9.3632e-08, -1.4868e-05,\n",
      "           1.2654e-07, -1.3183e-05, -1.4881e-06,  2.0645e-05,  4.9586e-07,\n",
      "           6.1311e-08,  5.1878e-06,  1.2307e-05, -5.6192e-07, -6.8642e-07,\n",
      "           3.7236e-06,  5.0237e-09, -2.3857e-08, -7.7664e-08, -8.3865e-09,\n",
      "           1.5629e-06,  2.6984e-07,  5.2005e-08, -3.9755e-06, -1.7400e-06,\n",
      "          -1.7448e-07,  1.1338e-07, -1.3464e-06, -4.8910e-06,  1.4077e-07,\n",
      "          -4.3792e-07, -4.1272e-08,  7.2853e-08, -8.9580e-08, -4.6144e-09,\n",
      "          -7.4259e-07, -1.5908e-05,  3.6718e-06,  9.4396e-06,  5.6789e-08,\n",
      "           1.7059e-09,  2.5213e-06, -4.6034e-08,  5.2949e-08,  5.4982e-08,\n",
      "          -5.4109e-08,  2.1200e-06,  3.1344e-07,  9.6897e-08, -2.8529e-07,\n",
      "          -1.0593e-07, -1.7308e-06,  2.8259e-07, -3.0128e-06,  2.8530e-07,\n",
      "          -4.6122e-07, -6.8373e-06, -3.2311e-07, -2.2906e-08, -2.5482e-07,\n",
      "          -2.2110e-09, -1.1769e-06,  7.2977e-08, -5.7473e-07, -1.1251e-07,\n",
      "           1.0031e-07,  6.1135e-07, -1.1308e-08, -2.1041e-06,  2.4877e-08,\n",
      "           4.2130e-06, -6.9241e-09, -9.0283e-08, -7.0564e-08,  3.2511e-09,\n",
      "           2.1240e-06,  2.7619e-07, -2.1908e-06,  5.6387e-08,  2.2908e-08,\n",
      "          -5.2895e-09,  5.7410e-06, -2.2186e-07,  8.8366e-09, -2.1829e-06,\n",
      "          -1.5606e-07, -7.4005e-09,  1.4464e-07,  6.0941e-08,  1.8124e-07,\n",
      "          -5.9934e-06, -4.1108e-08, -9.2321e-06, -1.2959e-07, -2.9012e-06,\n",
      "           4.0460e-06,  4.3070e-06,  1.2612e-07,  5.2592e-08, -2.4102e-05,\n",
      "           1.0350e-07, -1.0870e-08, -5.3403e-07,  2.3729e-08, -6.2486e-06,\n",
      "           6.4484e-07, -2.0996e-05,  2.8569e-06,  6.8823e-08,  4.8678e-07,\n",
      "          -8.1302e-07,  3.0096e-07, -5.7045e-07]]], device='cuda:0'), 'image_feats': tensor([[[ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9453e-07,\n",
      "           3.1838e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9454e-07,\n",
      "           3.1839e-07, -3.4249e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9451e-07,\n",
      "           3.1838e-07, -3.4252e-06],\n",
      "         ...,\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9452e-07,\n",
      "           3.1838e-07, -3.4245e-06],\n",
      "         [ 3.0764e-06, -1.7537e-07,  1.5568e-07,  ..., -7.9452e-07,\n",
      "           3.1839e-07, -3.4246e-06],\n",
      "         [ 3.0764e-06, -1.7536e-07,  1.5568e-07,  ..., -7.9445e-07,\n",
      "           3.1837e-07, -3.4245e-06]]], device='cuda:0'), 'cls_feats': tensor([[-2.5329e-01,  2.7298e-01, -2.7280e-01, -1.6352e-01, -2.5890e-01,\n",
      "         -2.7271e-01, -2.7103e-01, -2.7263e-01,  2.7364e-01,  2.4064e-01,\n",
      "          2.7141e-01,  2.7301e-01,  2.6473e-01, -4.2325e-06,  4.4231e-07,\n",
      "          1.5733e-01,  2.6908e-01, -2.5862e-01,  2.7277e-01, -2.7285e-01,\n",
      "          2.6162e-01,  2.7369e-01,  2.7239e-01, -2.7115e-01,  2.5504e-01,\n",
      "          2.7245e-01, -2.7290e-01,  2.7163e-01,  2.7095e-01,  2.7171e-01,\n",
      "         -2.7310e-01,  1.1716e-01,  1.7293e-01, -6.8875e-03,  2.7160e-01,\n",
      "          2.7121e-01, -2.6871e-01, -2.7221e-01, -2.6997e-01,  4.9694e-08,\n",
      "          9.2227e-03,  2.6083e-01, -2.7138e-01, -2.6946e-01,  2.7263e-01,\n",
      "         -2.7328e-01, -2.7197e-01, -2.7241e-01, -2.5311e-01,  2.7223e-01,\n",
      "          2.6809e-01, -2.7347e-01, -2.7346e-01, -2.7066e-01, -2.7207e-01,\n",
      "          2.5937e-01,  2.7125e-01,  2.7321e-01,  2.7196e-01, -2.5437e-01,\n",
      "         -2.7341e-01,  3.1951e-07,  2.7169e-01, -2.6424e-01, -2.7358e-01,\n",
      "          1.6841e-02,  1.5248e-01, -2.6188e-01,  1.5418e-01,  2.7122e-01,\n",
      "          2.7234e-01, -2.0134e-08,  2.7261e-01,  2.5505e-01, -1.6933e-01,\n",
      "         -2.7271e-01, -2.7113e-01, -2.7173e-01,  2.7162e-01, -2.7094e-01,\n",
      "          2.7282e-01,  2.7289e-01, -1.2384e-02,  2.7327e-01,  2.5079e-01,\n",
      "         -5.2214e-09,  2.7274e-01,  2.5253e-01, -2.7112e-01,  2.6295e-01,\n",
      "          7.2820e-02,  1.5179e-01, -2.6720e-01,  2.6206e-01, -2.5522e-01,\n",
      "          2.5453e-01,  2.7262e-01,  2.7172e-01, -2.5858e-01, -2.7368e-01,\n",
      "         -1.7650e-01, -2.7364e-01, -2.7761e-06, -2.7050e-01, -2.7242e-01,\n",
      "         -9.5408e-07,  2.8162e-07, -6.6777e-08,  2.7260e-01, -2.7262e-01,\n",
      "         -3.2197e-02, -2.7090e-01, -2.7245e-01,  2.5116e-01,  5.3597e-03,\n",
      "          2.6201e-01, -1.3350e-03, -9.9751e-02, -2.7307e-01,  2.6879e-06,\n",
      "          2.6327e-01, -2.5784e-01,  7.6070e-05,  2.6247e-01,  2.7262e-01,\n",
      "         -2.2627e-09,  2.4026e-01,  2.7228e-01,  2.7261e-01, -2.6966e-01,\n",
      "          9.4408e-07, -2.7287e-01, -3.0215e-06, -2.5881e-01, -4.4230e-08,\n",
      "         -2.6581e-01,  2.6156e-01,  2.7199e-01,  1.0778e-01,  2.7290e-01,\n",
      "          3.6665e-10, -6.5719e-03,  3.9806e-03, -2.6437e-01,  2.6979e-01,\n",
      "          5.7351e-03,  2.6954e-01,  2.7312e-01, -2.5813e-01,  2.6934e-01,\n",
      "         -2.7057e-01,  2.7349e-01,  2.7225e-01,  9.3279e-09, -2.7260e-01,\n",
      "         -3.3164e-05, -2.5894e-01, -2.7316e-01, -2.6113e-01, -2.7235e-01,\n",
      "          1.2625e-01, -2.7337e-01,  2.3563e-01, -2.5498e-01, -2.6821e-01,\n",
      "          2.6304e-01,  2.7362e-01, -2.5284e-01,  2.7051e-01, -2.7178e-01,\n",
      "          1.6742e-07, -2.3239e-01, -2.7683e-02, -2.6194e-01, -2.7192e-01,\n",
      "          2.4778e-07,  2.7302e-01, -2.7223e-01,  1.2483e-08,  2.5061e-01,\n",
      "         -9.8509e-02,  2.7330e-01,  2.7125e-01, -2.7161e-01,  2.7224e-01,\n",
      "         -2.6130e-01,  2.2664e-01,  1.0688e-10, -2.6499e-01,  2.7301e-01,\n",
      "          2.7192e-01,  2.6404e-01, -2.6144e-01,  2.7179e-01, -2.5356e-01,\n",
      "          2.7134e-01,  2.5808e-01,  2.7103e-01,  2.6942e-01,  2.7246e-01,\n",
      "         -2.7130e-01, -2.7259e-01,  2.7295e-01,  2.7233e-01,  3.3845e-07,\n",
      "         -2.0712e-01, -2.6847e-01,  2.7179e-01,  2.6877e-01, -2.6997e-01,\n",
      "         -2.6171e-01, -2.7193e-01,  2.0173e-01, -2.7287e-01,  2.6270e-04,\n",
      "         -2.8069e-06, -2.7149e-01,  2.6112e-01, -2.7188e-01,  2.5542e-01,\n",
      "          3.3272e-05,  1.4977e-01,  2.7326e-01, -2.5716e-01,  1.5103e-01,\n",
      "          2.6368e-01,  2.7272e-01,  2.7164e-01,  2.7301e-01,  2.7327e-01,\n",
      "          2.6694e-01, -3.0225e-08,  2.5672e-01, -2.7213e-01, -2.7201e-01,\n",
      "          2.7264e-01, -1.2435e-02, -2.6663e-01,  2.6263e-01,  2.7366e-01,\n",
      "          2.6474e-01,  2.6283e-01,  2.6809e-01,  2.7153e-01, -1.1546e-01,\n",
      "          2.7087e-01, -1.0964e-02,  2.6851e-01, -1.4769e-01,  2.7090e-01,\n",
      "          2.7092e-01,  2.7378e-01,  2.4240e-03,  2.5407e-01,  2.5658e-01,\n",
      "         -2.7265e-01, -2.5323e-03, -2.0187e-04, -2.7371e-01,  2.7115e-01,\n",
      "         -2.7124e-01, -2.7291e-01,  2.4970e-01,  5.9815e-07, -2.7289e-01,\n",
      "         -2.4732e-01,  2.7261e-01,  2.6020e-01,  2.6386e-01,  2.6247e-01,\n",
      "          2.6751e-08, -2.7202e-01,  2.3406e-01, -2.7045e-01,  2.7338e-01,\n",
      "         -2.6875e-01,  2.5824e-01,  2.7291e-01,  2.7290e-01,  2.4673e-01,\n",
      "         -2.7322e-01, -2.6165e-01,  2.6366e-01,  2.7266e-01,  3.6768e-05,\n",
      "         -2.7326e-01, -2.7268e-02, -2.6397e-01,  1.1433e-01, -2.6522e-01,\n",
      "         -2.6601e-01,  1.3108e-01, -2.7206e-01, -2.7207e-01, -1.2430e-05,\n",
      "         -2.7117e-01, -2.7116e-01, -2.5257e-01, -2.6989e-01,  2.5703e-01,\n",
      "         -2.7190e-01, -2.7168e-01,  2.7014e-01,  2.6280e-01, -2.7317e-01,\n",
      "          2.7230e-01,  7.2003e-07, -3.0523e-04,  2.7236e-01,  2.9854e-06,\n",
      "          2.1700e-01,  2.4211e-01, -1.9442e-06, -2.6873e-01,  2.7277e-01,\n",
      "         -2.7360e-01,  2.7330e-01,  2.7135e-01, -2.6965e-01, -2.2647e-02,\n",
      "          1.6124e-07,  2.7255e-01,  2.7328e-01, -2.7367e-01,  2.5577e-01,\n",
      "         -2.7231e-01, -2.6937e-01, -2.7147e-01, -1.1090e-01, -2.6133e-01,\n",
      "         -2.7257e-01, -2.6162e-01,  2.6645e-01,  2.2533e-07, -2.7141e-01,\n",
      "          2.7209e-01,  3.8620e-04,  1.6536e-02, -2.6058e-01,  2.6870e-01,\n",
      "         -2.6980e-01,  3.1584e-07, -2.7198e-01, -1.5198e-01, -2.7295e-01,\n",
      "         -2.7343e-01,  2.5247e-01,  2.5445e-01,  1.5226e-01,  2.7351e-01,\n",
      "          2.7271e-01,  2.6702e-01,  1.5340e-01,  2.7023e-01, -2.5675e-01,\n",
      "         -2.7217e-01,  2.6960e-01,  2.5103e-01, -2.7228e-01, -1.0551e-01,\n",
      "         -2.4441e-01,  2.7025e-01, -2.6249e-01,  2.7232e-01,  8.4238e-08,\n",
      "         -2.7359e-01, -2.7176e-01, -2.7291e-01, -2.7263e-01, -2.7089e-01,\n",
      "          2.3009e-02, -2.5393e-01, -2.7267e-01,  2.7359e-01,  2.7277e-01,\n",
      "          2.6673e-01, -2.7256e-01, -2.7292e-01,  2.5438e-01,  2.4872e-01,\n",
      "         -2.6349e-02,  2.7306e-01,  2.7272e-01, -2.7240e-01,  2.6908e-01,\n",
      "         -2.5230e-01,  2.7099e-01,  3.2165e-06,  2.2942e-03,  1.9737e-02,\n",
      "         -2.7261e-01,  2.5198e-01,  2.6387e-01,  2.7236e-01,  2.7049e-01,\n",
      "         -8.5241e-03, -2.7017e-01, -2.6183e-01,  2.7123e-01,  2.7288e-01,\n",
      "          1.3550e-01,  1.6019e-01,  2.7247e-01,  2.7167e-01, -2.7295e-01,\n",
      "          2.7165e-01, -2.6089e-01,  2.4908e-01, -2.7323e-01, -2.6206e-01,\n",
      "         -2.7173e-01,  2.7181e-01, -2.6966e-01,  1.2801e-06, -2.7334e-01,\n",
      "         -2.6206e-01, -2.7279e-01,  2.7226e-01,  2.7166e-01, -2.7324e-01,\n",
      "          1.2125e-08, -2.6470e-01, -2.7231e-01, -9.4538e-04, -1.6635e-01,\n",
      "         -1.3581e-01,  2.6416e-01, -1.6150e-01, -2.7024e-01, -2.7223e-01,\n",
      "         -2.3759e-01,  2.7153e-01,  2.7048e-01,  2.7280e-01,  2.5597e-01,\n",
      "          2.7314e-01, -1.6542e-01,  2.7044e-01,  2.4861e-01,  3.6537e-04,\n",
      "          2.7301e-01, -2.6170e-01, -2.7230e-01, -5.2143e-08, -2.7126e-01,\n",
      "          2.7298e-01, -1.6175e-08, -2.7138e-01,  2.7295e-01,  2.6272e-01,\n",
      "         -2.0160e-06,  2.7323e-01, -1.2852e-04,  2.7220e-01,  2.7177e-01,\n",
      "          2.7006e-01,  7.5270e-04,  2.7072e-01,  2.7324e-01, -2.2270e-01,\n",
      "          2.7274e-01, -2.7306e-01, -4.1577e-07, -2.7281e-01, -1.0313e-04,\n",
      "         -2.5432e-01, -2.7204e-01,  2.6971e-01, -2.7316e-01, -2.7034e-01,\n",
      "         -2.7253e-01,  4.1148e-07, -2.7264e-01,  2.7142e-01, -2.2444e-01,\n",
      "         -2.2670e-01,  6.9491e-07,  2.7069e-01, -2.7274e-01, -2.5658e-01,\n",
      "          2.6244e-01,  2.6912e-01, -2.6680e-01, -2.5361e-02,  2.7273e-01,\n",
      "         -2.7295e-01,  2.6339e-01,  2.4183e-01, -2.7272e-01,  2.3183e-01,\n",
      "          2.6350e-01,  5.8954e-03, -2.7357e-01, -2.6848e-01,  1.4947e-01,\n",
      "          2.7316e-01, -2.6478e-01,  2.6547e-01,  2.6885e-01, -2.7347e-01,\n",
      "         -2.7233e-01,  2.1855e-06, -3.2388e-02, -2.7295e-01,  2.5127e-01,\n",
      "         -2.8110e-02,  2.7183e-01,  2.4543e-01,  1.3275e-01,  2.7332e-01,\n",
      "         -2.6173e-01, -4.4202e-03,  2.2200e-01,  2.6853e-01, -4.4395e-04,\n",
      "         -2.7194e-01,  2.6979e-02,  2.5757e-01,  2.2338e-01, -2.7329e-01,\n",
      "          2.6293e-01,  2.4051e-01,  2.6220e-01, -2.7191e-01,  2.7227e-01,\n",
      "         -2.5946e-01,  3.5099e-02,  2.6400e-01, -2.7336e-01, -2.7260e-01,\n",
      "         -2.7285e-01,  5.2348e-04,  2.7274e-01, -2.7351e-01,  2.7193e-01,\n",
      "          1.8870e-02, -2.0834e-01,  2.5499e-01, -2.7214e-01,  6.2968e-08,\n",
      "          2.7236e-01,  2.5419e-01, -2.7341e-01,  1.5647e-04, -2.7237e-01,\n",
      "          1.3710e-03,  2.4738e-01,  2.7279e-01,  2.6177e-01, -2.7283e-01,\n",
      "          4.4240e-03,  2.6228e-01, -2.5398e-01, -2.6876e-01, -2.6492e-01,\n",
      "         -2.7282e-01,  1.0813e-08,  4.0760e-06, -2.7280e-01, -2.7259e-01,\n",
      "          2.5444e-01,  2.6198e-01,  2.7152e-01, -2.6067e-01,  2.7049e-01,\n",
      "         -2.4644e-01,  2.7136e-01, -2.7154e-01, -2.6196e-01, -2.7146e-01,\n",
      "          2.7064e-01, -2.7271e-01, -2.7218e-01, -2.7300e-01,  2.7229e-01,\n",
      "          2.6744e-01,  9.3937e-06, -1.2705e-02,  2.6767e-01,  2.6920e-01,\n",
      "          2.7285e-01, -2.7282e-01, -6.2177e-08, -2.6961e-01,  2.4165e-01,\n",
      "         -2.6665e-01,  1.3670e-03,  2.7087e-01, -2.6168e-01, -2.7303e-01,\n",
      "         -2.7342e-01,  2.5364e-01, -1.3879e-01, -2.7343e-01,  2.2210e-06,\n",
      "         -2.7116e-01, -2.6262e-01,  2.6627e-01,  2.7142e-01, -2.7356e-01,\n",
      "         -2.6524e-01,  2.7126e-01, -2.6414e-01,  2.6202e-01,  2.5281e-01,\n",
      "          2.7308e-01, -4.2828e-08, -2.2151e-07,  2.7244e-01,  2.6131e-01,\n",
      "         -2.7234e-01, -2.7266e-01, -2.7312e-01, -2.5223e-01, -2.6841e-01,\n",
      "          2.7149e-01, -1.4834e-08,  2.2038e-05,  2.7381e-01, -2.7356e-01,\n",
      "          2.7182e-01, -2.6702e-01,  2.7306e-01, -2.7175e-01, -2.7184e-01,\n",
      "         -2.7141e-01,  5.9924e-03, -1.0006e-02, -1.4745e-04, -2.5374e-01,\n",
      "          2.7147e-01,  8.8581e-06,  2.5426e-01, -1.9334e-03,  2.7121e-01,\n",
      "         -1.7211e-01,  2.4070e-02, -2.7326e-01, -2.7158e-01, -2.7188e-01,\n",
      "         -1.6470e-01,  1.1724e-08,  2.7160e-01, -3.1781e-07,  2.9881e-02,\n",
      "         -2.6133e-01,  2.7309e-01,  2.7135e-01, -1.7179e-05,  2.7253e-01,\n",
      "         -2.7346e-01, -2.5500e-01, -2.7213e-01, -2.7293e-01,  1.1335e-02,\n",
      "          2.7219e-01, -2.7200e-01, -2.3388e-01,  2.7184e-01,  2.7010e-01,\n",
      "         -2.4659e-01,  1.1877e-04,  2.6927e-01, -2.6908e-01,  2.7006e-01,\n",
      "         -2.6020e-01, -1.9822e-04,  1.0538e-01, -4.8812e-03,  2.7079e-01,\n",
      "         -2.7316e-01, -2.7206e-01, -2.7165e-01, -2.7226e-01, -4.5433e-04,\n",
      "          2.6929e-01, -2.6833e-01, -2.5352e-01, -2.7220e-01,  2.7324e-01,\n",
      "          2.7257e-01,  2.6701e-01,  2.7214e-01, -2.7124e-01,  2.7099e-01,\n",
      "         -2.7240e-01, -2.5856e-01,  2.6703e-01, -1.4720e-05,  8.5388e-09,\n",
      "          2.2639e-04,  2.7091e-01,  2.7100e-01, -2.7262e-01, -1.6317e-01,\n",
      "          2.6928e-01, -2.7154e-01, -2.7302e-01, -1.6911e-01, -2.7054e-01,\n",
      "          4.3386e-07, -2.7071e-01, -2.7054e-01,  1.3794e-01,  2.5327e-01,\n",
      "          2.7136e-01,  2.7082e-01,  2.6493e-01, -8.8703e-07, -2.7009e-01,\n",
      "          2.2928e-01,  2.6872e-01,  2.7208e-01, -2.7159e-01, -1.4761e-06,\n",
      "         -2.5449e-01, -2.7302e-01, -2.7221e-01,  2.6903e-01, -2.7323e-01,\n",
      "         -2.7337e-01, -2.5867e-01,  2.6244e-01,  2.6236e-01,  2.7024e-01,\n",
      "         -2.5688e-01, -2.7190e-01, -2.7045e-01,  1.2533e-01,  2.7131e-01,\n",
      "         -1.9350e-05,  2.7349e-01, -4.5170e-06,  2.5484e-01, -2.7258e-01,\n",
      "          2.7195e-01, -2.7275e-01,  2.3883e-01,  2.6131e-01, -2.6139e-01,\n",
      "          2.6701e-01,  2.5422e-01, -2.7042e-01,  2.5555e-01,  2.1375e-02,\n",
      "         -2.7165e-01,  2.7207e-01, -1.5697e-02,  2.7171e-01, -2.4122e-02,\n",
      "         -2.7250e-01, -2.4917e-01, -2.7290e-01, -2.7202e-01, -2.6841e-02,\n",
      "          2.6774e-01,  2.7053e-01, -2.5754e-04,  2.6161e-01,  2.7343e-01,\n",
      "          2.7171e-01,  2.7215e-01, -2.7246e-01, -2.7303e-01, -1.1138e-02,\n",
      "         -9.8367e-09,  2.7236e-01, -2.7244e-01]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.0705e-06, -1.7521e-07,  1.5456e-07, -2.1202e-08, -2.2476e-06,\n",
      "          3.2173e-06,  6.0811e-06, -5.3703e-09, -1.9807e-07, -7.9415e-07,\n",
      "         -1.5692e-07,  3.6335e-06, -3.8573e-08, -1.0093e-05, -1.3580e-08,\n",
      "         -1.8139e-07, -2.7724e-06,  9.1703e-09, -2.7452e-06, -5.7367e-09,\n",
      "          9.5353e-07,  3.9170e-09,  8.3201e-08, -1.0664e-08,  6.3288e-07,\n",
      "          3.1788e-06, -2.6123e-06, -1.0732e-09,  2.9125e-06,  4.4989e-06,\n",
      "         -9.3056e-06,  1.0643e-06, -4.5918e-07,  2.2646e-08, -9.7590e-07,\n",
      "          1.0921e-06,  5.4508e-08, -7.6128e-08, -2.9861e-09,  2.1609e-06,\n",
      "          7.3610e-08,  8.4742e-06,  3.4496e-08, -2.6085e-07, -4.6524e-06,\n",
      "          1.3929e-08, -5.0008e-07, -2.3619e-06, -2.0132e-06, -2.7828e-06,\n",
      "          1.0898e-07, -4.3159e-06, -3.1896e-08, -1.0233e-09, -2.3536e-06,\n",
      "         -1.7547e-09, -9.5498e-06,  2.6606e-06,  2.5922e-07, -5.3656e-08,\n",
      "          1.0648e-05, -2.6302e-07, -3.8063e-07,  1.8673e-08, -7.3548e-08,\n",
      "         -4.4497e-06,  5.8488e-10,  8.2012e-07, -1.0048e-08,  4.5198e-08,\n",
      "         -5.1587e-09, -4.5402e-07, -3.1521e-08,  7.7532e-07,  2.2411e-07,\n",
      "         -3.2163e-07,  2.4312e-09,  4.7526e-08, -1.8406e-09,  1.8258e-07,\n",
      "         -1.8963e-05,  3.5911e-10, -8.7442e-08, -2.1156e-06, -1.6889e-06,\n",
      "          1.6607e-08, -2.3158e-06,  1.8894e-07,  6.2720e-06, -3.0614e-07,\n",
      "          6.4272e-07,  8.4808e-10, -8.2978e-07,  3.7205e-06, -4.5556e-07,\n",
      "         -9.0431e-09, -2.6982e-08, -9.0076e-10, -1.1646e-08, -7.4008e-09,\n",
      "         -1.0095e-05,  4.5463e-07, -5.4923e-06,  4.2628e-06,  3.0881e-06,\n",
      "          4.9144e-06,  4.8263e-07,  1.9304e-05,  2.0120e-08, -1.8777e-08,\n",
      "          2.1247e-06,  2.9010e-06,  3.2299e-07, -7.7630e-08,  1.3161e-07,\n",
      "          5.7831e-06, -3.7161e-05, -2.9974e-07,  4.7228e-08, -3.7540e-07,\n",
      "         -5.0153e-06,  3.3485e-08, -4.8561e-06,  5.2390e-07,  3.9803e-06,\n",
      "          6.6331e-06,  1.0615e-06,  2.3658e-08,  1.9803e-07,  1.5534e-07,\n",
      "          1.2872e-05, -8.7163e-07, -2.0274e-07,  8.1007e-07,  1.9812e-05,\n",
      "          9.1543e-08, -1.8288e-07, -4.4274e-06, -8.2111e-06,  4.8516e-08,\n",
      "         -1.7234e-05,  3.3590e-06, -4.5906e-07,  3.0226e-06,  1.6611e-09,\n",
      "         -1.8436e-07,  7.9059e-07, -5.0428e-09,  2.3985e-07, -4.3570e-06,\n",
      "          2.7547e-06,  2.7828e-07, -2.0589e-07,  1.6140e-08, -8.7913e-06,\n",
      "         -4.1581e-06, -1.0303e-06, -4.2579e-06,  9.7744e-06,  3.5265e-08,\n",
      "          4.3416e-06,  3.5801e-07, -7.5727e-08,  1.3483e-06, -9.2882e-06,\n",
      "          1.5881e-06, -5.5673e-08,  1.8862e-10, -6.2908e-06,  1.1524e-07,\n",
      "          1.3282e-07,  1.1588e-08,  1.0572e-07, -1.8299e-10,  1.3464e-06,\n",
      "          7.6655e-07,  3.0214e-06,  4.7245e-07,  1.8281e-07, -8.3301e-09,\n",
      "         -5.3229e-08,  1.4018e-06, -3.9204e-07, -5.9981e-06,  1.8483e-06,\n",
      "         -1.2476e+01,  2.7466e-05,  1.7483e-07, -1.0044e-06,  1.1345e-06,\n",
      "          3.5873e-06,  1.6828e-08, -3.2386e-08,  1.3231e-06,  1.1067e-07,\n",
      "         -9.7024e-06, -3.4435e-06,  5.2040e-07,  9.8994e-09,  6.7235e-07,\n",
      "          1.1420e-07, -1.0131e-06, -1.9564e-06,  1.3763e-07,  2.0571e-09,\n",
      "          1.8330e-05,  2.3529e-09,  1.5280e-06, -1.3599e-05, -1.3387e-08,\n",
      "         -1.1265e-05,  6.5373e-06,  3.0832e-06,  2.0227e-09,  5.8327e-08,\n",
      "         -3.7830e-06, -2.6543e-07,  3.6740e-09, -1.6549e-09, -4.5770e-07,\n",
      "          1.5224e-09, -3.6159e-09,  8.5259e-07, -3.2185e-08,  1.1833e-05,\n",
      "         -3.8601e-08, -7.7616e-08, -1.2406e-07, -7.7444e-07,  3.5853e-07,\n",
      "         -1.1620e-06, -3.2386e-05,  5.1563e-08, -3.2455e-07, -1.0634e-08,\n",
      "         -4.7977e-06, -2.8738e-07,  2.0620e-06,  1.1597e-07,  1.9269e-07,\n",
      "          2.6304e-08, -1.5591e-06,  3.3739e-07,  6.7261e-09, -3.7009e-08,\n",
      "          2.0499e-05,  4.8093e-07,  6.9865e-08, -3.0791e-07,  2.4737e-09,\n",
      "          1.1568e-07, -1.1909e-08,  7.0830e-08, -3.8724e-07, -1.8492e-07,\n",
      "         -2.3770e-06, -4.9863e-08,  1.4903e-08,  1.7465e-07, -1.5729e-08,\n",
      "          1.3197e-06, -1.8862e-06, -3.4125e-07,  3.8081e-07,  4.1185e-09,\n",
      "          9.0387e-09, -7.8791e-07,  5.2688e-06,  1.0472e-06, -4.5152e-08,\n",
      "         -6.3991e-09, -2.5148e-05,  2.9618e-08, -3.2158e-08, -3.1126e-06,\n",
      "         -6.7059e-08, -1.1617e-05, -9.1126e-10, -1.2096e-06,  1.5687e-06,\n",
      "          1.2704e-05,  6.3805e-08,  2.4865e-07,  4.2421e-08, -1.3085e-06,\n",
      "          1.6238e-08,  1.1387e-07, -6.5243e-06, -3.7927e-09,  6.3239e-07,\n",
      "          2.1772e-07, -3.6238e-06,  7.8510e-06,  2.3451e-08, -1.6644e-08,\n",
      "         -6.7653e-09,  1.7376e-08,  4.2325e-06,  3.4382e-07, -7.9322e-07,\n",
      "          1.5175e-07, -2.0806e-06,  6.5979e-07, -1.1127e-06,  1.2323e-06,\n",
      "          8.6577e-07, -2.1816e-08, -2.4667e-07,  1.7908e-06, -2.7271e-06,\n",
      "         -8.2854e-07, -1.1972e-06,  5.9904e-06, -4.0014e-09,  1.3214e-06,\n",
      "         -3.6247e-09, -1.3826e-06,  1.5857e-07, -4.1422e-09,  1.3100e-06,\n",
      "         -1.9948e-07,  1.7546e-07, -1.6663e-07, -1.1876e-09,  1.2469e-05,\n",
      "          2.9894e-06, -6.9129e-08, -2.3283e-09,  4.1247e-07,  7.7483e-07,\n",
      "          9.1203e-08, -9.8638e-07, -6.5233e-07, -2.7579e-06,  6.2525e-07,\n",
      "          2.1035e-06, -2.5365e-07, -4.6151e-08, -8.3807e-07,  2.7435e-07,\n",
      "         -1.7637e-06, -2.1282e-05, -1.0119e-06,  5.4634e-07, -1.6888e-08,\n",
      "         -4.9618e-09,  7.0129e-06, -2.1212e-08, -3.2276e-08,  1.0739e-08,\n",
      "         -3.8581e-07,  2.0151e-06,  2.3192e-06,  2.5386e-06, -6.3311e-09,\n",
      "         -6.5924e-09,  1.6800e-05, -2.3732e-06,  1.1052e-08,  9.3492e-07,\n",
      "         -4.6413e-09, -4.5131e-09, -1.3413e-08, -1.7090e-08, -5.3563e-07,\n",
      "          3.9143e-08,  6.3290e-09, -3.9526e-06,  1.4251e-05,  1.4609e-09,\n",
      "         -4.7690e-06,  1.8617e-06, -2.3164e-06,  4.7677e-07,  5.5473e-07,\n",
      "         -6.5424e-07,  1.9043e-07,  2.8705e-09,  4.1889e-09,  3.1675e-08,\n",
      "         -5.6701e-07, -3.3418e-07, -4.8245e-06, -1.2208e-07, -1.2579e-05,\n",
      "         -1.0292e-08,  5.6189e-06, -3.3867e-07, -1.4220e-07, -1.1529e-06,\n",
      "          1.4955e-05, -9.9755e-06,  4.2098e-08, -4.7091e-07, -1.9709e-06,\n",
      "         -1.4910e-08,  6.7365e-08, -3.3627e-08,  1.7504e-06, -4.4959e-06,\n",
      "          7.6812e-06,  1.3357e-04, -1.0540e-07, -3.6329e-07,  2.1173e-07,\n",
      "          2.2562e-07,  1.1760e-07,  2.1037e-09, -1.2845e-08, -6.2611e-06,\n",
      "         -8.4922e-06, -6.6068e-09,  2.3698e-06,  1.0364e-06, -9.1273e-08,\n",
      "          7.9910e-07,  5.8195e-08,  3.8610e-07, -2.8259e-07, -2.8695e-06,\n",
      "          2.0003e-07,  8.7001e-09,  2.7057e-06,  9.6254e-06,  6.2056e-07,\n",
      "         -9.2256e-08,  4.0103e-06,  5.0245e-08, -3.0545e-09, -7.4120e-09,\n",
      "         -1.3020e-06, -5.8061e-07, -5.0467e-07,  2.0459e-06,  1.2453e-05,\n",
      "          2.8797e-09,  5.9096e-08, -1.3650e-07, -1.7231e-06,  5.6155e-08,\n",
      "         -7.2823e-06,  2.2833e-06,  1.0018e-06, -4.8581e-07, -2.4255e-08,\n",
      "         -1.1936e-06, -3.0576e-08, -1.3276e-07,  1.8906e-08, -3.4256e-07,\n",
      "         -4.7502e-07,  3.0308e-06, -1.9504e-06,  1.6687e-07, -1.4067e-07,\n",
      "         -3.2389e-06, -1.9622e-06,  1.2840e-06, -5.1929e-06, -5.7583e-09,\n",
      "          8.1122e-09, -2.4988e-07,  6.2490e-08, -7.1051e-06,  6.2890e-06,\n",
      "          1.5064e-06, -1.2966e-08, -6.0107e-07, -1.0128e-06,  2.2346e-06,\n",
      "         -9.7218e-09,  5.6699e-06, -2.6947e-06, -1.3278e-05, -1.7069e-08,\n",
      "          5.2235e-07, -9.7624e-09, -3.6445e-07, -7.0137e-07,  1.5170e-06,\n",
      "         -1.1140e-08, -8.9419e-08,  3.2134e-06, -3.2035e-08,  7.0367e-08,\n",
      "          3.3939e-08,  7.3384e-06,  6.1663e-07, -2.0497e-07, -3.0772e-08,\n",
      "         -1.4267e-06,  8.9570e-08, -4.0509e-06, -1.8876e-05,  2.0238e-07,\n",
      "          6.6535e-08,  2.4708e-07, -4.8657e-09, -3.6573e-07,  7.8544e-09,\n",
      "         -6.8982e-08,  4.8089e-07, -1.4776e-09, -2.3595e-08, -6.8129e-06,\n",
      "          6.9381e-10, -2.3210e-07, -1.1164e-06, -2.4618e-06,  2.6487e-09,\n",
      "          4.8181e-08, -9.3347e-06,  5.3015e-06,  3.2759e-07, -4.6297e-09,\n",
      "         -1.5417e-08, -6.7973e-10,  1.9806e-06,  2.1304e-08, -2.3451e-09,\n",
      "          9.6351e-06,  8.7539e-07,  6.9686e-08, -1.6368e-06, -7.8648e-06,\n",
      "          4.2370e-07, -9.8600e-08, -4.2517e-07,  4.9696e-07, -6.1395e-09,\n",
      "          5.2236e-09,  1.1263e-06,  3.4314e-09,  8.3416e-08, -2.0653e-08,\n",
      "          3.1180e-06,  4.5942e-06,  5.5429e-07, -1.1697e-08,  3.1334e-06,\n",
      "          3.5614e-07, -2.9946e-07,  2.4542e-06,  2.1006e-07,  3.4275e-07,\n",
      "          5.9653e-07, -6.7314e-08, -2.6987e-06, -1.2318e-07, -1.9951e-05,\n",
      "          4.1791e-09,  9.0749e-08,  1.4211e-07, -1.2994e-08,  7.7482e-09,\n",
      "          2.0325e-06,  3.2872e-08,  1.8770e-07, -3.6565e-08, -4.4565e-07,\n",
      "          5.4924e-09, -2.8814e-07, -1.0286e-05, -1.2779e-05, -3.6414e-09,\n",
      "          4.2905e-08,  9.2467e-08,  1.1076e-06,  6.8068e-07,  1.0772e-08,\n",
      "         -2.5271e-07, -1.2429e-08, -1.7459e-07,  3.1883e-06,  3.0819e-08,\n",
      "          8.3064e-07,  1.4967e-06,  3.5149e-07,  1.9145e-06,  1.0433e-05,\n",
      "         -4.0918e-06,  3.2103e-06, -1.2838e-09, -1.7808e-08, -9.6167e-06,\n",
      "          4.1772e-06,  9.4751e-08,  7.0124e-09, -5.8482e-08,  4.1038e-06,\n",
      "          9.3137e-08,  1.4279e-08, -8.8191e-09,  2.0127e-08, -2.0090e-07,\n",
      "          1.7485e-06,  5.9445e-09, -6.4444e-07, -5.8941e-07, -3.4347e-08,\n",
      "          1.4494e-06,  1.6853e-08, -5.9534e-08, -1.2081e-06, -1.1556e-07,\n",
      "         -2.2007e-06, -1.6601e-06,  2.1164e-07,  6.2411e-08,  1.6189e-06,\n",
      "         -1.4095e-05,  1.2285e-05,  1.9836e-09, -1.1830e-09,  1.4245e-07,\n",
      "         -4.2388e-09, -7.5148e-06, -5.9049e-06, -4.7331e-06,  1.6933e-08,\n",
      "         -4.7904e-08,  3.3373e-09, -1.2266e-08, -1.7535e-08, -9.2677e-07,\n",
      "         -1.2172e-06, -3.0298e-08, -1.1858e-07, -6.2225e-06,  7.0328e-08,\n",
      "          6.8054e-08, -7.1502e-07, -2.6513e-07,  2.7297e-06,  2.3259e-09,\n",
      "          7.3877e-08, -7.2392e-10,  3.1065e-07,  2.8778e-08,  4.6586e-09,\n",
      "          2.0189e-07, -1.7172e-09,  1.2696e-07,  2.4887e-09, -6.3959e-08,\n",
      "          1.3936e-07, -1.6669e-07,  4.6270e-07,  1.2864e-07, -5.1439e-06,\n",
      "          1.0699e-07, -1.8113e-06,  8.5400e-10,  2.5707e-07, -3.5609e-06,\n",
      "          1.6756e-05,  3.0093e-06, -4.4207e-07, -9.3632e-08, -1.4868e-05,\n",
      "          1.2654e-07, -1.3183e-05, -1.4881e-06,  2.0645e-05,  4.9586e-07,\n",
      "          6.1311e-08,  5.1878e-06,  1.2307e-05, -5.6192e-07, -6.8642e-07,\n",
      "          3.7236e-06,  5.0237e-09, -2.3857e-08, -7.7664e-08, -8.3865e-09,\n",
      "          1.5629e-06,  2.6984e-07,  5.2005e-08, -3.9755e-06, -1.7400e-06,\n",
      "         -1.7448e-07,  1.1338e-07, -1.3464e-06, -4.8910e-06,  1.4077e-07,\n",
      "         -4.3792e-07, -4.1272e-08,  7.2853e-08, -8.9580e-08, -4.6144e-09,\n",
      "         -7.4259e-07, -1.5908e-05,  3.6718e-06,  9.4396e-06,  5.6789e-08,\n",
      "          1.7059e-09,  2.5213e-06, -4.6034e-08,  5.2949e-08,  5.4982e-08,\n",
      "         -5.4109e-08,  2.1200e-06,  3.1344e-07,  9.6897e-08, -2.8529e-07,\n",
      "         -1.0593e-07, -1.7308e-06,  2.8259e-07, -3.0128e-06,  2.8530e-07,\n",
      "         -4.6122e-07, -6.8373e-06, -3.2311e-07, -2.2906e-08, -2.5482e-07,\n",
      "         -2.2110e-09, -1.1769e-06,  7.2977e-08, -5.7473e-07, -1.1251e-07,\n",
      "          1.0031e-07,  6.1135e-07, -1.1308e-08, -2.1041e-06,  2.4877e-08,\n",
      "          4.2130e-06, -6.9241e-09, -9.0283e-08, -7.0564e-08,  3.2511e-09,\n",
      "          2.1240e-06,  2.7619e-07, -2.1908e-06,  5.6387e-08,  2.2908e-08,\n",
      "         -5.2895e-09,  5.7410e-06, -2.2186e-07,  8.8366e-09, -2.1829e-06,\n",
      "         -1.5606e-07, -7.4005e-09,  1.4464e-07,  6.0941e-08,  1.8124e-07,\n",
      "         -5.9934e-06, -4.1108e-08, -9.2321e-06, -1.2959e-07, -2.9012e-06,\n",
      "          4.0460e-06,  4.3070e-06,  1.2612e-07,  5.2592e-08, -2.4102e-05,\n",
      "          1.0350e-07, -1.0870e-08, -5.3403e-07,  2.3729e-08, -6.2486e-06,\n",
      "          6.4484e-07, -2.0996e-05,  2.8569e-06,  6.8823e-08,  4.8678e-07,\n",
      "         -8.1302e-07,  3.0096e-07, -5.7045e-07]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 9,  7],\n",
      "         [ 7, 10],\n",
      "         [ 4, 10],\n",
      "         [ 9,  5],\n",
      "         [ 7,  3],\n",
      "         [ 7,  7],\n",
      "         [10,  2],\n",
      "         [ 8,  1],\n",
      "         [ 9,  4],\n",
      "         [ 2,  1],\n",
      "         [ 4, 18],\n",
      "         [ 5, 10],\n",
      "         [ 8, 18],\n",
      "         [ 6,  3],\n",
      "         [ 3,  2],\n",
      "         [ 3,  0],\n",
      "         [ 6, 16],\n",
      "         [ 1,  3],\n",
      "         [ 9, 10],\n",
      "         [ 6,  2],\n",
      "         [ 2, 14],\n",
      "         [ 2,  6],\n",
      "         [ 6,  1],\n",
      "         [ 7, 16],\n",
      "         [ 3, 16],\n",
      "         [ 9,  3],\n",
      "         [ 6, 18],\n",
      "         [ 3, 18],\n",
      "         [10, 15],\n",
      "         [10,  0],\n",
      "         [ 3, 17],\n",
      "         [ 8,  0],\n",
      "         [ 4, 13],\n",
      "         [ 3, 11],\n",
      "         [ 1, 17],\n",
      "         [ 8, 11],\n",
      "         [ 0, 13],\n",
      "         [ 8,  8],\n",
      "         [ 9,  6],\n",
      "         [ 1,  6],\n",
      "         [ 1, 15],\n",
      "         [ 5,  7],\n",
      "         [ 2, 16],\n",
      "         [ 6,  6],\n",
      "         [ 1, 12],\n",
      "         [ 7, 18],\n",
      "         [ 2, 18],\n",
      "         [ 8,  2],\n",
      "         [ 0, 16],\n",
      "         [ 3,  5],\n",
      "         [ 6,  5],\n",
      "         [ 0,  7],\n",
      "         [ 4, 14],\n",
      "         [ 5, 12],\n",
      "         [ 0,  3],\n",
      "         [ 1,  7],\n",
      "         [ 2,  7],\n",
      "         [ 3,  8],\n",
      "         [10, 18],\n",
      "         [10,  8],\n",
      "         [ 9,  8],\n",
      "         [ 9, 14],\n",
      "         [ 1,  9],\n",
      "         [ 2,  0],\n",
      "         [ 4,  9],\n",
      "         [10,  1],\n",
      "         [ 5,  3],\n",
      "         [ 5,  9],\n",
      "         [ 0, 10],\n",
      "         [ 6, 17],\n",
      "         [ 7,  8],\n",
      "         [ 1, 10],\n",
      "         [ 4,  2],\n",
      "         [ 0,  5],\n",
      "         [ 6, 10],\n",
      "         [ 0,  6],\n",
      "         [ 7, 14],\n",
      "         [ 5,  5],\n",
      "         [ 5, 16],\n",
      "         [ 4, 15],\n",
      "         [ 9, 13],\n",
      "         [ 6,  9],\n",
      "         [ 5,  4],\n",
      "         [ 8,  7],\n",
      "         [ 3,  4],\n",
      "         [ 7,  1],\n",
      "         [ 5,  6],\n",
      "         [ 1,  2],\n",
      "         [10, 12],\n",
      "         [ 1, 18],\n",
      "         [10, 14],\n",
      "         [ 7,  5],\n",
      "         [ 4,  6],\n",
      "         [ 0, 11],\n",
      "         [ 9,  0],\n",
      "         [ 1,  5],\n",
      "         [ 3, 13],\n",
      "         [ 2,  3],\n",
      "         [ 6,  0],\n",
      "         [ 9, 11],\n",
      "         [ 8,  5],\n",
      "         [ 3,  9],\n",
      "         [10, 11],\n",
      "         [ 7, 15],\n",
      "         [ 7, 11],\n",
      "         [ 2, 15],\n",
      "         [ 4, 11],\n",
      "         [ 3,  7],\n",
      "         [ 6, 14],\n",
      "         [ 6, 12],\n",
      "         [ 7,  4],\n",
      "         [ 3, 12],\n",
      "         [ 8, 14],\n",
      "         [ 5, 17],\n",
      "         [ 4, 16],\n",
      "         [ 1,  8],\n",
      "         [ 9, 16],\n",
      "         [ 0,  8],\n",
      "         [ 2, 13],\n",
      "         [ 9, 15],\n",
      "         [ 9, 17],\n",
      "         [ 5,  2],\n",
      "         [ 8, 12],\n",
      "         [ 0,  4],\n",
      "         [ 4,  7],\n",
      "         [ 4, 17],\n",
      "         [ 1, 14],\n",
      "         [10, 17],\n",
      "         [ 5, 15],\n",
      "         [ 6, 11],\n",
      "         [ 4, 12],\n",
      "         [ 5, 18],\n",
      "         [ 2,  8],\n",
      "         [ 3,  1],\n",
      "         [ 4,  3],\n",
      "         [ 3,  3],\n",
      "         [ 2, 11],\n",
      "         [ 8, 15],\n",
      "         [ 4,  4],\n",
      "         [ 7, 13],\n",
      "         [ 0, 14],\n",
      "         [10,  5],\n",
      "         [ 0, 15],\n",
      "         [ 5, 13],\n",
      "         [ 6,  4],\n",
      "         [ 7, 12],\n",
      "         [ 9, 12],\n",
      "         [ 6,  8],\n",
      "         [ 2, 12],\n",
      "         [ 3, 15],\n",
      "         [ 5, 14],\n",
      "         [ 4,  5],\n",
      "         [ 0,  2],\n",
      "         [ 2, 17],\n",
      "         [ 0,  1],\n",
      "         [ 0,  9],\n",
      "         [ 9,  1],\n",
      "         [ 2, 10],\n",
      "         [ 0, 18],\n",
      "         [ 7,  0],\n",
      "         [ 1, 13],\n",
      "         [ 8,  6],\n",
      "         [ 1,  4],\n",
      "         [ 5,  8],\n",
      "         [ 5,  0],\n",
      "         [10, 16],\n",
      "         [ 4,  0],\n",
      "         [ 1, 16],\n",
      "         [ 4,  1],\n",
      "         [ 7,  6],\n",
      "         [ 6, 13],\n",
      "         [ 2,  5],\n",
      "         [ 7,  2],\n",
      "         [ 3, 10],\n",
      "         [ 5,  1],\n",
      "         [ 8, 16],\n",
      "         [10,  9],\n",
      "         [ 7, 17],\n",
      "         [ 1,  1],\n",
      "         [ 1,  0],\n",
      "         [ 6,  7],\n",
      "         [ 8, 13],\n",
      "         [ 8,  3],\n",
      "         [10, 10],\n",
      "         [10,  6],\n",
      "         [ 0,  0],\n",
      "         [ 3,  6],\n",
      "         [ 8,  4],\n",
      "         [ 6, 15],\n",
      "         [ 9, 18],\n",
      "         [ 4,  8],\n",
      "         [10,  7],\n",
      "         [ 5, 11],\n",
      "         [ 7,  9],\n",
      "         [10,  3],\n",
      "         [ 2,  4],\n",
      "         [ 2,  2],\n",
      "         [ 8, 17],\n",
      "         [ 8, 10],\n",
      "         [ 9,  9],\n",
      "         [ 1, 11],\n",
      "         [ 0, 17],\n",
      "         [ 3, 14],\n",
      "         [10, 13],\n",
      "         [ 0, 12],\n",
      "         [ 2,  9],\n",
      "         [ 8,  9],\n",
      "         [ 9,  2],\n",
      "         [10,  4]]]), (11, 19)), 'cls_output': tensor([[0.9757]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446673/164085027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 1817\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch_junsheng_39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29fd19f11c6b89e267402bb3227bc1208f7e2c9719aa03eba13baf7684fe5867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
