{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    debug = False\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # root_path = r'E:\\\\Download\\\\xiangguan' # 存放数据的根目录\n",
    "    root_path = r'/home/junsheng/data/xiangguan' # 存放数据的根目录\n",
    "    n_fold = 5\n",
    "\n",
    "    # wandb \n",
    "    wandb_name = \"vilt|290 sensor only\"\n",
    "    \n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Sensor\n",
    "    # senser_input_num = 11 # 翔冠的传感器参数\n",
    "    senser_input_num = 19 # 天航的传感器参数\n",
    "    \n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-4 # 0.01 ->1e-4\n",
    "    decay_power = 1\n",
    "    max_epoch = 50\n",
    "    max_steps = 25000\n",
    "    warmup_steps = 2500\n",
    "    end_lr = 0\n",
    "    lr_mult = 1  # multiply lr for downstream heads\n",
    "    # T_max = 8000/train_batch_size*max_epoch \n",
    "    T_max = 1000/train_batch_size*max_epoch \n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "# config = vars(config)\n",
    "# config = dict(config)\n",
    "config\n",
    "\n",
    "if config.debug:\n",
    "    config.max_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)  # 为CPU设置随机种子\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)  # 为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有GPU设置随机种子\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "setup_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorViLTransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pretrained weights exist or were found for this model. Using random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 sensorViLTransformerSS(\n",
      "  (sensor_linear): Linear(in_features=19, out_features=768, bias=True)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n",
      "1 Linear(in_features=19, out_features=768, bias=True)\n",
      "2 Embedding(2, 768)\n",
      "3 VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "4 PatchEmbed(\n",
      "  (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      ")\n",
      "5 Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "6 Dropout(p=0.1, inplace=False)\n",
      "7 ModuleList(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "8 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "9 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "10 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "11 Linear(in_features=768, out_features=2304, bias=True)\n",
      "12 Dropout(p=0.0, inplace=False)\n",
      "13 Linear(in_features=768, out_features=768, bias=True)\n",
      "14 Dropout(p=0.1, inplace=False)\n",
      "15 Identity()\n",
      "16 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "17 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "18 Linear(in_features=768, out_features=3072, bias=True)\n",
      "19 GELU(approximate=none)\n",
      "20 Linear(in_features=3072, out_features=768, bias=True)\n",
      "21 Dropout(p=0.1, inplace=False)\n",
      "22 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "23 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "24 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "25 Linear(in_features=768, out_features=2304, bias=True)\n",
      "26 Dropout(p=0.0, inplace=False)\n",
      "27 Linear(in_features=768, out_features=768, bias=True)\n",
      "28 Dropout(p=0.1, inplace=False)\n",
      "29 Identity()\n",
      "30 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "31 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "32 Linear(in_features=768, out_features=3072, bias=True)\n",
      "33 GELU(approximate=none)\n",
      "34 Linear(in_features=3072, out_features=768, bias=True)\n",
      "35 Dropout(p=0.1, inplace=False)\n",
      "36 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "37 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "38 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "39 Linear(in_features=768, out_features=2304, bias=True)\n",
      "40 Dropout(p=0.0, inplace=False)\n",
      "41 Linear(in_features=768, out_features=768, bias=True)\n",
      "42 Dropout(p=0.1, inplace=False)\n",
      "43 Identity()\n",
      "44 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "45 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "46 Linear(in_features=768, out_features=3072, bias=True)\n",
      "47 GELU(approximate=none)\n",
      "48 Linear(in_features=3072, out_features=768, bias=True)\n",
      "49 Dropout(p=0.1, inplace=False)\n",
      "50 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "51 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "52 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "53 Linear(in_features=768, out_features=2304, bias=True)\n",
      "54 Dropout(p=0.0, inplace=False)\n",
      "55 Linear(in_features=768, out_features=768, bias=True)\n",
      "56 Dropout(p=0.1, inplace=False)\n",
      "57 Identity()\n",
      "58 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "59 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "60 Linear(in_features=768, out_features=3072, bias=True)\n",
      "61 GELU(approximate=none)\n",
      "62 Linear(in_features=3072, out_features=768, bias=True)\n",
      "63 Dropout(p=0.1, inplace=False)\n",
      "64 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "65 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "66 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "67 Linear(in_features=768, out_features=2304, bias=True)\n",
      "68 Dropout(p=0.0, inplace=False)\n",
      "69 Linear(in_features=768, out_features=768, bias=True)\n",
      "70 Dropout(p=0.1, inplace=False)\n",
      "71 Identity()\n",
      "72 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "73 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "74 Linear(in_features=768, out_features=3072, bias=True)\n",
      "75 GELU(approximate=none)\n",
      "76 Linear(in_features=3072, out_features=768, bias=True)\n",
      "77 Dropout(p=0.1, inplace=False)\n",
      "78 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "79 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "80 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "81 Linear(in_features=768, out_features=2304, bias=True)\n",
      "82 Dropout(p=0.0, inplace=False)\n",
      "83 Linear(in_features=768, out_features=768, bias=True)\n",
      "84 Dropout(p=0.1, inplace=False)\n",
      "85 Identity()\n",
      "86 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "87 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "88 Linear(in_features=768, out_features=3072, bias=True)\n",
      "89 GELU(approximate=none)\n",
      "90 Linear(in_features=3072, out_features=768, bias=True)\n",
      "91 Dropout(p=0.1, inplace=False)\n",
      "92 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "93 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "94 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "95 Linear(in_features=768, out_features=2304, bias=True)\n",
      "96 Dropout(p=0.0, inplace=False)\n",
      "97 Linear(in_features=768, out_features=768, bias=True)\n",
      "98 Dropout(p=0.1, inplace=False)\n",
      "99 Identity()\n",
      "100 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "101 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "102 Linear(in_features=768, out_features=3072, bias=True)\n",
      "103 GELU(approximate=none)\n",
      "104 Linear(in_features=3072, out_features=768, bias=True)\n",
      "105 Dropout(p=0.1, inplace=False)\n",
      "106 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "107 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "108 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "109 Linear(in_features=768, out_features=2304, bias=True)\n",
      "110 Dropout(p=0.0, inplace=False)\n",
      "111 Linear(in_features=768, out_features=768, bias=True)\n",
      "112 Dropout(p=0.1, inplace=False)\n",
      "113 Identity()\n",
      "114 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "115 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "116 Linear(in_features=768, out_features=3072, bias=True)\n",
      "117 GELU(approximate=none)\n",
      "118 Linear(in_features=3072, out_features=768, bias=True)\n",
      "119 Dropout(p=0.1, inplace=False)\n",
      "120 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "121 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "122 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "123 Linear(in_features=768, out_features=2304, bias=True)\n",
      "124 Dropout(p=0.0, inplace=False)\n",
      "125 Linear(in_features=768, out_features=768, bias=True)\n",
      "126 Dropout(p=0.1, inplace=False)\n",
      "127 Identity()\n",
      "128 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "129 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "130 Linear(in_features=768, out_features=3072, bias=True)\n",
      "131 GELU(approximate=none)\n",
      "132 Linear(in_features=3072, out_features=768, bias=True)\n",
      "133 Dropout(p=0.1, inplace=False)\n",
      "134 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "135 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "136 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "137 Linear(in_features=768, out_features=2304, bias=True)\n",
      "138 Dropout(p=0.0, inplace=False)\n",
      "139 Linear(in_features=768, out_features=768, bias=True)\n",
      "140 Dropout(p=0.1, inplace=False)\n",
      "141 Identity()\n",
      "142 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "143 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "144 Linear(in_features=768, out_features=3072, bias=True)\n",
      "145 GELU(approximate=none)\n",
      "146 Linear(in_features=3072, out_features=768, bias=True)\n",
      "147 Dropout(p=0.1, inplace=False)\n",
      "148 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "149 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "150 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "151 Linear(in_features=768, out_features=2304, bias=True)\n",
      "152 Dropout(p=0.0, inplace=False)\n",
      "153 Linear(in_features=768, out_features=768, bias=True)\n",
      "154 Dropout(p=0.1, inplace=False)\n",
      "155 Identity()\n",
      "156 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "157 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "158 Linear(in_features=768, out_features=3072, bias=True)\n",
      "159 GELU(approximate=none)\n",
      "160 Linear(in_features=3072, out_features=768, bias=True)\n",
      "161 Dropout(p=0.1, inplace=False)\n",
      "162 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "163 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "164 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "165 Linear(in_features=768, out_features=2304, bias=True)\n",
      "166 Dropout(p=0.0, inplace=False)\n",
      "167 Linear(in_features=768, out_features=768, bias=True)\n",
      "168 Dropout(p=0.1, inplace=False)\n",
      "169 Identity()\n",
      "170 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "171 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "172 Linear(in_features=768, out_features=3072, bias=True)\n",
      "173 GELU(approximate=none)\n",
      "174 Linear(in_features=3072, out_features=768, bias=True)\n",
      "175 Dropout(p=0.1, inplace=False)\n",
      "176 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "177 Linear(in_features=768, out_features=768, bias=True)\n",
      "178 Tanh()\n",
      "179 Pooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "180 Linear(in_features=768, out_features=768, bias=True)\n",
      "181 Tanh()\n",
      "182 Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# model = sensorOnlyViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "\n",
    "model = sensorViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "model.to(config.device)\n",
    "print(config.device)\n",
    "for i,m in enumerate(model.modules()):\n",
    "    print(i,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"test_rice_model.pth\")\n",
    "model.eval()\n",
    "device = config.device\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = dict()\n",
    "    batch[\"image\"] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8512, 0.4091, 0.4373, 0.4750, 0.1375, 0.8209, 0.0185, 0.3641, 0.6405,\n",
      "        0.4498, 0.3304, 0.3734, 0.0022, 0.7310, 0.2874, 0.9734, 0.3282, 0.9755,\n",
      "        0.5969])\n",
      "img.shape: torch.Size([1, 3, 352, 608])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3621890/3499233738.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
      "/tmp/ipykernel_3621890/3953777603.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensor_feats': tensor([[[-1.1892e+00, -6.5976e-01,  1.1355e-01,  1.6988e+00,  6.4553e-01,\n",
      "          -1.7900e+00,  5.4092e-01, -2.7702e-01,  2.7851e-01, -1.4476e+00,\n",
      "           5.9511e-01,  2.5263e-02, -2.5724e-01,  1.4464e+00, -2.8418e-01,\n",
      "           5.0998e-01,  3.2094e-01, -1.8048e-01, -9.3112e-01, -8.5211e-01,\n",
      "           2.1284e-01,  4.6768e-01, -1.7012e+00,  2.4799e-01, -2.1181e-01,\n",
      "          -2.7804e-03,  7.7204e-01,  5.2347e-01,  2.4244e-01, -9.8112e-01,\n",
      "          -1.7004e+00,  1.5200e+00, -2.3774e-01, -4.0605e-01,  9.8693e-01,\n",
      "           1.9028e-01, -7.6656e-01,  1.3540e+00,  1.0020e+00,  4.6315e-01,\n",
      "           9.6439e-01, -6.5193e-01,  8.0270e-01, -1.9090e+00, -4.5478e-01,\n",
      "           4.1064e-01,  5.2737e-01, -4.6365e-01, -1.4041e-01,  3.0134e-02,\n",
      "           1.0750e+00,  9.3958e-01,  2.1100e-01,  6.5412e-01,  8.4500e-02,\n",
      "          -3.9005e-02,  2.3801e-01, -3.5697e-01,  4.8972e-01,  2.4392e-01,\n",
      "          -2.9463e-01, -7.4681e-01, -3.7742e-01,  8.9952e-01, -7.0212e-01,\n",
      "          -2.6352e+00, -1.5202e+00, -6.5950e-01,  1.6278e-01, -3.2669e-01,\n",
      "           2.7633e-01, -1.9148e-01, -1.0449e+00, -2.0497e-01,  3.7251e-01,\n",
      "           3.1001e-01, -5.8179e-01, -2.8983e-01,  1.5089e+00, -3.1549e-02,\n",
      "          -2.0225e-01,  1.3472e+00, -4.8737e-01,  1.0179e-01,  6.4992e-01,\n",
      "          -1.0923e+00, -8.9972e-02,  8.7497e-01,  1.9445e-01,  1.4434e-01,\n",
      "          -1.7882e+00, -2.6514e+00,  7.8621e-01, -1.1762e+00, -6.6349e-01,\n",
      "          -1.0614e+00, -7.5455e-01,  3.9699e-01, -3.1852e-01, -3.5621e-01,\n",
      "          -1.4758e+00, -4.3002e-01,  3.9010e-01,  1.8947e-01, -9.4848e-01,\n",
      "           1.7389e-01,  6.8452e-01,  4.0326e-01, -9.2242e-01, -2.6169e-01,\n",
      "           1.5323e-01, -1.0776e+00,  1.0774e+00, -1.7541e+00,  2.7950e+00,\n",
      "           7.6849e-01, -2.6891e-01,  9.8618e-01,  1.1081e+00,  1.0112e+00,\n",
      "           1.1704e+00, -1.2239e+00,  6.0292e-02,  8.6192e-01, -1.2588e+00,\n",
      "          -1.9222e+00,  3.3721e-01, -7.7348e-01,  1.9657e-02, -5.9169e-01,\n",
      "          -1.1876e+00,  8.6868e-02,  8.4649e-01,  2.0241e+00,  2.7799e-01,\n",
      "           2.4029e-01,  1.8778e+00, -1.7558e+00, -1.1161e+00,  3.1091e-01,\n",
      "          -7.6185e-01,  2.4229e-01,  3.7542e-01, -6.7666e-02,  1.4800e-01,\n",
      "           1.2567e+00,  4.0943e-01, -9.9586e-01,  5.1350e-01, -8.1519e-01,\n",
      "          -1.6717e+00,  1.7919e-01, -3.4080e-01,  8.2078e-01, -1.2848e+00,\n",
      "           8.6047e-01, -4.2296e-01,  1.9494e-01,  3.9988e-01, -1.0550e+00,\n",
      "          -1.7054e-02, -6.4787e-01, -1.4370e+00,  1.0313e+00,  1.5628e+00,\n",
      "          -7.2584e-02,  7.7819e-01,  9.8558e-02, -5.1827e-01, -1.7120e-01,\n",
      "           9.9369e-01,  9.6554e-01, -1.6156e+00,  5.4309e-01,  4.7737e-01,\n",
      "          -1.2302e+00, -1.2374e+00, -1.0787e+00, -3.1143e-02,  3.2141e-01,\n",
      "           1.6320e+00, -6.4428e-01,  2.5207e-01,  1.4776e-01,  1.1074e+00,\n",
      "           1.6835e+00,  5.7848e-01, -1.9242e-01,  7.0872e-01,  6.6173e-01,\n",
      "          -1.3548e-01,  1.8456e+00, -1.9358e-01,  1.7734e+00, -1.6905e+00,\n",
      "          -1.1290e+00, -1.7398e+00,  1.2429e+00, -2.4863e-01,  1.0033e+00,\n",
      "           2.2472e-01,  3.0588e+00, -1.7475e-01, -2.3991e-01,  8.4222e-01,\n",
      "           1.5985e+00,  1.8636e+00,  2.2192e-01, -1.9212e+00, -8.2105e-01,\n",
      "           5.9976e-01,  1.1664e+00, -7.4646e-02,  7.2531e-02,  5.1472e-01,\n",
      "          -3.9410e-01,  6.3974e-03,  1.8117e-01,  6.9195e-01, -1.4533e+00,\n",
      "          -1.0345e-01,  7.8432e-01,  9.0284e-01, -6.5436e-02, -7.5388e-01,\n",
      "           1.9806e+00, -1.6391e+00,  1.3085e+00, -5.3483e-01, -7.2895e-02,\n",
      "          -9.1360e-01, -3.6242e-01, -4.9765e-01, -3.3512e-01,  5.8515e-01,\n",
      "          -2.4907e-01,  4.2167e-01, -1.1471e+00,  1.2737e+00, -2.1089e-01,\n",
      "           1.1791e-01,  8.4220e-01, -4.1429e-01,  5.4248e-01, -4.6041e-02,\n",
      "           3.7507e-01, -7.4256e-01,  5.4081e-01, -9.1999e-01, -2.1650e-01,\n",
      "           2.6138e-01,  2.1997e-01,  9.8866e-01, -7.8723e-01,  6.9525e-01,\n",
      "          -2.5791e-01,  1.1033e+00,  1.5810e+00, -1.7659e-01,  8.0285e-01,\n",
      "          -5.1848e-01, -1.6123e-01,  1.1886e+00, -3.0692e-01,  3.6850e-01,\n",
      "           1.2512e+00, -1.0247e-01,  2.1423e-01, -1.3030e+00,  6.0236e-01,\n",
      "           2.7740e+00,  1.4582e+00, -4.3736e-01, -1.2283e+00, -1.8261e+00,\n",
      "          -1.4521e+00,  9.9683e-02, -4.9254e-02,  5.4651e-01,  4.7473e-01,\n",
      "          -1.5455e-01, -1.6973e+00,  1.2150e+00, -1.3408e+00,  6.6180e-01,\n",
      "          -8.9291e-01, -2.6023e+00, -3.1264e-01,  4.8273e-01,  2.0204e+00,\n",
      "           5.8501e-01, -7.6636e-01,  6.3527e-01,  1.6442e+00,  1.3418e+00,\n",
      "           7.1619e-01, -8.1757e-01,  6.0127e-01, -1.3784e+00,  7.8262e-01,\n",
      "           5.8825e-01,  1.1508e+00, -1.1275e+00, -8.9633e-01,  5.8768e-01,\n",
      "           1.3410e+00, -5.5719e-01, -4.2001e-01,  1.0631e+00,  8.1754e-01,\n",
      "           1.0667e+00,  3.0906e+00,  6.3215e-01, -8.0305e-01, -3.9481e-01,\n",
      "          -8.3273e-01, -8.6025e-01,  1.7472e+00, -1.2412e+00,  5.4892e-01,\n",
      "           7.1599e-01, -1.0599e+00,  3.4544e-01, -5.3643e-01,  1.3973e+00,\n",
      "           1.7066e+00, -1.6276e+00, -1.2566e+00, -1.3620e-01, -1.3125e+00,\n",
      "          -9.8953e-01,  1.2682e+00, -5.0914e-01, -2.3653e-01,  8.3902e-01,\n",
      "           1.9017e+00,  3.3224e-01,  1.7189e+00, -1.4509e+00, -7.3020e-01,\n",
      "          -9.9293e-01,  1.2262e-01, -5.0353e-01,  1.0058e+00,  1.6892e+00,\n",
      "           9.2245e-01,  3.3444e-01,  4.9420e-01, -7.6859e-01, -4.4616e-01,\n",
      "          -1.3486e+00,  1.1077e+00,  1.7789e-01,  7.5600e-01, -1.1737e+00,\n",
      "          -1.8930e-01, -5.6007e-01, -7.6188e-02, -6.8361e-01,  2.5639e-01,\n",
      "          -1.7181e+00,  1.1239e+00,  1.2407e+00,  2.3965e+00, -1.4918e+00,\n",
      "           1.0499e-01, -2.0078e+00, -1.1041e+00, -3.8195e-01,  9.2138e-01,\n",
      "           7.6959e-01,  2.2054e+00,  1.4967e+00,  4.3759e-01, -2.6007e-01,\n",
      "          -2.0767e-01, -2.2273e-01,  9.4174e-01, -1.7796e-01, -1.2589e+00,\n",
      "           7.1431e-01, -1.8730e+00,  3.6514e-01,  4.0258e-01, -1.3488e+00,\n",
      "          -6.9113e-01, -5.6116e-01,  2.0621e+00, -1.5733e+00, -1.4766e-01,\n",
      "          -9.9509e-01, -1.6099e+00,  1.1728e+00, -1.0750e+00, -7.2133e-01,\n",
      "           1.4143e+00, -5.2205e-01, -7.9497e-01, -1.4594e+00,  1.5183e-01,\n",
      "           1.6857e+00,  1.2551e+00, -8.4683e-01, -1.2247e+00, -3.0202e-01,\n",
      "          -1.7056e+00,  2.6580e+00,  5.4291e-01,  2.0800e+00,  1.7370e+00,\n",
      "           3.7774e-01,  1.9718e+00, -2.6616e+00,  1.1088e+00,  1.1108e+00,\n",
      "          -6.8477e-02,  8.6968e-01,  1.5784e-01, -1.0667e-01,  8.1488e-01,\n",
      "           3.3524e-03, -3.6480e-01,  3.3194e-01,  6.1434e-01,  6.9309e-01,\n",
      "          -2.4727e-01, -7.5749e-01,  1.9534e+00, -1.5293e+00, -6.9762e-01,\n",
      "           6.6057e-01,  3.1654e-01,  1.1961e+00,  8.8604e-03, -1.2555e+00,\n",
      "          -1.3148e-01, -7.4226e-01,  4.3312e-01,  4.5247e-01, -4.8259e-01,\n",
      "           1.0250e+00, -1.6794e+00, -7.2300e-01,  1.2521e-01,  1.3775e+00,\n",
      "          -2.9052e-01,  1.2680e+00,  6.9828e-01,  6.8823e-01,  1.3427e+00,\n",
      "          -1.3814e+00, -4.4449e-01,  1.6361e+00, -3.6094e-01, -1.7881e+00,\n",
      "           5.8473e-01, -4.5965e-02, -1.6062e+00, -3.0050e-01, -6.0386e-01,\n",
      "           8.4926e-01,  8.4911e-02, -1.3333e+00, -1.3828e+00, -1.0995e+00,\n",
      "           1.4847e-01, -7.5114e-01,  1.4152e+00, -2.0215e-01,  2.0296e+00,\n",
      "          -2.9836e-01, -8.6522e-01,  4.4161e-01, -8.8722e-01, -2.0999e-01,\n",
      "           2.6375e-01, -1.3266e-01, -9.2877e-01,  1.6286e+00, -1.8092e+00,\n",
      "          -2.2359e-02,  1.1351e+00,  9.0318e-01,  1.8595e-01,  4.7434e-01,\n",
      "           2.5736e-01,  4.3071e-01,  2.9305e-01, -2.1835e-01,  8.7699e-02,\n",
      "          -1.2244e+00,  1.9205e-02, -2.1803e+00, -1.1187e+00,  2.1350e-01,\n",
      "          -2.4716e-01, -1.1024e+00, -2.3205e+00,  4.6802e-02, -7.5182e-02,\n",
      "           7.1941e-01, -8.0002e-01,  8.7485e-02,  6.3998e-01, -7.3360e-01,\n",
      "           1.5542e+00,  1.7424e-01,  1.6992e+00, -3.5947e-01,  9.6195e-01,\n",
      "          -5.9740e-01,  1.3797e+00, -8.3129e-01,  5.6835e-01,  1.8097e+00,\n",
      "           1.3418e+00, -4.1910e-01,  2.0670e-01, -1.5393e+00, -1.2554e-01,\n",
      "          -6.9065e-01, -1.2042e+00, -6.2306e-01,  7.5365e-01,  1.1850e+00,\n",
      "           3.6064e-01, -2.6067e+00,  7.6451e-01,  2.1797e-01,  7.5748e-01,\n",
      "          -4.9843e-01, -9.2029e-01, -1.5074e+00, -7.5299e-02,  1.8872e+00,\n",
      "           1.9505e+00,  2.7778e-01,  1.6871e-01, -7.0903e-01, -5.4164e-01,\n",
      "          -8.7813e-01, -7.8321e-01, -6.1648e-01, -1.7675e+00, -2.4498e-01,\n",
      "           1.4681e+00, -4.1651e-01,  1.1244e-01,  1.7507e+00,  3.6447e-01,\n",
      "          -5.7791e-01, -1.1334e+00, -4.2454e-01, -5.3813e-01,  5.6288e-01,\n",
      "          -1.5036e+00,  1.3460e+00, -4.7335e-01,  1.0969e+00, -2.8190e+00,\n",
      "           9.3514e-01, -1.4553e+00,  7.1827e-01,  3.3491e-01,  1.0435e+00,\n",
      "          -4.9615e-02,  2.4440e-01, -1.4104e+00,  4.2460e-01,  2.7608e-01,\n",
      "           2.3900e-02, -5.1994e-01, -1.3002e+00, -9.1520e-01,  5.5301e-01,\n",
      "          -5.8321e-01,  2.3495e-01, -7.0703e-01, -6.1154e-01, -1.5655e+00,\n",
      "           7.6630e-01,  3.8279e-01,  5.3895e-01, -9.7071e-01,  2.0581e+00,\n",
      "           8.2198e-01, -3.9919e-02,  6.0308e-01, -8.6691e-01, -1.3494e-01,\n",
      "          -3.8830e-01, -1.0868e+00,  4.6085e-01, -4.0112e-01, -4.7707e-02,\n",
      "           1.4370e+00, -2.3434e-01, -1.0239e+00,  9.5962e-01,  9.9363e-01,\n",
      "          -5.4572e-01,  8.9614e-01, -5.9271e-01, -1.2286e+00, -2.6211e-01,\n",
      "           3.9993e-02, -2.1332e+00,  5.3733e-01, -4.7483e-02, -4.4168e-01,\n",
      "          -1.0309e+00, -9.6291e-01,  5.4291e-01, -8.6411e-01,  1.0588e-01,\n",
      "          -2.2221e-01, -2.3978e-01, -5.4686e-01,  7.4716e-02,  1.1677e+00,\n",
      "          -8.2023e-02,  1.0459e+00, -5.5499e-02, -4.8691e-01, -6.5535e-01,\n",
      "           2.2251e+00,  5.6068e-01,  4.6953e-01, -3.2975e-01,  4.8959e-01,\n",
      "          -4.7281e-01,  1.5543e+00, -2.3988e+00, -3.9454e-01, -1.5744e-01,\n",
      "          -1.3201e+00, -6.2052e-01,  8.1950e-02,  1.3280e+00, -8.9996e-01,\n",
      "          -4.2930e-01, -1.1980e-01, -2.3977e-01, -5.7209e-01, -1.3212e+00,\n",
      "           1.7179e+00,  2.0062e-01,  1.4441e+00, -5.3458e-01,  2.0745e-01,\n",
      "          -3.5128e-02,  4.7739e-01, -2.5294e+00, -5.1139e-01, -4.0047e-01,\n",
      "          -3.8376e-01,  1.6647e-02, -4.1708e-01,  6.5592e-01, -1.5207e+00,\n",
      "          -1.9883e+00,  3.9510e-01,  7.1256e-01,  2.4498e+00,  4.6055e-01,\n",
      "           1.4327e+00, -2.2883e+00,  2.2939e-01,  4.4246e-01,  8.2480e-01,\n",
      "           1.9109e-01,  3.4499e-01,  1.2239e+00,  2.1394e-01, -1.3200e+00,\n",
      "           3.4810e-01,  1.0938e-01,  1.0492e-01,  2.9914e-01,  4.0923e-01,\n",
      "           1.7798e+00, -3.3724e-01, -1.7231e-01,  1.8108e+00,  1.0028e+00,\n",
      "          -5.7314e-01,  3.3628e-01, -1.2925e+00, -1.2507e+00, -1.1936e+00,\n",
      "           9.4100e-01,  1.2371e+00, -7.7598e-01,  7.9586e-02,  5.6826e-01,\n",
      "          -1.9500e+00,  9.6223e-01,  3.1784e-02,  3.0983e-01,  1.0796e+00,\n",
      "           8.8559e-01, -4.5496e-01,  7.5853e-01, -4.0506e-01,  5.7884e-01,\n",
      "          -4.0796e-01,  8.9393e-01,  7.1601e-02,  1.0084e+00,  1.0376e+00,\n",
      "           1.6719e-01, -1.9182e+00, -3.5906e-01,  7.3688e-01, -8.3268e-01,\n",
      "           5.0973e-01, -8.0124e-01, -1.0270e+00,  5.7102e-01, -1.5759e+00,\n",
      "          -1.4464e+00, -5.3797e-01,  9.7515e-01,  2.7326e-01, -3.7181e-01,\n",
      "           1.0818e+00, -3.0677e-03, -1.7264e+00, -3.7697e-01, -3.1935e-01,\n",
      "          -1.0622e+00, -1.2206e+00, -1.4525e+00,  2.4015e-01, -9.6455e-02,\n",
      "           4.7613e-01, -8.3563e-01,  1.3794e+00,  9.6878e-01, -8.1944e-03,\n",
      "           3.0491e-01, -9.5900e-01, -1.1549e+00,  4.2825e-01, -3.4394e-01,\n",
      "           9.9989e-01,  5.1163e-01, -7.8477e-02, -2.1041e+00, -1.4764e+00,\n",
      "           8.1209e-02,  2.1403e-01,  4.6271e-01,  1.0576e+00, -6.9871e-01,\n",
      "          -2.5416e+00, -1.1972e+00, -1.5776e-01, -8.1160e-01,  1.3049e+00,\n",
      "           2.0386e+00, -8.6790e-02, -1.2023e-01,  9.8273e-02, -6.7059e-01,\n",
      "           1.1069e+00,  1.1844e+00, -2.7074e-01]]], device='cuda:0'), 'image_feats': tensor([[[-1.2693,  0.4055,  0.2240,  ...,  1.1863,  0.6150, -1.0773],\n",
      "         [-0.8015, -0.6367, -0.6537,  ...,  1.3623, -0.2785,  0.8892],\n",
      "         [-0.3794,  0.9936, -0.2741,  ...,  0.3610,  0.2437, -0.9179],\n",
      "         ...,\n",
      "         [-0.8210, -0.6582, -0.2339,  ...,  0.5180,  0.9392, -0.3051],\n",
      "         [-0.2075, -0.8335, -0.6344,  ...,  1.5077,  0.5324, -1.0325],\n",
      "         [-1.3459, -0.5539,  0.2051,  ...,  0.3782, -0.8727, -0.2990]]],\n",
      "       device='cuda:0'), 'cls_feats': tensor([[-8.3400e-03,  2.3320e-01,  4.5333e-01,  5.5402e-01,  3.8571e-02,\n",
      "          1.5833e-01, -4.7001e-01, -4.9392e-01, -2.3619e-01, -3.5636e-01,\n",
      "         -5.0079e-01,  3.4554e-01,  3.0524e-01, -5.6742e-01,  6.1169e-01,\n",
      "         -7.0018e-01,  6.4245e-01,  4.5595e-01, -2.5633e-01,  8.4042e-01,\n",
      "          1.2231e-01, -5.2733e-01,  7.5025e-02, -3.8777e-02, -5.8132e-01,\n",
      "         -2.2431e-01, -6.1278e-01,  8.4037e-01, -2.3057e-02,  1.8107e-01,\n",
      "         -5.5789e-01,  6.0436e-01,  5.3807e-01,  2.0087e-01,  5.9067e-01,\n",
      "         -1.4018e-01,  7.2656e-01, -1.7734e-01,  3.6370e-01, -6.1147e-01,\n",
      "         -7.6904e-01, -9.6929e-02, -6.4704e-02, -2.3383e-01, -4.5237e-01,\n",
      "          7.9840e-01,  4.4710e-01,  1.0033e-01, -7.2084e-02,  7.2358e-01,\n",
      "         -3.6146e-03,  6.3798e-01, -3.7973e-01,  5.7669e-01, -1.3259e-01,\n",
      "          1.3221e-01,  3.0297e-01, -2.3417e-01,  8.2022e-01,  5.6151e-01,\n",
      "         -2.1310e-01, -2.3417e-01, -2.5586e-01,  9.7433e-02,  1.3531e-01,\n",
      "         -8.1945e-01,  3.5121e-01,  4.6086e-01, -1.9275e-01, -1.5557e-01,\n",
      "         -1.0717e-01, -1.6661e-01, -2.9041e-02, -2.6392e-01, -7.2731e-01,\n",
      "         -1.1585e-01,  8.6312e-01,  5.2976e-01,  6.1951e-01,  3.5904e-01,\n",
      "         -8.1449e-01,  5.5953e-01, -4.9069e-01, -7.8293e-01, -5.6853e-01,\n",
      "          2.8399e-01, -7.9588e-01,  7.5259e-01, -6.8028e-01, -6.8182e-02,\n",
      "          2.0817e-01,  7.2645e-01, -6.5629e-01,  1.8416e-02, -8.5214e-01,\n",
      "          5.4140e-01,  2.1315e-01, -7.0869e-02,  2.8412e-01,  7.5466e-01,\n",
      "         -2.3495e-01, -5.6356e-01,  8.2485e-01,  2.8345e-01,  5.2719e-01,\n",
      "         -8.4882e-02,  4.4225e-01, -1.8388e-01,  3.0784e-01,  7.2345e-01,\n",
      "         -1.3049e-01,  5.6332e-01,  3.4121e-02, -6.8818e-02, -5.6319e-01,\n",
      "         -1.2499e-02, -5.0219e-01, -1.4571e-01,  2.2545e-01,  2.7992e-01,\n",
      "         -5.0555e-01, -3.9253e-01,  2.1448e-02, -2.7925e-01, -5.8051e-02,\n",
      "          6.3847e-01, -7.5019e-01, -4.1270e-01, -1.3589e-01,  4.6587e-01,\n",
      "          6.0953e-01,  5.9646e-01, -6.7452e-01, -8.4436e-01,  5.1749e-02,\n",
      "          1.2498e-01,  4.5520e-01,  5.7751e-01, -1.8654e-01,  5.9917e-01,\n",
      "          4.6265e-01, -6.1062e-02,  5.2114e-02, -4.5587e-02, -9.3382e-01,\n",
      "          5.0900e-01,  2.4050e-01, -2.0388e-01,  6.1198e-01,  2.2761e-01,\n",
      "          1.4319e-01,  2.5037e-01,  5.6430e-02, -2.5219e-01,  8.0048e-01,\n",
      "         -7.3549e-01,  7.7076e-01, -3.9721e-01, -5.2733e-01,  5.7817e-01,\n",
      "         -3.5578e-01, -1.4347e-01,  3.7519e-01, -7.1391e-02, -6.0063e-01,\n",
      "          1.7748e-01, -6.1716e-01, -3.1791e-01,  3.0127e-01,  1.0608e-01,\n",
      "         -6.6726e-02, -7.4736e-01, -2.5893e-01, -2.5793e-01, -4.6065e-01,\n",
      "         -2.2433e-01,  2.0482e-01, -5.7719e-01, -5.7008e-02, -1.2262e-02,\n",
      "         -2.4820e-01,  5.0108e-01, -4.9168e-01,  5.4597e-01,  5.4641e-01,\n",
      "          5.7416e-01,  5.7421e-01,  2.3311e-01,  1.7609e-01, -6.1218e-01,\n",
      "          8.0887e-01, -3.6009e-01, -8.3695e-01,  3.4549e-01, -8.0307e-01,\n",
      "          9.2362e-01,  2.2475e-01, -8.3775e-01, -1.8749e-01,  2.9219e-01,\n",
      "          1.5796e-01,  3.6750e-01, -5.9533e-01,  7.0431e-01,  5.8798e-01,\n",
      "          1.5024e-01, -3.4755e-02, -2.4945e-01,  7.9384e-01, -1.5253e-01,\n",
      "          6.8856e-02, -6.8892e-01,  9.1940e-02, -8.0325e-01, -4.4550e-01,\n",
      "         -2.4436e-04,  6.8946e-01, -5.2144e-01,  6.3644e-02,  7.0748e-01,\n",
      "         -8.9708e-01, -5.2300e-01,  2.4038e-01,  4.6762e-01, -7.1681e-02,\n",
      "          8.6326e-01,  1.6002e-01,  1.0880e-02, -2.3463e-01, -6.2105e-01,\n",
      "          5.4706e-01, -1.0055e-01, -2.7209e-02, -6.8887e-01, -4.5742e-01,\n",
      "         -1.9849e-01, -1.3945e-01,  7.2203e-01,  3.0530e-01, -8.2775e-01,\n",
      "          4.2519e-01, -7.1007e-01,  1.7123e-01, -6.6995e-01,  8.6315e-01,\n",
      "         -7.2780e-01, -1.1408e-01, -7.3891e-02,  6.7122e-01, -7.7014e-02,\n",
      "         -6.8232e-02,  2.3617e-01,  7.8876e-01, -2.1647e-01, -2.8731e-01,\n",
      "         -6.5843e-01, -5.5849e-01, -2.2978e-01, -6.8522e-01, -3.8968e-01,\n",
      "          2.5112e-01,  6.5587e-01,  1.5223e-01, -1.8280e-01,  2.7876e-01,\n",
      "          7.2794e-01,  3.4684e-01, -1.3766e-01,  6.6569e-01, -1.2614e-01,\n",
      "         -7.4786e-01,  6.3524e-01,  4.4099e-01, -6.6760e-02,  7.1262e-01,\n",
      "         -4.6714e-01,  2.6982e-01, -3.4326e-01,  3.3189e-01, -5.0612e-01,\n",
      "          1.3527e-01,  2.5815e-01, -6.5139e-01, -4.7356e-01,  3.2802e-01,\n",
      "          1.3428e-01, -3.5023e-01, -7.8008e-02,  3.3969e-01, -2.3193e-01,\n",
      "         -1.0559e-01, -6.5966e-01, -3.7761e-01, -2.8132e-01,  2.7461e-01,\n",
      "         -4.4873e-01,  8.5923e-01,  3.3685e-01,  5.9613e-01,  7.8493e-01,\n",
      "         -6.0348e-01, -4.1834e-01, -4.1697e-01, -8.2753e-01,  3.1106e-01,\n",
      "          2.4513e-01,  5.5055e-01,  6.0477e-01,  4.5455e-01,  1.6340e-01,\n",
      "          8.4950e-01, -8.4724e-01, -6.1725e-01,  6.0191e-01,  1.0217e-01,\n",
      "          4.9418e-02,  6.9491e-02,  2.0913e-01,  8.0947e-01,  5.2278e-01,\n",
      "          4.5785e-01, -3.0879e-01, -1.2158e-01, -5.4447e-01, -4.3249e-01,\n",
      "         -5.5586e-01,  5.3550e-01, -3.2711e-01,  4.2559e-01, -1.6877e-01,\n",
      "          4.8075e-01,  3.5199e-01,  7.5981e-01, -4.0080e-01, -3.5094e-01,\n",
      "         -3.8679e-01,  1.3735e-01, -1.8362e-01, -3.6195e-01,  3.4231e-01,\n",
      "          1.0296e-01, -9.0886e-02, -2.7194e-01,  5.5230e-01,  2.2869e-02,\n",
      "          3.1730e-01, -5.3534e-01, -3.9964e-01, -9.9126e-02, -9.2402e-02,\n",
      "         -1.4325e-01,  3.1146e-02,  4.1021e-02, -5.3764e-01, -7.0076e-01,\n",
      "         -4.4761e-01,  4.4954e-01, -7.5194e-01, -3.4193e-01, -5.1853e-01,\n",
      "          5.3874e-01,  5.4377e-01, -3.8688e-01, -7.0853e-01,  3.2711e-01,\n",
      "         -3.0001e-01,  2.6156e-01,  9.4065e-01,  6.9384e-01, -4.8359e-01,\n",
      "          5.6425e-01,  4.2759e-01,  6.9157e-01,  6.0489e-01, -6.7528e-01,\n",
      "          1.8733e-01,  3.9554e-01,  2.2427e-01,  2.9801e-01,  5.4851e-01,\n",
      "          9.4143e-02, -3.8245e-01,  5.1571e-01, -1.2645e-01,  1.7681e-01,\n",
      "          5.9473e-01, -2.6452e-01, -1.8503e-01,  3.8724e-01, -1.5649e-01,\n",
      "          2.8783e-01, -9.7056e-02, -7.7655e-01,  7.5964e-01,  2.9835e-01,\n",
      "         -3.9789e-01,  9.8917e-01,  3.2223e-02, -2.2047e-01, -8.6530e-01,\n",
      "          3.0919e-01,  2.0665e-01,  4.0888e-01, -6.1813e-01, -3.0477e-01,\n",
      "         -2.5175e-01,  3.7821e-01,  3.6680e-01, -6.8169e-01,  1.9824e-01,\n",
      "          8.0808e-02, -3.7737e-01, -9.3932e-02,  3.3334e-01,  4.2678e-01,\n",
      "         -4.8205e-01, -2.8191e-02,  4.5644e-01, -4.7022e-01,  3.0677e-01,\n",
      "         -5.6415e-02, -6.1849e-01, -9.0720e-01, -1.7934e-01,  2.9866e-01,\n",
      "          3.6431e-01,  9.8513e-02, -5.1195e-01, -7.8650e-01, -7.3111e-01,\n",
      "          8.3075e-01,  1.4411e-01,  7.9141e-01, -3.8425e-01,  2.9768e-02,\n",
      "         -3.5761e-03, -3.4607e-01, -6.0884e-01,  6.4994e-01, -6.8544e-01,\n",
      "         -1.1251e-01,  5.2305e-01, -9.2085e-01, -2.0236e-01,  4.6865e-01,\n",
      "          3.3320e-01, -1.2263e-01, -5.3800e-01, -4.5006e-01,  3.0938e-01,\n",
      "          2.2471e-03, -1.4224e-01, -8.0307e-01,  2.8234e-01,  3.3189e-01,\n",
      "          5.4893e-01,  5.9112e-01,  4.5165e-01, -2.8124e-01,  5.9604e-01,\n",
      "          4.9512e-01,  9.8185e-02, -3.7201e-01, -1.3573e-01, -5.1154e-01,\n",
      "         -2.4184e-01,  1.6651e-01,  7.5329e-02, -4.1988e-01,  2.6941e-01,\n",
      "         -1.6168e-01, -3.7290e-01,  5.8385e-01,  7.2520e-01, -8.3227e-01,\n",
      "         -2.0144e-01, -4.4038e-01,  3.4847e-01,  6.2679e-01, -3.9482e-01,\n",
      "         -5.2578e-01, -3.8307e-01, -1.0288e-01,  8.8206e-01, -3.2210e-01,\n",
      "          1.2524e-01,  3.0338e-01,  2.6860e-01, -4.7813e-01, -3.2345e-01,\n",
      "         -3.4914e-01, -5.3558e-01,  1.9872e-01, -1.3810e-01,  2.8988e-01,\n",
      "         -3.8883e-02,  5.6502e-01, -5.0547e-01,  5.8684e-01, -4.6707e-02,\n",
      "          5.4887e-01,  5.1536e-01, -1.0218e-01,  6.7019e-01, -7.2684e-01,\n",
      "         -2.2386e-01,  7.3435e-01, -3.5622e-01, -3.1084e-01,  2.5890e-01,\n",
      "         -2.4241e-01, -5.3714e-01,  4.1942e-01, -2.2025e-01, -1.5965e-02,\n",
      "         -2.6410e-02, -6.5652e-02, -6.9304e-01, -6.8810e-01,  5.3441e-01,\n",
      "          3.6831e-01, -4.6702e-01,  3.4142e-01,  2.5980e-01,  9.1448e-01,\n",
      "          7.7438e-01,  1.4187e-02, -6.9750e-01,  5.2112e-01,  7.7724e-01,\n",
      "          1.0517e-01, -7.6853e-01,  9.5416e-01,  5.6707e-01, -2.8879e-01,\n",
      "         -4.7280e-01, -6.7412e-01,  4.6437e-01,  4.0814e-01, -8.2265e-01,\n",
      "         -5.4362e-01,  9.0260e-01, -6.4449e-01, -7.2057e-01, -9.0409e-01,\n",
      "          9.8060e-03,  5.2854e-01,  8.1086e-01,  4.5050e-01,  7.1133e-02,\n",
      "         -5.4774e-01,  8.4859e-01,  7.1953e-01,  5.6859e-01, -1.6130e-01,\n",
      "          3.6293e-01,  1.5999e-01, -3.7926e-02, -2.6418e-01,  3.2780e-01,\n",
      "         -1.3689e-01,  3.9824e-01, -1.2027e-01,  6.1155e-01,  7.2896e-01,\n",
      "          4.8541e-01,  1.4741e-01, -1.5154e-01, -2.4609e-01,  4.8528e-02,\n",
      "         -1.0140e-01,  2.9583e-01,  5.9826e-01,  8.6192e-01,  7.1877e-01,\n",
      "          7.6041e-01,  1.9948e-01,  2.8783e-01, -4.7068e-01, -1.5230e-01,\n",
      "          2.4610e-01,  3.4616e-03,  4.7590e-01, -7.3572e-01, -7.6098e-01,\n",
      "          2.6696e-01, -4.6656e-01, -2.4566e-01,  6.6039e-01, -3.8140e-01,\n",
      "         -6.2750e-01, -3.8237e-01,  3.6562e-01, -4.3495e-01,  1.3467e-01,\n",
      "          6.0130e-01,  7.9261e-01, -8.3178e-01,  3.1790e-01,  1.3159e-01,\n",
      "         -7.9701e-01, -1.1775e-01, -6.3873e-01, -2.1086e-01, -2.3258e-01,\n",
      "         -5.3156e-01,  3.3286e-01,  8.1526e-01, -4.3844e-01,  3.2639e-01,\n",
      "          9.7609e-02, -5.1534e-01, -6.8800e-01, -6.9641e-01,  5.5792e-01,\n",
      "          8.2711e-01, -1.3057e-01,  3.7223e-01,  2.0329e-01, -5.4834e-02,\n",
      "         -6.4503e-02, -5.0966e-01, -3.6631e-01,  2.3825e-01,  3.1707e-02,\n",
      "         -4.4660e-01, -2.8927e-01, -1.3797e-01,  8.3240e-01, -3.0501e-01,\n",
      "         -7.6753e-02, -9.2488e-02,  6.8593e-01, -7.7139e-01,  4.8987e-01,\n",
      "         -4.4748e-01,  5.2852e-01, -3.9958e-01, -3.7727e-01,  2.8509e-01,\n",
      "         -1.7732e-01,  7.7739e-01,  2.2132e-01, -5.2018e-01,  6.3665e-02,\n",
      "         -6.0569e-01,  2.1933e-01,  4.7343e-01, -4.1043e-01,  5.8108e-01,\n",
      "         -5.8489e-01, -6.1310e-02,  4.8332e-01,  5.2006e-01,  5.9793e-02,\n",
      "         -1.1925e-01,  3.7783e-01, -9.8463e-04,  4.5813e-01, -2.6405e-01,\n",
      "         -8.9054e-03, -4.1918e-01,  5.1343e-01, -4.5937e-01,  8.4139e-01,\n",
      "         -2.9079e-01,  8.3941e-03,  1.0972e-01,  6.0226e-01,  3.3879e-01,\n",
      "         -3.3510e-01,  4.2272e-01, -6.9985e-01,  1.2472e-02, -4.3417e-02,\n",
      "          1.4421e-01,  6.3132e-01,  4.0151e-01,  7.8548e-01, -1.9672e-01,\n",
      "         -4.8407e-02,  3.1980e-01,  2.0020e-01,  6.1134e-01, -7.6349e-01,\n",
      "         -3.5718e-01,  4.5841e-01, -4.0013e-01, -3.1395e-01,  1.1840e-01,\n",
      "          6.9116e-01,  5.7144e-01,  1.6956e-01, -5.1936e-01,  8.3431e-01,\n",
      "         -1.0711e-01,  4.5577e-01,  1.2853e-02,  6.5769e-01, -3.6613e-01,\n",
      "         -5.5190e-01,  3.1951e-01, -5.8926e-01,  2.3041e-01,  7.6299e-01,\n",
      "         -1.3093e-01,  4.2858e-01,  2.1361e-01, -5.3237e-01, -4.5500e-02,\n",
      "         -5.4188e-01, -6.0976e-01, -3.2665e-01, -3.5083e-01, -4.6364e-01,\n",
      "          5.0562e-01, -4.7856e-01,  3.7099e-01,  5.9245e-01, -2.8516e-01,\n",
      "          6.6443e-01, -4.8156e-01,  2.4210e-01,  7.8640e-01, -3.4058e-01,\n",
      "          2.5574e-01,  5.6914e-01,  3.2873e-01, -5.5641e-01,  8.6877e-01,\n",
      "          4.7508e-01, -6.2078e-01, -1.0472e-01,  1.1879e-01,  7.9911e-01,\n",
      "          6.0122e-01,  4.8312e-01,  8.0932e-01, -5.0765e-01,  2.0111e-01,\n",
      "         -5.2194e-01, -1.1874e-01,  2.7837e-01, -4.6084e-01,  6.7779e-01,\n",
      "          5.9167e-01,  8.9654e-03,  1.4303e-01,  2.0068e-01, -2.7502e-01,\n",
      "          5.0987e-01, -2.6632e-01,  5.9764e-01,  5.5110e-01, -9.2395e-02,\n",
      "         -3.1345e-01,  4.7439e-01,  4.1934e-01,  1.9679e-01,  4.8936e-01,\n",
      "         -3.5057e-02, -4.5802e-01,  4.9391e-01,  5.2335e-01,  4.1640e-01,\n",
      "         -5.7906e-01,  3.2268e-01, -6.0248e-01]], device='cuda:0'), 'raw_cls_feats': tensor([[-1.1892e+00, -6.5976e-01,  1.1355e-01,  1.6988e+00,  6.4553e-01,\n",
      "         -1.7900e+00,  5.4092e-01, -2.7702e-01,  2.7851e-01, -1.4476e+00,\n",
      "          5.9511e-01,  2.5263e-02, -2.5724e-01,  1.4464e+00, -2.8418e-01,\n",
      "          5.0998e-01,  3.2094e-01, -1.8048e-01, -9.3112e-01, -8.5211e-01,\n",
      "          2.1284e-01,  4.6768e-01, -1.7012e+00,  2.4799e-01, -2.1181e-01,\n",
      "         -2.7804e-03,  7.7204e-01,  5.2347e-01,  2.4244e-01, -9.8112e-01,\n",
      "         -1.7004e+00,  1.5200e+00, -2.3774e-01, -4.0605e-01,  9.8693e-01,\n",
      "          1.9028e-01, -7.6656e-01,  1.3540e+00,  1.0020e+00,  4.6315e-01,\n",
      "          9.6439e-01, -6.5193e-01,  8.0270e-01, -1.9090e+00, -4.5478e-01,\n",
      "          4.1064e-01,  5.2737e-01, -4.6365e-01, -1.4041e-01,  3.0134e-02,\n",
      "          1.0750e+00,  9.3958e-01,  2.1100e-01,  6.5412e-01,  8.4500e-02,\n",
      "         -3.9005e-02,  2.3801e-01, -3.5697e-01,  4.8972e-01,  2.4392e-01,\n",
      "         -2.9463e-01, -7.4681e-01, -3.7742e-01,  8.9952e-01, -7.0212e-01,\n",
      "         -2.6352e+00, -1.5202e+00, -6.5950e-01,  1.6278e-01, -3.2669e-01,\n",
      "          2.7633e-01, -1.9148e-01, -1.0449e+00, -2.0497e-01,  3.7251e-01,\n",
      "          3.1001e-01, -5.8179e-01, -2.8983e-01,  1.5089e+00, -3.1549e-02,\n",
      "         -2.0225e-01,  1.3472e+00, -4.8737e-01,  1.0179e-01,  6.4992e-01,\n",
      "         -1.0923e+00, -8.9972e-02,  8.7497e-01,  1.9445e-01,  1.4434e-01,\n",
      "         -1.7882e+00, -2.6514e+00,  7.8621e-01, -1.1762e+00, -6.6349e-01,\n",
      "         -1.0614e+00, -7.5455e-01,  3.9699e-01, -3.1852e-01, -3.5621e-01,\n",
      "         -1.4758e+00, -4.3002e-01,  3.9010e-01,  1.8947e-01, -9.4848e-01,\n",
      "          1.7389e-01,  6.8452e-01,  4.0326e-01, -9.2242e-01, -2.6169e-01,\n",
      "          1.5323e-01, -1.0776e+00,  1.0774e+00, -1.7541e+00,  2.7950e+00,\n",
      "          7.6849e-01, -2.6891e-01,  9.8618e-01,  1.1081e+00,  1.0112e+00,\n",
      "          1.1704e+00, -1.2239e+00,  6.0292e-02,  8.6192e-01, -1.2588e+00,\n",
      "         -1.9222e+00,  3.3721e-01, -7.7348e-01,  1.9657e-02, -5.9169e-01,\n",
      "         -1.1876e+00,  8.6868e-02,  8.4649e-01,  2.0241e+00,  2.7799e-01,\n",
      "          2.4029e-01,  1.8778e+00, -1.7558e+00, -1.1161e+00,  3.1091e-01,\n",
      "         -7.6185e-01,  2.4229e-01,  3.7542e-01, -6.7666e-02,  1.4800e-01,\n",
      "          1.2567e+00,  4.0943e-01, -9.9586e-01,  5.1350e-01, -8.1519e-01,\n",
      "         -1.6717e+00,  1.7919e-01, -3.4080e-01,  8.2078e-01, -1.2848e+00,\n",
      "          8.6047e-01, -4.2296e-01,  1.9494e-01,  3.9988e-01, -1.0550e+00,\n",
      "         -1.7054e-02, -6.4787e-01, -1.4370e+00,  1.0313e+00,  1.5628e+00,\n",
      "         -7.2584e-02,  7.7819e-01,  9.8558e-02, -5.1827e-01, -1.7120e-01,\n",
      "          9.9369e-01,  9.6554e-01, -1.6156e+00,  5.4309e-01,  4.7737e-01,\n",
      "         -1.2302e+00, -1.2374e+00, -1.0787e+00, -3.1143e-02,  3.2141e-01,\n",
      "          1.6320e+00, -6.4428e-01,  2.5207e-01,  1.4776e-01,  1.1074e+00,\n",
      "          1.6835e+00,  5.7848e-01, -1.9242e-01,  7.0872e-01,  6.6173e-01,\n",
      "         -1.3548e-01,  1.8456e+00, -1.9358e-01,  1.7734e+00, -1.6905e+00,\n",
      "         -1.1290e+00, -1.7398e+00,  1.2429e+00, -2.4863e-01,  1.0033e+00,\n",
      "          2.2472e-01,  3.0588e+00, -1.7475e-01, -2.3991e-01,  8.4222e-01,\n",
      "          1.5985e+00,  1.8636e+00,  2.2192e-01, -1.9212e+00, -8.2105e-01,\n",
      "          5.9976e-01,  1.1664e+00, -7.4646e-02,  7.2531e-02,  5.1472e-01,\n",
      "         -3.9410e-01,  6.3974e-03,  1.8117e-01,  6.9195e-01, -1.4533e+00,\n",
      "         -1.0345e-01,  7.8432e-01,  9.0284e-01, -6.5436e-02, -7.5388e-01,\n",
      "          1.9806e+00, -1.6391e+00,  1.3085e+00, -5.3483e-01, -7.2895e-02,\n",
      "         -9.1360e-01, -3.6242e-01, -4.9765e-01, -3.3512e-01,  5.8515e-01,\n",
      "         -2.4907e-01,  4.2167e-01, -1.1471e+00,  1.2737e+00, -2.1089e-01,\n",
      "          1.1791e-01,  8.4220e-01, -4.1429e-01,  5.4248e-01, -4.6041e-02,\n",
      "          3.7507e-01, -7.4256e-01,  5.4081e-01, -9.1999e-01, -2.1650e-01,\n",
      "          2.6138e-01,  2.1997e-01,  9.8866e-01, -7.8723e-01,  6.9525e-01,\n",
      "         -2.5791e-01,  1.1033e+00,  1.5810e+00, -1.7659e-01,  8.0285e-01,\n",
      "         -5.1848e-01, -1.6123e-01,  1.1886e+00, -3.0692e-01,  3.6850e-01,\n",
      "          1.2512e+00, -1.0247e-01,  2.1423e-01, -1.3030e+00,  6.0236e-01,\n",
      "          2.7740e+00,  1.4582e+00, -4.3736e-01, -1.2283e+00, -1.8261e+00,\n",
      "         -1.4521e+00,  9.9683e-02, -4.9254e-02,  5.4651e-01,  4.7473e-01,\n",
      "         -1.5455e-01, -1.6973e+00,  1.2150e+00, -1.3408e+00,  6.6180e-01,\n",
      "         -8.9291e-01, -2.6023e+00, -3.1264e-01,  4.8273e-01,  2.0204e+00,\n",
      "          5.8501e-01, -7.6636e-01,  6.3527e-01,  1.6442e+00,  1.3418e+00,\n",
      "          7.1619e-01, -8.1757e-01,  6.0127e-01, -1.3784e+00,  7.8262e-01,\n",
      "          5.8825e-01,  1.1508e+00, -1.1275e+00, -8.9633e-01,  5.8768e-01,\n",
      "          1.3410e+00, -5.5719e-01, -4.2001e-01,  1.0631e+00,  8.1754e-01,\n",
      "          1.0667e+00,  3.0906e+00,  6.3215e-01, -8.0305e-01, -3.9481e-01,\n",
      "         -8.3273e-01, -8.6025e-01,  1.7472e+00, -1.2412e+00,  5.4892e-01,\n",
      "          7.1599e-01, -1.0599e+00,  3.4544e-01, -5.3643e-01,  1.3973e+00,\n",
      "          1.7066e+00, -1.6276e+00, -1.2566e+00, -1.3620e-01, -1.3125e+00,\n",
      "         -9.8953e-01,  1.2682e+00, -5.0914e-01, -2.3653e-01,  8.3902e-01,\n",
      "          1.9017e+00,  3.3224e-01,  1.7189e+00, -1.4509e+00, -7.3020e-01,\n",
      "         -9.9293e-01,  1.2262e-01, -5.0353e-01,  1.0058e+00,  1.6892e+00,\n",
      "          9.2245e-01,  3.3444e-01,  4.9420e-01, -7.6859e-01, -4.4616e-01,\n",
      "         -1.3486e+00,  1.1077e+00,  1.7789e-01,  7.5600e-01, -1.1737e+00,\n",
      "         -1.8930e-01, -5.6007e-01, -7.6188e-02, -6.8361e-01,  2.5639e-01,\n",
      "         -1.7181e+00,  1.1239e+00,  1.2407e+00,  2.3965e+00, -1.4918e+00,\n",
      "          1.0499e-01, -2.0078e+00, -1.1041e+00, -3.8195e-01,  9.2138e-01,\n",
      "          7.6959e-01,  2.2054e+00,  1.4967e+00,  4.3759e-01, -2.6007e-01,\n",
      "         -2.0767e-01, -2.2273e-01,  9.4174e-01, -1.7796e-01, -1.2589e+00,\n",
      "          7.1431e-01, -1.8730e+00,  3.6514e-01,  4.0258e-01, -1.3488e+00,\n",
      "         -6.9113e-01, -5.6116e-01,  2.0621e+00, -1.5733e+00, -1.4766e-01,\n",
      "         -9.9509e-01, -1.6099e+00,  1.1728e+00, -1.0750e+00, -7.2133e-01,\n",
      "          1.4143e+00, -5.2205e-01, -7.9497e-01, -1.4594e+00,  1.5183e-01,\n",
      "          1.6857e+00,  1.2551e+00, -8.4683e-01, -1.2247e+00, -3.0202e-01,\n",
      "         -1.7056e+00,  2.6580e+00,  5.4291e-01,  2.0800e+00,  1.7370e+00,\n",
      "          3.7774e-01,  1.9718e+00, -2.6616e+00,  1.1088e+00,  1.1108e+00,\n",
      "         -6.8477e-02,  8.6968e-01,  1.5784e-01, -1.0667e-01,  8.1488e-01,\n",
      "          3.3524e-03, -3.6480e-01,  3.3194e-01,  6.1434e-01,  6.9309e-01,\n",
      "         -2.4727e-01, -7.5749e-01,  1.9534e+00, -1.5293e+00, -6.9762e-01,\n",
      "          6.6057e-01,  3.1654e-01,  1.1961e+00,  8.8604e-03, -1.2555e+00,\n",
      "         -1.3148e-01, -7.4226e-01,  4.3312e-01,  4.5247e-01, -4.8259e-01,\n",
      "          1.0250e+00, -1.6794e+00, -7.2300e-01,  1.2521e-01,  1.3775e+00,\n",
      "         -2.9052e-01,  1.2680e+00,  6.9828e-01,  6.8823e-01,  1.3427e+00,\n",
      "         -1.3814e+00, -4.4449e-01,  1.6361e+00, -3.6094e-01, -1.7881e+00,\n",
      "          5.8473e-01, -4.5965e-02, -1.6062e+00, -3.0050e-01, -6.0386e-01,\n",
      "          8.4926e-01,  8.4911e-02, -1.3333e+00, -1.3828e+00, -1.0995e+00,\n",
      "          1.4847e-01, -7.5114e-01,  1.4152e+00, -2.0215e-01,  2.0296e+00,\n",
      "         -2.9836e-01, -8.6522e-01,  4.4161e-01, -8.8722e-01, -2.0999e-01,\n",
      "          2.6375e-01, -1.3266e-01, -9.2877e-01,  1.6286e+00, -1.8092e+00,\n",
      "         -2.2359e-02,  1.1351e+00,  9.0318e-01,  1.8595e-01,  4.7434e-01,\n",
      "          2.5736e-01,  4.3071e-01,  2.9305e-01, -2.1835e-01,  8.7699e-02,\n",
      "         -1.2244e+00,  1.9205e-02, -2.1803e+00, -1.1187e+00,  2.1350e-01,\n",
      "         -2.4716e-01, -1.1024e+00, -2.3205e+00,  4.6802e-02, -7.5182e-02,\n",
      "          7.1941e-01, -8.0002e-01,  8.7485e-02,  6.3998e-01, -7.3360e-01,\n",
      "          1.5542e+00,  1.7424e-01,  1.6992e+00, -3.5947e-01,  9.6195e-01,\n",
      "         -5.9740e-01,  1.3797e+00, -8.3129e-01,  5.6835e-01,  1.8097e+00,\n",
      "          1.3418e+00, -4.1910e-01,  2.0670e-01, -1.5393e+00, -1.2554e-01,\n",
      "         -6.9065e-01, -1.2042e+00, -6.2306e-01,  7.5365e-01,  1.1850e+00,\n",
      "          3.6064e-01, -2.6067e+00,  7.6451e-01,  2.1797e-01,  7.5748e-01,\n",
      "         -4.9843e-01, -9.2029e-01, -1.5074e+00, -7.5299e-02,  1.8872e+00,\n",
      "          1.9505e+00,  2.7778e-01,  1.6871e-01, -7.0903e-01, -5.4164e-01,\n",
      "         -8.7813e-01, -7.8321e-01, -6.1648e-01, -1.7675e+00, -2.4498e-01,\n",
      "          1.4681e+00, -4.1651e-01,  1.1244e-01,  1.7507e+00,  3.6447e-01,\n",
      "         -5.7791e-01, -1.1334e+00, -4.2454e-01, -5.3813e-01,  5.6288e-01,\n",
      "         -1.5036e+00,  1.3460e+00, -4.7335e-01,  1.0969e+00, -2.8190e+00,\n",
      "          9.3514e-01, -1.4553e+00,  7.1827e-01,  3.3491e-01,  1.0435e+00,\n",
      "         -4.9615e-02,  2.4440e-01, -1.4104e+00,  4.2460e-01,  2.7608e-01,\n",
      "          2.3900e-02, -5.1994e-01, -1.3002e+00, -9.1520e-01,  5.5301e-01,\n",
      "         -5.8321e-01,  2.3495e-01, -7.0703e-01, -6.1154e-01, -1.5655e+00,\n",
      "          7.6630e-01,  3.8279e-01,  5.3895e-01, -9.7071e-01,  2.0581e+00,\n",
      "          8.2198e-01, -3.9919e-02,  6.0308e-01, -8.6691e-01, -1.3494e-01,\n",
      "         -3.8830e-01, -1.0868e+00,  4.6085e-01, -4.0112e-01, -4.7707e-02,\n",
      "          1.4370e+00, -2.3434e-01, -1.0239e+00,  9.5962e-01,  9.9363e-01,\n",
      "         -5.4572e-01,  8.9614e-01, -5.9271e-01, -1.2286e+00, -2.6211e-01,\n",
      "          3.9993e-02, -2.1332e+00,  5.3733e-01, -4.7483e-02, -4.4168e-01,\n",
      "         -1.0309e+00, -9.6291e-01,  5.4291e-01, -8.6411e-01,  1.0588e-01,\n",
      "         -2.2221e-01, -2.3978e-01, -5.4686e-01,  7.4716e-02,  1.1677e+00,\n",
      "         -8.2023e-02,  1.0459e+00, -5.5499e-02, -4.8691e-01, -6.5535e-01,\n",
      "          2.2251e+00,  5.6068e-01,  4.6953e-01, -3.2975e-01,  4.8959e-01,\n",
      "         -4.7281e-01,  1.5543e+00, -2.3988e+00, -3.9454e-01, -1.5744e-01,\n",
      "         -1.3201e+00, -6.2052e-01,  8.1950e-02,  1.3280e+00, -8.9996e-01,\n",
      "         -4.2930e-01, -1.1980e-01, -2.3977e-01, -5.7209e-01, -1.3212e+00,\n",
      "          1.7179e+00,  2.0062e-01,  1.4441e+00, -5.3458e-01,  2.0745e-01,\n",
      "         -3.5128e-02,  4.7739e-01, -2.5294e+00, -5.1139e-01, -4.0047e-01,\n",
      "         -3.8376e-01,  1.6647e-02, -4.1708e-01,  6.5592e-01, -1.5207e+00,\n",
      "         -1.9883e+00,  3.9510e-01,  7.1256e-01,  2.4498e+00,  4.6055e-01,\n",
      "          1.4327e+00, -2.2883e+00,  2.2939e-01,  4.4246e-01,  8.2480e-01,\n",
      "          1.9109e-01,  3.4499e-01,  1.2239e+00,  2.1394e-01, -1.3200e+00,\n",
      "          3.4810e-01,  1.0938e-01,  1.0492e-01,  2.9914e-01,  4.0923e-01,\n",
      "          1.7798e+00, -3.3724e-01, -1.7231e-01,  1.8108e+00,  1.0028e+00,\n",
      "         -5.7314e-01,  3.3628e-01, -1.2925e+00, -1.2507e+00, -1.1936e+00,\n",
      "          9.4100e-01,  1.2371e+00, -7.7598e-01,  7.9586e-02,  5.6826e-01,\n",
      "         -1.9500e+00,  9.6223e-01,  3.1784e-02,  3.0983e-01,  1.0796e+00,\n",
      "          8.8559e-01, -4.5496e-01,  7.5853e-01, -4.0506e-01,  5.7884e-01,\n",
      "         -4.0796e-01,  8.9393e-01,  7.1601e-02,  1.0084e+00,  1.0376e+00,\n",
      "          1.6719e-01, -1.9182e+00, -3.5906e-01,  7.3688e-01, -8.3268e-01,\n",
      "          5.0973e-01, -8.0124e-01, -1.0270e+00,  5.7102e-01, -1.5759e+00,\n",
      "         -1.4464e+00, -5.3797e-01,  9.7515e-01,  2.7326e-01, -3.7181e-01,\n",
      "          1.0818e+00, -3.0677e-03, -1.7264e+00, -3.7697e-01, -3.1935e-01,\n",
      "         -1.0622e+00, -1.2206e+00, -1.4525e+00,  2.4015e-01, -9.6455e-02,\n",
      "          4.7613e-01, -8.3563e-01,  1.3794e+00,  9.6878e-01, -8.1944e-03,\n",
      "          3.0491e-01, -9.5900e-01, -1.1549e+00,  4.2825e-01, -3.4394e-01,\n",
      "          9.9989e-01,  5.1163e-01, -7.8477e-02, -2.1041e+00, -1.4764e+00,\n",
      "          8.1209e-02,  2.1403e-01,  4.6271e-01,  1.0576e+00, -6.9871e-01,\n",
      "         -2.5416e+00, -1.1972e+00, -1.5776e-01, -8.1160e-01,  1.3049e+00,\n",
      "          2.0386e+00, -8.6790e-02, -1.2023e-01,  9.8273e-02, -6.7059e-01,\n",
      "          1.1069e+00,  1.1844e+00, -2.7074e-01]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 6, 13],\n",
      "         [ 6,  4],\n",
      "         [ 5, 10],\n",
      "         [ 8, 15],\n",
      "         [ 5, 11],\n",
      "         [ 2, 12],\n",
      "         [ 4, 14],\n",
      "         [ 2,  9],\n",
      "         [ 8, 17],\n",
      "         [ 3,  0],\n",
      "         [ 7, 13],\n",
      "         [ 9, 13],\n",
      "         [ 4, 12],\n",
      "         [ 5,  7],\n",
      "         [ 0,  5],\n",
      "         [ 1, 13],\n",
      "         [ 1,  6],\n",
      "         [ 0, 11],\n",
      "         [ 3, 15],\n",
      "         [ 0, 10],\n",
      "         [ 4,  0],\n",
      "         [ 4, 10],\n",
      "         [10, 13],\n",
      "         [ 2, 18],\n",
      "         [ 3, 18],\n",
      "         [ 1, 17],\n",
      "         [ 9, 16],\n",
      "         [ 5, 13],\n",
      "         [ 9,  4],\n",
      "         [ 3, 14],\n",
      "         [10, 17],\n",
      "         [ 2, 10],\n",
      "         [ 1,  3],\n",
      "         [ 3,  4],\n",
      "         [ 3, 11],\n",
      "         [ 5,  3],\n",
      "         [ 8, 18],\n",
      "         [ 1,  4],\n",
      "         [ 0,  7],\n",
      "         [ 3,  8],\n",
      "         [ 4, 13],\n",
      "         [ 2, 13],\n",
      "         [10, 11],\n",
      "         [ 7,  3],\n",
      "         [ 1,  7],\n",
      "         [ 5,  6],\n",
      "         [ 4, 15],\n",
      "         [ 7, 14],\n",
      "         [ 0,  8],\n",
      "         [ 0,  3],\n",
      "         [ 8,  3],\n",
      "         [10,  8],\n",
      "         [ 9,  1],\n",
      "         [ 4,  6],\n",
      "         [10,  6],\n",
      "         [ 7,  1],\n",
      "         [ 6, 17],\n",
      "         [ 8,  5],\n",
      "         [ 9,  6],\n",
      "         [ 6,  9],\n",
      "         [10, 16],\n",
      "         [ 1,  8],\n",
      "         [ 8, 12],\n",
      "         [ 3,  2],\n",
      "         [ 5, 15],\n",
      "         [ 4,  7],\n",
      "         [ 7, 12],\n",
      "         [ 6,  1],\n",
      "         [10, 10],\n",
      "         [10, 18],\n",
      "         [ 2,  5],\n",
      "         [ 9, 15],\n",
      "         [ 2,  4],\n",
      "         [ 6, 16],\n",
      "         [ 8,  7],\n",
      "         [ 5,  4],\n",
      "         [10,  0],\n",
      "         [ 7,  2],\n",
      "         [ 9,  2],\n",
      "         [ 7,  7],\n",
      "         [ 4,  8],\n",
      "         [ 8,  0],\n",
      "         [ 6,  0],\n",
      "         [ 3, 17],\n",
      "         [ 8,  8],\n",
      "         [ 7, 15],\n",
      "         [ 8, 16],\n",
      "         [ 5, 14],\n",
      "         [ 7, 11],\n",
      "         [ 6,  6],\n",
      "         [ 7,  6],\n",
      "         [ 6,  5],\n",
      "         [ 4,  4],\n",
      "         [ 9, 12],\n",
      "         [ 2,  6],\n",
      "         [ 5,  9],\n",
      "         [ 3,  6],\n",
      "         [ 4,  3],\n",
      "         [ 2,  8],\n",
      "         [ 0,  2],\n",
      "         [ 2, 17],\n",
      "         [ 9, 10],\n",
      "         [ 1,  1],\n",
      "         [ 1, 11],\n",
      "         [ 9,  3],\n",
      "         [ 9, 17],\n",
      "         [10,  7],\n",
      "         [10,  3],\n",
      "         [ 2,  7],\n",
      "         [ 7, 17],\n",
      "         [ 8,  4],\n",
      "         [ 2,  2],\n",
      "         [ 1,  9],\n",
      "         [ 1,  0],\n",
      "         [ 5,  2],\n",
      "         [10, 14],\n",
      "         [ 3,  3],\n",
      "         [ 7,  0],\n",
      "         [ 5, 18],\n",
      "         [ 5,  1],\n",
      "         [ 0,  9],\n",
      "         [ 2,  3],\n",
      "         [10, 12],\n",
      "         [ 0, 18],\n",
      "         [ 0, 14],\n",
      "         [10,  9],\n",
      "         [ 6,  8],\n",
      "         [10,  2],\n",
      "         [ 1, 12],\n",
      "         [ 4, 16],\n",
      "         [ 5,  0],\n",
      "         [ 2,  1],\n",
      "         [ 2, 14],\n",
      "         [ 3,  9],\n",
      "         [ 1, 15],\n",
      "         [ 2, 15],\n",
      "         [ 4, 11],\n",
      "         [ 7,  8],\n",
      "         [ 9, 18],\n",
      "         [ 5, 17],\n",
      "         [ 7, 18],\n",
      "         [ 6, 10],\n",
      "         [ 6,  7],\n",
      "         [ 0,  6],\n",
      "         [ 3, 12],\n",
      "         [ 1, 18],\n",
      "         [10,  4],\n",
      "         [ 1, 16],\n",
      "         [ 8,  9],\n",
      "         [ 7,  5],\n",
      "         [ 8,  6],\n",
      "         [ 0, 15],\n",
      "         [ 6, 12],\n",
      "         [ 3, 16],\n",
      "         [ 6,  3],\n",
      "         [10, 15],\n",
      "         [ 0, 16],\n",
      "         [ 9,  9],\n",
      "         [ 0, 13],\n",
      "         [ 4,  5],\n",
      "         [ 0,  4],\n",
      "         [ 8,  1],\n",
      "         [ 4,  1],\n",
      "         [ 4, 17],\n",
      "         [ 1,  5],\n",
      "         [ 5,  8],\n",
      "         [ 4,  9],\n",
      "         [ 9,  7],\n",
      "         [ 8, 10],\n",
      "         [ 6, 18],\n",
      "         [ 3, 13],\n",
      "         [ 7,  9],\n",
      "         [ 1, 14],\n",
      "         [ 2, 16],\n",
      "         [ 3,  1],\n",
      "         [ 9, 11],\n",
      "         [ 6,  2],\n",
      "         [ 3,  5],\n",
      "         [ 8, 11],\n",
      "         [ 8, 14],\n",
      "         [ 2, 11],\n",
      "         [ 6, 11],\n",
      "         [ 0, 12],\n",
      "         [ 9, 14],\n",
      "         [ 8, 13],\n",
      "         [ 9,  8],\n",
      "         [ 7, 16],\n",
      "         [ 7,  4],\n",
      "         [10,  1],\n",
      "         [ 2,  0],\n",
      "         [ 5, 12],\n",
      "         [ 6, 15],\n",
      "         [ 8,  2],\n",
      "         [ 9,  5],\n",
      "         [ 4,  2],\n",
      "         [ 3, 10],\n",
      "         [ 0,  1],\n",
      "         [ 3,  7],\n",
      "         [ 1,  2],\n",
      "         [ 6, 14],\n",
      "         [ 5,  5],\n",
      "         [ 0, 17],\n",
      "         [ 0,  0],\n",
      "         [ 4, 18],\n",
      "         [ 9,  0],\n",
      "         [ 1, 10],\n",
      "         [ 7, 10],\n",
      "         [ 5, 16],\n",
      "         [10,  5]]]), (11, 19)), 'cls_output': tensor([[0.4049]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junsheng/.conda/envs/pytorch_junsheng_39/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples=[\n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\", #0\n",
    "            \n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-07-18-04-22-30-preset-18.jpeg\", # 3\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "n = 1\n",
    "sensor = torch.rand(config.senser_input_num)\n",
    "# sensor = torch.ones(config.senser_input_num)\n",
    "print(sensor)\n",
    "sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "out = infer(examples[0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40492728\n"
     ]
    }
   ],
   "source": [
    "print(out[0].cpu().numpy()[0][0])\n",
    "#0.00031266143"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch_junsheng_39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29fd19f11c6b89e267402bb3227bc1208f7e2c9719aa03eba13baf7684fe5867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
