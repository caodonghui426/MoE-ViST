{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vilt.modules import heads, objectives\n",
    "import vilt.modules.vision_transformer as vit\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class config:\n",
    "    debug = False\n",
    "    exp_name = \"vilt\"\n",
    "    seed = 101\n",
    "    batch_size = 4096  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 4\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_fold = 5\n",
    "\n",
    "    model_name = \"sensorViLOnlyTransformerSS\" #仅图片\n",
    "    # model_name = \"sensorOnlyViLTransformerSS\"  #仅vilt传感器\n",
    "    # model_name = \"sensorViLTransformerSS\"  #vilt图像+传感器\n",
    "    # wandb \n",
    "    # wandb_name = \"vilt|大豆|290图像加传感器\"\n",
    "    # wandb_name = \"vilt|大豆|290仅传感器\"\n",
    "    wandb_name = \"vilt|大豆|290仅图片\"\n",
    "    \n",
    "\n",
    "    # Image setting\n",
    "    train_transform_keys = [\"pixelbert\"]\n",
    "    val_transform_keys = [\"pixelbert\"]\n",
    "    img_size = 384\n",
    "    max_image_len = -1\n",
    "    patch_size = 32\n",
    "    draw_false_image = 1\n",
    "    image_only = False\n",
    "\n",
    "    # Sensor\n",
    "    # senser_input_num = 11 # 翔冠的传感器参数\n",
    "    senser_input_num = 19 # 天航的传感器参数\n",
    "    \n",
    "    # Text Setting\n",
    "    vqav2_label_size = 3129\n",
    "    max_text_len = 40\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    vocab_size = 30522 # vocabulary词汇数量\n",
    "    whole_word_masking = False\n",
    "    mlm_prob = 0.15\n",
    "    draw_false_text = 0\n",
    "\n",
    "    # Transformer Setting\n",
    "    vit = \"vit_base_patch32_384\"\n",
    "    hidden_size = 768  # 嵌入向量大小\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    mlp_ratio = 4\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim_type = \"adamw\"\n",
    "    learning_rate = 1e-3 #0.0015#2e-3 #\n",
    "    weight_decay = 1e-4 # 0.01 ->1e-4\n",
    "    decay_power = 1\n",
    "    max_epoch = 50\n",
    "    max_steps = 25000\n",
    "    warmup_steps = 2500\n",
    "    end_lr = 0\n",
    "    lr_mult = 1  # multiply lr for downstream heads\n",
    "    # T_max = 8000/train_batch_size*max_epoch \n",
    "    T_max = 2126/train_batch_size*max_epoch \n",
    "\n",
    "    # Downstream Setting\n",
    "    get_recall_metric = False\n",
    "\n",
    "\n",
    "    # below params varies with the environment\n",
    "    data_root = \"\"\n",
    "    log_dir = \"result\"\n",
    "    per_gpu_batchsize = 0  # you should define this manually with per_gpu_batch_size=#\n",
    "    num_gpus = 1\n",
    "    num_nodes = 1\n",
    "    load_path = \"weights/vilt_200k_mlm_itm.ckpt\"\n",
    "    # load_path = \"save_model_dict.pt\"\n",
    "    num_workers = 1\n",
    "    precision = 16\n",
    "\n",
    "# config = vars(config)\n",
    "# config = dict(config)\n",
    "config\n",
    "\n",
    "if config.debug:\n",
    "    config.max_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)  # 为CPU设置随机种子\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)  # 为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有GPU设置随机种子\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "setup_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"] = 'dryrun' # 离线模式\n",
    "try:\n",
    "    # wandb.log(key=\"*******\") # if debug\n",
    "    wandb.login() # storage in ~/.netrc file\n",
    "    anonymous = None\n",
    "except:\n",
    "    anonymous = \"must\"\n",
    "    print('\\nGet your W&B access token from here: https://wandb.ai/authorize\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2658, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pic_key</th>\n",
       "      <th>date_hour</th>\n",
       "      <th>date</th>\n",
       "      <th>co2</th>\n",
       "      <th>stemp</th>\n",
       "      <th>stemp2</th>\n",
       "      <th>stemp3</th>\n",
       "      <th>stemp4</th>\n",
       "      <th>stemp5</th>\n",
       "      <th>...</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>press</th>\n",
       "      <th>solar</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind_d</th>\n",
       "      <th>wind_sp</th>\n",
       "      <th>LAI</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>/794/1655497027_1655496664_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022/6/18</td>\n",
       "      <td>419.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.4</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>991.1</td>\n",
       "      <td>2.52</td>\n",
       "      <td>17.26</td>\n",
       "      <td>274.3</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.3175</td>\n",
       "      <td>/home/junsheng/data/tianhang_soybean/165549702...</td>\n",
       "      <td>1.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>/794/1655497027_1655496664_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022/6/18</td>\n",
       "      <td>419.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.4</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>991.2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>17.18</td>\n",
       "      <td>268.7</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.3175</td>\n",
       "      <td>/home/junsheng/data/tianhang_soybean/165549702...</td>\n",
       "      <td>1.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>/794/1655497027_1655496664_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022/6/18</td>\n",
       "      <td>418.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.1</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.4</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>991.1</td>\n",
       "      <td>2.52</td>\n",
       "      <td>17.26</td>\n",
       "      <td>274.3</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.3175</td>\n",
       "      <td>/home/junsheng/data/tianhang_soybean/165549702...</td>\n",
       "      <td>1.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>/794/1655497027_1655496664_4.jpg</td>\n",
       "      <td>2022-06-18 04</td>\n",
       "      <td>2022/6/18</td>\n",
       "      <td>418.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.1</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.4</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>991.2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>17.18</td>\n",
       "      <td>268.7</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.3175</td>\n",
       "      <td>/home/junsheng/data/tianhang_soybean/165549702...</td>\n",
       "      <td>1.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>/794/1655504185_1655503864_4.jpg</td>\n",
       "      <td>2022-06-18 06</td>\n",
       "      <td>2022/6/18</td>\n",
       "      <td>419.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.7</td>\n",
       "      <td>18.3</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>991.9</td>\n",
       "      <td>8.84</td>\n",
       "      <td>17.75</td>\n",
       "      <td>248.6</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.3175</td>\n",
       "      <td>/home/junsheng/data/tianhang_soybean/165550418...</td>\n",
       "      <td>1.3175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                           pic_key      date_hour       date    co2  \\\n",
       "0     32  /794/1655497027_1655496664_4.jpg  2022-06-18 04  2022/6/18  419.0   \n",
       "1     33  /794/1655497027_1655496664_4.jpg  2022-06-18 04  2022/6/18  419.0   \n",
       "2     34  /794/1655497027_1655496664_4.jpg  2022-06-18 04  2022/6/18  418.0   \n",
       "3     35  /794/1655497027_1655496664_4.jpg  2022-06-18 04  2022/6/18  418.0   \n",
       "4     36  /794/1655504185_1655503864_4.jpg  2022-06-18 06  2022/6/18  419.0   \n",
       "\n",
       "   stemp  stemp2  stemp3  stemp4  stemp5  ...  pm10  pm25  press  solar  \\\n",
       "0   19.2    19.3    19.1    18.8    18.4  ...   6.0   6.0  991.1   2.52   \n",
       "1   19.2    19.3    19.1    18.8    18.4  ...   7.0   7.0  991.2   5.93   \n",
       "2   19.1    19.2    19.1    18.8    18.4  ...   6.0   6.0  991.1   2.52   \n",
       "3   19.1    19.2    19.1    18.8    18.4  ...   7.0   7.0  991.2   5.93   \n",
       "4   18.8    19.0    18.9    18.7    18.3  ...   5.0   5.0  991.9   8.84   \n",
       "\n",
       "    temp wind_d wind_sp     LAI  \\\n",
       "0  17.26  274.3    3.75  1.3175   \n",
       "1  17.18  268.7    2.67  1.3175   \n",
       "2  17.26  274.3    3.75  1.3175   \n",
       "3  17.18  268.7    2.67  1.3175   \n",
       "4  17.75  248.6    2.07  1.3175   \n",
       "\n",
       "                                          image_path   label  \n",
       "0  /home/junsheng/data/tianhang_soybean/165549702...  1.3175  \n",
       "1  /home/junsheng/data/tianhang_soybean/165549702...  1.3175  \n",
       "2  /home/junsheng/data/tianhang_soybean/165549702...  1.3175  \n",
       "3  /home/junsheng/data/tianhang_soybean/165549702...  1.3175  \n",
       "4  /home/junsheng/data/tianhang_soybean/165550418...  1.3175  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tianhang = pd.read_csv(\"/home/junsheng/ViLT/data/290-tianhang-soybean.csv\")\n",
    "df_tianhang['image_path'] = df_tianhang['pic_key'].map(lambda x:os.path.join('/home/junsheng/data/tianhang_soybean',x.split('/')[-1]))\n",
    "df_tianhang['label'] = df_tianhang['LAI']\n",
    "df_tianhang = df_tianhang.dropna()\n",
    "df_tianhang = df_tianhang.reset_index()\n",
    "print(df_tianhang.shape)\n",
    "df_tianhang.to_csv(\"test.csv\",index=False)\n",
    "df_tianhang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648\n",
      "811\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "# 检查图片下载的全不全\n",
    "pic = df_tianhang.image_path.map(lambda x:x.split('/')[-1]).unique()\n",
    "print(len(pic))\n",
    "file_ls = os.listdir(\"/home/junsheng/data/tianhang_soybean\")\n",
    "print(len(file_ls))\n",
    "ret = list(set(pic) ^ set(file_ls))\n",
    "print(len(ret)) #差集\n",
    "# assert len(pic)==len(file_ls),\"请检查下载的图片，缺了{}个\".format(len(pic)-len(file_ls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化非object列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'pic_key',\n",
       " 'date_hour',\n",
       " 'date',\n",
       " 'co2',\n",
       " 'stemp',\n",
       " 'stemp2',\n",
       " 'stemp3',\n",
       " 'stemp4',\n",
       " 'stemp5',\n",
       " 'shumi',\n",
       " 'shumi2',\n",
       " 'shumi3',\n",
       " 'shumi4',\n",
       " 'shumi5',\n",
       " 'ts',\n",
       " 'insert_time',\n",
       " 'humi',\n",
       " 'pm10',\n",
       " 'pm25',\n",
       " 'press',\n",
       " 'solar',\n",
       " 'temp',\n",
       " 'wind_d',\n",
       " 'wind_sp',\n",
       " 'LAI',\n",
       " 'image_path',\n",
       " 'label']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_tianhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'co2', 'stemp', 'stemp2', 'stemp3', 'stemp4', 'stemp5', 'shumi', 'shumi2', 'shumi3', 'shumi4', 'shumi5', 'humi', 'pm10', 'pm25', 'press', 'solar', 'temp', 'wind_d', 'wind_sp', 'LAI', 'label']\n",
      "{'index': (32, 3161), 'co2': (341.0, 751.0), 'stemp': (14.0, 29.0), 'stemp2': (14.8, 27.5), 'stemp3': (15.5, 25.7), 'stemp4': (15.6, 24.6), 'stemp5': (16.0, 24.3), 'shumi': (44.6, 75.7), 'shumi2': (36.5, 71.3), 'shumi3': (38.9, 71.7), 'shumi4': (43.6, 75.0), 'shumi5': (61.6, 80.0), 'humi': (31.0, 100.0), 'pm10': (0.0, 1333.0), 'pm25': (0.0, 1333.0), 'press': (981.1, 1014.8), 'solar': (0.0, 200.0), 'temp': (7.25, 32.0), 'wind_d': (0.0, 359.8), 'wind_sp': (0.0, 10.27), 'LAI': (1.3175, 2.23), 'label': (1.3175, 2.23)}\n"
     ]
    }
   ],
   "source": [
    "number_title = []\n",
    "recorder = {}\n",
    "for title in df_tianhang:\n",
    "    # print(df_xiangguan[title].head())\n",
    "    if title == 'raw_label':\n",
    "        continue\n",
    "    if df_tianhang[title].dtype != \"object\":\n",
    "        \n",
    "        number_title.append(title)\n",
    "        x_min = df_tianhang[title].min()\n",
    "        x_max = df_tianhang[title].max()\n",
    "        # print(x_min,x_max)\n",
    "        recorder[title] = (x_min,x_max)\n",
    "        df_tianhang[title] = df_tianhang[title].map(lambda x:(x-x_min)/(x_max - x_min))\n",
    "print(number_title)\n",
    "print(recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tianhang['stemp4'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 19\n"
     ]
    }
   ],
   "source": [
    "# xiangguan_sensor = ['temperature', 'humidity', 'illuminance', 'soil_temperature', 'soil_humidity', 'pressure', 'wind_speed', 'photosynthetic', 'sun_exposure_time', 'COz', 'soil_ph']\n",
    "tianhang_sensor = ['co2', 'stemp', 'stemp2', 'stemp3', 'stemp4', 'stemp5', 'shumi', 'shumi2', 'shumi3', 'shumi4', 'shumi5', 'humi', 'pm10', 'pm25', 'press', 'solar', 'temp', 'wind_d', 'wind_sp']\n",
    "# tianhang_sensor = ['co2', 'stemp', 'stemp2', 'stemp3', 'stemp5', 'shumi', 'shumi2', 'shumi3', 'shumi5', 'humi', 'pm10', 'pm25', 'press', 'solar', 'temp', 'wind_d', 'wind_sp']\n",
    "\n",
    "df_tianhang['sensor'] = df_tianhang[tianhang_sensor].values.tolist()\n",
    "print(\"input dim:\",len(tianhang_sensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2658, 29)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_tianhang\n",
    "if config.debug:\n",
    "    df = df[:100]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tianhang.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0.0    532\n",
       "1.0    532\n",
       "2.0    532\n",
       "3.0    531\n",
       "4.0    531\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=config.seed)  \n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df,df.date)):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "df.groupby(['fold'])['label'].count()# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv(\"test_fold.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransforms = transforms.Compose([\n",
    "    transforms.Resize((config.img_size,config.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.7136, 0.7118, 0.6788],\n",
    "    std=[0.3338, 0.3453, 0.3020],\n",
    "    \n",
    ")\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img =  Image.open(path).convert('RGB')\n",
    "    img = myTransforms(img)\n",
    "    return img\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df         = df\n",
    "        self.label      = label\n",
    "        self.sensors = df['sensor'].tolist()\n",
    "        self.img_paths  = df['image_path'].tolist()   \n",
    "        if self.label:\n",
    "            self.labels = df['label'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path  = self.img_paths[index]\n",
    "        img = load_img(img_path)\n",
    "        sensor = self.sensors[index]\n",
    "        sensor = torch.tensor(sensor).unsqueeze(0) #[1,n]\n",
    "        if self.label:\n",
    "            label = self.labels[index]\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
    "        else:\n",
    "            return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataloader(fold:int):\n",
    "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "\n",
    "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    print(\"train_df.shape:\",train_df.shape)\n",
    "    print(\"valid_df.shape:\",valid_df.shape)\n",
    "\n",
    "    train_data  = BuildDataset(df=train_df,label=True)\n",
    "    valid_data = BuildDataset(df=valid_df,label=True)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=config.train_batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=config.valid_batch_size,shuffle=False)\n",
    "    # test_loader = DataLoader(test_data, batch_size=config.test_batch_size,shuffle=False)\n",
    "    return train_loader,valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (2126, 30)\n",
      "valid_df.shape: (532, 30)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = BuildDataset(df=df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.train_batch_size,shuffle=True)\n",
    "# valid_loader = DataLoader(train_dataset, batch_size=config.valid_batch_size,shuffle=True)\n",
    "train_loader,valid_loader = fetch_dataloader(fold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126287/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 384, 384])\n",
      "torch.Size([32, 1, 19])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "img,sensor,label = next(iter(train_loader))\n",
    "print(img.shape)\n",
    "print(sensor.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorViLOnlyTransformerSS-仅vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sensorViLOnlyTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self, sensor_class_n, output_class_n):\n",
    "        super().__init__()\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "            pretrained=True, config=vars(config)\n",
    "        )\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, output_class_n)\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device)\n",
    "\n",
    "            (\n",
    "                image_embeds,  # torch.Size([1, 217, 768])\n",
    "                image_masks,  # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "            torch.full_like(image_masks, image_token_type_idx)\n",
    "        )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size, 1).to(config.device)  # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = image_embeds\n",
    "        co_masks = image_masks\n",
    "\n",
    "        x = co_embeds.to(config.device)  # torch.Size([1, 145, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)  # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x)  # torch.Size([1, 240, 768])\n",
    "        image_feats = x\n",
    "        cls_feats = self.pooler(x)  # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "\n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "\n",
    "        ret = {\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats,  # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "\n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\": cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "\n",
    "        ret.update(self.infer(batch))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorViLTransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        if image_embeds is None and image_masks is None:\n",
    "            img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "            (\n",
    "                image_embeds, # torch.Size([1, 217, 768])\n",
    "                image_masks, # torch.Size([1, 217])\n",
    "                patch_index,\n",
    "                image_labels,\n",
    "            ) = self.transformer.visual_embed(\n",
    "                img,\n",
    "                max_image_len=config.max_image_len,\n",
    "                mask_it=mask_image,\n",
    "            )\n",
    "        else:\n",
    "            patch_index, image_labels = (\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_labels\": image_labels,\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "            \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorOnlyViLTransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorOnlyViLTransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        # mask_image=False,\n",
    "        # image_token_type_idx=1,\n",
    "        # image_embeds=None,\n",
    "        # image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        \n",
    "\n",
    "        # if image_embeds is None and image_masks is None:\n",
    "        #     img = batch[\"image\"].to(config.device)\n",
    "       \n",
    "        #     (\n",
    "        #         image_embeds, # torch.Size([1, 217, 768])\n",
    "        #         image_masks, # torch.Size([1, 217])\n",
    "        #         patch_index,\n",
    "        #         image_labels,\n",
    "        #     ) = self.transformer.visual_embed(\n",
    "        #         img,\n",
    "        #         max_image_len=config.max_image_len,\n",
    "        #         mask_it=mask_image,\n",
    "        #     )\n",
    "        # else:\n",
    "        #     patch_index, image_labels = (\n",
    "        #         None,\n",
    "        #         None,\n",
    "        #     )\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        # image_embeds = image_embeds + self.token_type_embeddings(\n",
    "        #         torch.full_like(image_masks, image_token_type_idx)\n",
    "        #     )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        # batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(sensor_embeds.shape[1],1).to(config.device) # 序列数量\n",
    "        # image_masks = image_masks.to(config.device)\n",
    "        # co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        # co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "        co_embeds = sensor_embeds\n",
    "        co_masks = sensor_masks\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 1, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        # sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "        #     x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "        #     x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        # )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "        #    \"sensor_feats\":sensor_feats,\n",
    "            # \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            # \"image_labels\": image_labels,\n",
    "            # \"image_masks\": image_masks,\n",
    "           \n",
    "            # \"patch_index\": patch_index,\n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorResnet50TransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorResnet50TransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "        # resnet model\n",
    "        resnet_model = pretrainedmodels.__dict__[\"resnet50\"](\n",
    "    num_classes=1000, pretrained='imagenet')\n",
    "        features = list([resnet_model.conv1, resnet_model.bn1, resnet_model.relu, resnet_model.maxpool, resnet_model.layer1, resnet_model.layer2, resnet_model.layer3,resnet_model.layer4])\n",
    "        conv = nn.Conv2d(2048, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        bn = nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "        self.resnet_features = nn.Sequential(*features,conv,bn,relu)\n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        img = batch[\"image\"].to(config.device)\n",
    "        image_embeds = self.resnet_features(img) \n",
    "        image_embeds = image_embeds.flatten(2).transpose(1, 2)\n",
    "        image_masks = torch.ones(image_embeds.shape[0],image_embeds.shape[1],dtype=torch.int64).to(config.device)\n",
    "\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensorResnet101TransformerSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sensorResnet101TransformerSS(nn.Module):\n",
    "\n",
    "    def __init__(self,sensor_class_n,output_class_n):\n",
    "        super().__init__()\n",
    "        self.sensor_linear = nn.Linear(sensor_class_n,config.hidden_size) \n",
    "        # resnet model\n",
    "        resnet_model = pretrainedmodels.__dict__[\"resnet101\"](\n",
    "    num_classes=1000, pretrained='imagenet')\n",
    "        features = list([resnet_model.conv1, resnet_model.bn1, resnet_model.relu, resnet_model.maxpool, resnet_model.layer1, resnet_model.layer2, resnet_model.layer3,resnet_model.layer4])\n",
    "        conv = nn.Conv2d(2048, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        bn = nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "        self.resnet_features = nn.Sequential(*features,conv,bn,relu)\n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(2, config.hidden_size)\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.transformer = getattr(vit, config.vit)(\n",
    "                pretrained=True, config=vars(config)\n",
    "            )\n",
    "       \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.pooler = heads.Pooler(config.hidden_size)\n",
    "\n",
    "        # self.pooler.apply(objectives.init_weights)\n",
    "        self.classifier = nn.Linear(config.hidden_size,output_class_n)\n",
    "\n",
    "        hs = config.hidden_size\n",
    "\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_image=False,\n",
    "        image_token_type_idx=1,\n",
    "        image_embeds=None,\n",
    "        image_masks=None,\n",
    "    ):\n",
    "        sensor = batch['sensor'].to(config.device)\n",
    "        sensor_embeds = self.sensor_linear(sensor) # input[1,1,12]  output[1,1,768]\n",
    "        img = batch[\"image\"].to(config.device)\n",
    "        image_embeds = self.resnet_features(img) \n",
    "        image_embeds = image_embeds.flatten(2).transpose(1, 2)\n",
    "        image_masks = torch.ones(image_embeds.shape[0],image_embeds.shape[1],dtype=torch.int64).to(config.device)\n",
    "\n",
    "        # 用embedding对数据输入预处理，降低维度\n",
    "        image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx)\n",
    "            )\n",
    "        # sensor_masks = batch['sensor_masks'] # 序列数量\n",
    "        batch_size = img.shape[0]\n",
    "        sensor_masks = torch.ones(batch_size,1).to(config.device) # 序列数量\n",
    "        image_masks = image_masks.to(config.device)\n",
    "        co_embeds = torch.cat([sensor_embeds, image_embeds], dim=1) # torch.Size([1, 240, 768]) ->240=217+23\n",
    "        co_masks = torch.cat([sensor_masks, image_masks], dim=1) # torch.Size([1, 240])\n",
    "\n",
    "        x = co_embeds.to(config.device) # torch.Size([1, 211, 768])\n",
    "\n",
    "        for i, blk in enumerate(self.transformer.blocks): \n",
    "            blk = blk.to(config.device)\n",
    "            x, _attn = blk(x, mask=co_masks) # co_masks = torch.Size([1, 211])\n",
    "\n",
    "        x = self.transformer.norm(x) # torch.Size([1, 240, 768])\n",
    "        sensor_feats, image_feats = ( # torch.Size([1, 23, 768]),torch.Size([1, 217, 768])\n",
    "            x[:, : sensor_embeds.shape[1]], # 后面字数输出23维\n",
    "            x[:, sensor_embeds.shape[1] :], # 前面图片输出217维\n",
    "        )\n",
    "        cls_feats = self.pooler(x) # torch.Size([1, 768])\n",
    "        # cls_feats = self.dense(x)\n",
    "        # cls_feats = self.activation(cls_feats)\n",
    "        cls_output = self.classifier(cls_feats)\n",
    "        # m = nn.Softmax(dim=1)\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        cls_output = m(cls_output)\n",
    "        \n",
    "        ret = {\n",
    "           \"sensor_feats\":sensor_feats,\n",
    "            \"image_feats\": image_feats,\n",
    "            \"cls_feats\": cls_feats, # class features\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"image_masks\": image_masks,\n",
    "           \n",
    "\n",
    "            \"cls_output\":cls_output,\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "        \n",
    "        ret.update(self.infer(batch))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pretrained weights exist or were found for this model. Using random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 sensorViLTransformerSS(\n",
      "  (sensor_linear): Linear(in_features=19, out_features=768, bias=True)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n",
      "1 Linear(in_features=19, out_features=768, bias=True)\n",
      "2 Embedding(2, 768)\n",
      "3 VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "4 PatchEmbed(\n",
      "  (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      ")\n",
      "5 Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "6 Dropout(p=0.1, inplace=False)\n",
      "7 ModuleList(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate=none)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "8 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "9 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "10 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "11 Linear(in_features=768, out_features=2304, bias=True)\n",
      "12 Dropout(p=0.0, inplace=False)\n",
      "13 Linear(in_features=768, out_features=768, bias=True)\n",
      "14 Dropout(p=0.1, inplace=False)\n",
      "15 Identity()\n",
      "16 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "17 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "18 Linear(in_features=768, out_features=3072, bias=True)\n",
      "19 GELU(approximate=none)\n",
      "20 Linear(in_features=3072, out_features=768, bias=True)\n",
      "21 Dropout(p=0.1, inplace=False)\n",
      "22 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "23 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "24 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "25 Linear(in_features=768, out_features=2304, bias=True)\n",
      "26 Dropout(p=0.0, inplace=False)\n",
      "27 Linear(in_features=768, out_features=768, bias=True)\n",
      "28 Dropout(p=0.1, inplace=False)\n",
      "29 Identity()\n",
      "30 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "31 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "32 Linear(in_features=768, out_features=3072, bias=True)\n",
      "33 GELU(approximate=none)\n",
      "34 Linear(in_features=3072, out_features=768, bias=True)\n",
      "35 Dropout(p=0.1, inplace=False)\n",
      "36 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "37 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "38 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "39 Linear(in_features=768, out_features=2304, bias=True)\n",
      "40 Dropout(p=0.0, inplace=False)\n",
      "41 Linear(in_features=768, out_features=768, bias=True)\n",
      "42 Dropout(p=0.1, inplace=False)\n",
      "43 Identity()\n",
      "44 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "45 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "46 Linear(in_features=768, out_features=3072, bias=True)\n",
      "47 GELU(approximate=none)\n",
      "48 Linear(in_features=3072, out_features=768, bias=True)\n",
      "49 Dropout(p=0.1, inplace=False)\n",
      "50 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "51 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "52 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "53 Linear(in_features=768, out_features=2304, bias=True)\n",
      "54 Dropout(p=0.0, inplace=False)\n",
      "55 Linear(in_features=768, out_features=768, bias=True)\n",
      "56 Dropout(p=0.1, inplace=False)\n",
      "57 Identity()\n",
      "58 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "59 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "60 Linear(in_features=768, out_features=3072, bias=True)\n",
      "61 GELU(approximate=none)\n",
      "62 Linear(in_features=3072, out_features=768, bias=True)\n",
      "63 Dropout(p=0.1, inplace=False)\n",
      "64 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "65 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "66 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "67 Linear(in_features=768, out_features=2304, bias=True)\n",
      "68 Dropout(p=0.0, inplace=False)\n",
      "69 Linear(in_features=768, out_features=768, bias=True)\n",
      "70 Dropout(p=0.1, inplace=False)\n",
      "71 Identity()\n",
      "72 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "73 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "74 Linear(in_features=768, out_features=3072, bias=True)\n",
      "75 GELU(approximate=none)\n",
      "76 Linear(in_features=3072, out_features=768, bias=True)\n",
      "77 Dropout(p=0.1, inplace=False)\n",
      "78 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "79 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "80 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "81 Linear(in_features=768, out_features=2304, bias=True)\n",
      "82 Dropout(p=0.0, inplace=False)\n",
      "83 Linear(in_features=768, out_features=768, bias=True)\n",
      "84 Dropout(p=0.1, inplace=False)\n",
      "85 Identity()\n",
      "86 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "87 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "88 Linear(in_features=768, out_features=3072, bias=True)\n",
      "89 GELU(approximate=none)\n",
      "90 Linear(in_features=3072, out_features=768, bias=True)\n",
      "91 Dropout(p=0.1, inplace=False)\n",
      "92 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "93 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "94 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "95 Linear(in_features=768, out_features=2304, bias=True)\n",
      "96 Dropout(p=0.0, inplace=False)\n",
      "97 Linear(in_features=768, out_features=768, bias=True)\n",
      "98 Dropout(p=0.1, inplace=False)\n",
      "99 Identity()\n",
      "100 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "101 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "102 Linear(in_features=768, out_features=3072, bias=True)\n",
      "103 GELU(approximate=none)\n",
      "104 Linear(in_features=3072, out_features=768, bias=True)\n",
      "105 Dropout(p=0.1, inplace=False)\n",
      "106 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "107 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "108 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "109 Linear(in_features=768, out_features=2304, bias=True)\n",
      "110 Dropout(p=0.0, inplace=False)\n",
      "111 Linear(in_features=768, out_features=768, bias=True)\n",
      "112 Dropout(p=0.1, inplace=False)\n",
      "113 Identity()\n",
      "114 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "115 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "116 Linear(in_features=768, out_features=3072, bias=True)\n",
      "117 GELU(approximate=none)\n",
      "118 Linear(in_features=3072, out_features=768, bias=True)\n",
      "119 Dropout(p=0.1, inplace=False)\n",
      "120 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "121 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "122 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "123 Linear(in_features=768, out_features=2304, bias=True)\n",
      "124 Dropout(p=0.0, inplace=False)\n",
      "125 Linear(in_features=768, out_features=768, bias=True)\n",
      "126 Dropout(p=0.1, inplace=False)\n",
      "127 Identity()\n",
      "128 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "129 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "130 Linear(in_features=768, out_features=3072, bias=True)\n",
      "131 GELU(approximate=none)\n",
      "132 Linear(in_features=3072, out_features=768, bias=True)\n",
      "133 Dropout(p=0.1, inplace=False)\n",
      "134 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "135 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "136 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "137 Linear(in_features=768, out_features=2304, bias=True)\n",
      "138 Dropout(p=0.0, inplace=False)\n",
      "139 Linear(in_features=768, out_features=768, bias=True)\n",
      "140 Dropout(p=0.1, inplace=False)\n",
      "141 Identity()\n",
      "142 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "143 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "144 Linear(in_features=768, out_features=3072, bias=True)\n",
      "145 GELU(approximate=none)\n",
      "146 Linear(in_features=3072, out_features=768, bias=True)\n",
      "147 Dropout(p=0.1, inplace=False)\n",
      "148 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "149 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "150 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "151 Linear(in_features=768, out_features=2304, bias=True)\n",
      "152 Dropout(p=0.0, inplace=False)\n",
      "153 Linear(in_features=768, out_features=768, bias=True)\n",
      "154 Dropout(p=0.1, inplace=False)\n",
      "155 Identity()\n",
      "156 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "157 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "158 Linear(in_features=768, out_features=3072, bias=True)\n",
      "159 GELU(approximate=none)\n",
      "160 Linear(in_features=3072, out_features=768, bias=True)\n",
      "161 Dropout(p=0.1, inplace=False)\n",
      "162 Block(\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (act): GELU(approximate=none)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "163 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "164 Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "165 Linear(in_features=768, out_features=2304, bias=True)\n",
      "166 Dropout(p=0.0, inplace=False)\n",
      "167 Linear(in_features=768, out_features=768, bias=True)\n",
      "168 Dropout(p=0.1, inplace=False)\n",
      "169 Identity()\n",
      "170 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "171 Mlp(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (act): GELU(approximate=none)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "172 Linear(in_features=768, out_features=3072, bias=True)\n",
      "173 GELU(approximate=none)\n",
      "174 Linear(in_features=3072, out_features=768, bias=True)\n",
      "175 Dropout(p=0.1, inplace=False)\n",
      "176 LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "177 Linear(in_features=768, out_features=768, bias=True)\n",
      "178 Tanh()\n",
      "179 Pooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "180 Linear(in_features=768, out_features=768, bias=True)\n",
      "181 Tanh()\n",
      "182 Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def build_model(model_name: str,pre_train):\n",
    "    if model_name[:6] == \"resnet50\":\n",
    "        model = pretrainedmodels.__dict__[config.model_name](\n",
    "            num_classes=1000, pretrained='imagenet')\n",
    "        dim_feats = model.last_linear.in_features  # =2048\n",
    "        nb_classes = 1\n",
    "        model.last_linear = nn.Linear(dim_feats, nb_classes)\n",
    "        return model\n",
    "    if model_name == \"se_resnet50\":\n",
    "        model = pretrainedmodels.__dict__[config.model_name](\n",
    "            num_classes=1000, pretrained='imagenet')\n",
    "        model.last_linear = nn.Linear(204800, 1,bias=True)\n",
    "        return model\n",
    "    if model_name == \"efficientnet-b4\": # efficient net\n",
    "        # refer:https://github.com/lukemelas/EfficientNet-PyTorch#example-classification\n",
    "        nb_classes = 1\n",
    "        if pre_train:\n",
    "            model = EfficientNet.from_pretrained(config.model_name)# 'efficientnet-b4'\n",
    "        else:\n",
    "            model = EfficientNet.from_name(config.model_name)# 'efficientnet-b4'\n",
    "        model._fc = nn.Linear(1792, nb_classes)\n",
    "        return model\n",
    "\n",
    "    if model_name == \"sensorOnlyViLTransformerSS\": #仅传感器\n",
    "        model = sensorOnlyViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "    if model_name == \"sensorViLOnlyTransformerSS\": # 仅vit图像\n",
    "        model = sensorViLOnlyTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "        \n",
    "    if model_name == \"sensorResnet50TransformerSS\":\n",
    "        model = sensorResnet50TransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "    if model_name == \"sensorResnet101TransformerSS\":\n",
    "        model = sensorResnet101TransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "\n",
    "    if model_name == \"sensorViLTransformerSS\":\n",
    "        model = sensorViLTransformerSS(sensor_class_n= config.senser_input_num,output_class_n = 1)\n",
    "        return model\n",
    "\n",
    "model = build_model(config.model_name,True)\n",
    "model.to(config.device)\n",
    "print(config.device)\n",
    "for i,m in enumerate(model.modules()):\n",
    "    print(i,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sensor = torch.rand(config.senser_input_num)\n",
    "# # sensor = torch.ones(config.senser_input_num)\n",
    "# print(sensor)\n",
    "# sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "# batch = {}\n",
    "# batch['sensor'] = sensor\n",
    "# batch['image'] = \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\"\n",
    "# model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.mse_loss #均方误差损失函数\n",
    "criterion_mae = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
    "    for step, (img, sensor,label) in pbar:         \n",
    "        # img = img.to(device, dtype=torch.float)\n",
    "        # sensor  = sensor.to(device, dtype=torch.float)\n",
    "        # label  = label.to(device, dtype=torch.float)\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        \n",
    "        #一坨优化\n",
    "        optimizer.zero_grad()#每一次反向传播之前都要归零梯度\n",
    "        loss.backward()      #反向传播\n",
    "        optimizer.step()     #固定写法\n",
    "        scheduler.step()\n",
    "     \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_mem=f'{mem:0.2f} GB')\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    running_loss_mae = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
    "    for step, (img, sensor,label) in pbar:               \n",
    "        \n",
    "        \n",
    "        batch_size = img.size(0)\n",
    "        batch = {\"image\":img,\"sensor\":sensor}\n",
    "\n",
    "        y_pred  = model(batch)\n",
    "        label = label.to(config.device).unsqueeze(1)\n",
    "\n",
    "        loss = criterion(y_pred['cls_output'], label)\n",
    "        loss_mae = criterion_mae(y_pred['cls_output'], label)\n",
    "\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_loss_mae += (loss_mae.item() * batch_size)\n",
    "\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_loss_mae = running_loss_mae / dataset_size\n",
    "        \n",
    "        \n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n",
    "        valid_loss_mae=f'{epoch_loss_mae:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_memory=f'{mem:0.2f} GB')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss,epoch_loss_mae#MSE，MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "     # init wandb\n",
    "    run = wandb.init(project=\"vilt\",\n",
    "                    config={k: v for k, v in dict(vars(config)).items() if '__' not in k},\n",
    "                    # config={k: v for k, v in dict(config).items() if '__' not in k},\n",
    "                    anonymous=anonymous,\n",
    "                    # name=f\"vilt|fold-{config.valid_fold}\",\n",
    "                    name=config.wandb_name,\n",
    "                    # group=config.wandb_group,\n",
    "                    )\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_valid_loss = 9999\n",
    "    history = defaultdict(list)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        val_loss,val_loss_mae = valid_one_epoch(model,valid_loader,device=device,optimizer=optimizer)\n",
    "        history['Train Loss'].append(train_loss)\n",
    "        history['Valid Loss'].append(val_loss)\n",
    "        history['Valid Loss MAE'].append(val_loss_mae)\n",
    "\n",
    "        wandb.log({\"Train Loss\": train_loss,\n",
    "                    \"Valid Loss\": val_loss,\n",
    "                    \"Valid Loss MAE\": val_loss_mae,\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "                })\n",
    "        if best_valid_loss > val_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            # model_file_path = os.path.join(wandb.run.dir,\"epoch-{}-{}.bin\".format(epoch,wandb.run.id))\n",
    "            model_file_path = os.path.join(wandb.run.dir,\"epoch-best.bin\")\n",
    "            run.summary[\"Best Epoch\"] = epoch\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(\"model save to\", model_file_path)\n",
    "               \n",
    "    os.system(\"cp /home/junsheng/ViLT/my_vilt_tianhang_soybean.ipynb {}\".format(wandb.run.dir))\n",
    "    run.finish()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=config.T_max, \n",
    "                                                   eta_min=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: NVIDIA GeForce RTX 3090\n",
      "\n",
      "Epoch 1/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :   0%|          | 0/67 [00:00<?, ?it/s]/tmp/ipykernel_1126287/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n",
      "Train : 100%|██████████| 67/67 [02:06<00:00,  1.89s/it, gpu_mem=5.68 GB, lr=0.00100, train_loss=0.2906]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.45it/s, gpu_memory=1.91 GB, lr=0.00100, valid_loss=0.2937, valid_loss_mae=0.4646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 2/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:06<00:00,  1.89s/it, gpu_mem=5.57 GB, lr=0.00098, train_loss=0.2933]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.32it/s, gpu_memory=1.91 GB, lr=0.00098, valid_loss=0.2937, valid_loss_mae=0.4646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 3/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00096, train_loss=0.2953]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.44it/s, gpu_memory=1.91 GB, lr=0.00096, valid_loss=0.3644, valid_loss_mae=0.5354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00093, train_loss=0.2164]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.32it/s, gpu_memory=1.91 GB, lr=0.00093, valid_loss=0.0840, valid_loss_mae=0.2552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 5/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.91s/it, gpu_mem=5.57 GB, lr=0.00089, train_loss=0.0824]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.33it/s, gpu_memory=1.91 GB, lr=0.00089, valid_loss=0.0821, valid_loss_mae=0.2583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 6/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00085, train_loss=0.0803]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.31it/s, gpu_memory=1.91 GB, lr=0.00085, valid_loss=0.0805, valid_loss_mae=0.2577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 7/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:08<00:00,  1.91s/it, gpu_mem=5.57 GB, lr=0.00080, train_loss=0.0802]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.32it/s, gpu_memory=1.91 GB, lr=0.00080, valid_loss=0.0768, valid_loss_mae=0.2545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 8/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00074, train_loss=0.0530]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.43it/s, gpu_memory=1.91 GB, lr=0.00074, valid_loss=0.0203, valid_loss_mae=0.1105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 9/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.91s/it, gpu_mem=5.57 GB, lr=0.00068, train_loss=0.0190]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.41it/s, gpu_memory=1.91 GB, lr=0.00068, valid_loss=0.0131, valid_loss_mae=0.0899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 10/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00061, train_loss=0.0169]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.43it/s, gpu_memory=1.91 GB, lr=0.00061, valid_loss=0.0087, valid_loss_mae=0.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 11/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.91s/it, gpu_mem=5.57 GB, lr=0.00055, train_loss=0.0174]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.36it/s, gpu_memory=1.91 GB, lr=0.00055, valid_loss=0.0159, valid_loss_mae=0.0968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00048, train_loss=0.0144]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.34it/s, gpu_memory=1.91 GB, lr=0.00048, valid_loss=0.0104, valid_loss_mae=0.0837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00042, train_loss=0.0088]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.34it/s, gpu_memory=1.91 GB, lr=0.00042, valid_loss=0.0068, valid_loss_mae=0.0654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 14/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:08<00:00,  1.92s/it, gpu_mem=5.57 GB, lr=0.00035, train_loss=0.0060]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.31it/s, gpu_memory=1.91 GB, lr=0.00035, valid_loss=0.0047, valid_loss_mae=0.0534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 15/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:08<00:00,  1.91s/it, gpu_mem=5.57 GB, lr=0.00029, train_loss=0.0038]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.32it/s, gpu_memory=1.91 GB, lr=0.00029, valid_loss=0.0031, valid_loss_mae=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 16/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.91s/it, gpu_mem=5.57 GB, lr=0.00023, train_loss=0.0026]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.34it/s, gpu_memory=1.91 GB, lr=0.00023, valid_loss=0.0031, valid_loss_mae=0.0432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.90s/it, gpu_mem=5.57 GB, lr=0.00018, train_loss=0.0021]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.43it/s, gpu_memory=1.91 GB, lr=0.00018, valid_loss=0.0026, valid_loss_mae=0.0384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 18/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:07<00:00,  1.91s/it, gpu_mem=5.57 GB, lr=0.00013, train_loss=0.0017]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00013, valid_loss=0.0015, valid_loss_mae=0.0294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 19/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00009, train_loss=0.0013]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.65it/s, gpu_memory=1.91 GB, lr=0.00009, valid_loss=0.0015, valid_loss_mae=0.0293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 20/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00006, train_loss=0.0011]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.49it/s, gpu_memory=1.91 GB, lr=0.00006, valid_loss=0.0013, valid_loss_mae=0.0274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 21/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00003, train_loss=0.0010]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.66it/s, gpu_memory=1.91 GB, lr=0.00003, valid_loss=0.0013, valid_loss_mae=0.0270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 22/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:03<00:00,  1.84s/it, gpu_mem=5.57 GB, lr=0.00002, train_loss=0.0009]\n",
      "Valid : 100%|██████████| 133/133 [00:30<00:00,  4.43it/s, gpu_memory=1.91 GB, lr=0.00002, valid_loss=0.0011, valid_loss_mae=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 23/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.81s/it, gpu_mem=5.57 GB, lr=0.00001, train_loss=0.0008]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.54it/s, gpu_memory=1.91 GB, lr=0.00001, valid_loss=0.0011, valid_loss_mae=0.0246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 24/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.81s/it, gpu_mem=5.57 GB, lr=0.00001, train_loss=0.0008]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.73it/s, gpu_memory=1.91 GB, lr=0.00001, valid_loss=0.0010, valid_loss_mae=0.0240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save to /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln/files/epoch-best.bin\n",
      "Epoch 25/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00002, train_loss=0.0008]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.56it/s, gpu_memory=1.91 GB, lr=0.00002, valid_loss=0.0010, valid_loss_mae=0.0239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00004, train_loss=0.0008]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.57it/s, gpu_memory=1.91 GB, lr=0.00004, valid_loss=0.0011, valid_loss_mae=0.0248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:04<00:00,  1.85s/it, gpu_mem=5.57 GB, lr=0.00007, train_loss=0.0009]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00007, valid_loss=0.0012, valid_loss_mae=0.0259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.83s/it, gpu_mem=5.57 GB, lr=0.00011, train_loss=0.0011]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.61it/s, gpu_memory=1.91 GB, lr=0.00011, valid_loss=0.0013, valid_loss_mae=0.0269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:00<00:00,  1.80s/it, gpu_mem=5.57 GB, lr=0.00015, train_loss=0.0011]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.54it/s, gpu_memory=1.91 GB, lr=0.00015, valid_loss=0.0015, valid_loss_mae=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.83s/it, gpu_mem=5.57 GB, lr=0.00020, train_loss=0.0017]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00020, valid_loss=0.0028, valid_loss_mae=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:03<00:00,  1.84s/it, gpu_mem=5.57 GB, lr=0.00025, train_loss=0.0037]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.53it/s, gpu_memory=1.91 GB, lr=0.00025, valid_loss=0.0020, valid_loss_mae=0.0356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:03<00:00,  1.84s/it, gpu_mem=5.57 GB, lr=0.00031, train_loss=0.0017]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.59it/s, gpu_memory=1.91 GB, lr=0.00031, valid_loss=0.0056, valid_loss_mae=0.0578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:00<00:00,  1.79s/it, gpu_mem=5.57 GB, lr=0.00037, train_loss=0.0036]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.54it/s, gpu_memory=1.91 GB, lr=0.00037, valid_loss=0.0039, valid_loss_mae=0.0474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.83s/it, gpu_mem=5.57 GB, lr=0.00044, train_loss=0.0066]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.57it/s, gpu_memory=1.91 GB, lr=0.00044, valid_loss=0.0039, valid_loss_mae=0.0484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.81s/it, gpu_mem=5.57 GB, lr=0.00051, train_loss=0.0034]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.63it/s, gpu_memory=1.91 GB, lr=0.00051, valid_loss=0.0031, valid_loss_mae=0.0436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.81s/it, gpu_mem=5.57 GB, lr=0.00057, train_loss=0.0030]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00057, valid_loss=0.0022, valid_loss_mae=0.0367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00064, train_loss=0.0023]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.54it/s, gpu_memory=1.91 GB, lr=0.00064, valid_loss=0.0016, valid_loss_mae=0.0313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:04<00:00,  1.86s/it, gpu_mem=5.57 GB, lr=0.00070, train_loss=0.0019]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.52it/s, gpu_memory=1.91 GB, lr=0.00070, valid_loss=0.0023, valid_loss_mae=0.0376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00076, train_loss=0.0079]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.61it/s, gpu_memory=1.91 GB, lr=0.00076, valid_loss=0.0035, valid_loss_mae=0.0462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.83s/it, gpu_mem=5.57 GB, lr=0.00081, train_loss=0.0043]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.58it/s, gpu_memory=1.91 GB, lr=0.00081, valid_loss=0.0057, valid_loss_mae=0.0571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.81s/it, gpu_mem=5.57 GB, lr=0.00086, train_loss=0.0045]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00086, valid_loss=0.0038, valid_loss_mae=0.0455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00091, train_loss=0.0030]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.64it/s, gpu_memory=1.91 GB, lr=0.00091, valid_loss=0.0032, valid_loss_mae=0.0409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:00<00:00,  1.80s/it, gpu_mem=5.57 GB, lr=0.00094, train_loss=0.0026]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.62it/s, gpu_memory=1.91 GB, lr=0.00094, valid_loss=0.0020, valid_loss_mae=0.0350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:03<00:00,  1.84s/it, gpu_mem=5.57 GB, lr=0.00097, train_loss=0.0019]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.53it/s, gpu_memory=1.91 GB, lr=0.00097, valid_loss=0.0024, valid_loss_mae=0.0359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00099, train_loss=0.0020]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.54it/s, gpu_memory=1.91 GB, lr=0.00099, valid_loss=0.0020, valid_loss_mae=0.0357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00100, train_loss=0.0025]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00100, valid_loss=0.0040, valid_loss_mae=0.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00100, train_loss=0.0056]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00100, valid_loss=0.0030, valid_loss_mae=0.0427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:01<00:00,  1.82s/it, gpu_mem=5.57 GB, lr=0.00099, train_loss=0.0038]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.54it/s, gpu_memory=1.91 GB, lr=0.00099, valid_loss=0.0030, valid_loss_mae=0.0422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.83s/it, gpu_mem=5.57 GB, lr=0.00098, train_loss=0.0023]\n",
      "Valid : 100%|██████████| 133/133 [00:29<00:00,  4.55it/s, gpu_memory=1.91 GB, lr=0.00098, valid_loss=0.0025, valid_loss_mae=0.0391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 67/67 [02:02<00:00,  1.83s/it, gpu_mem=5.57 GB, lr=0.00095, train_loss=0.0022]\n",
      "Valid : 100%|██████████| 133/133 [00:28<00:00,  4.64it/s, gpu_memory=1.91 GB, lr=0.00095, valid_loss=0.0019, valid_loss_mae=0.0340]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>███▆▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid Loss</td><td>▇▇█▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid Loss MAE</td><td>▇▇█▄▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁▁▁▂▂▃▃▄▄▅▅▆▆▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Epoch</td><td>24</td></tr><tr><td>Train Loss</td><td>0.00217</td></tr><tr><td>Valid Loss</td><td>0.00192</td></tr><tr><td>Valid Loss MAE</td><td>0.03395</td></tr><tr><td>lr</td><td>0.00095</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /home/junsheng/ViLT/wandb/offline-run-20221107_190011-25yw3mln<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20221107_190011-25yw3mln/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model, history = run_training(model, optimizer, scheduler,device=config.device,num_epochs=config.max_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 384, 384]) torch.Size([4, 1, 19]) tensor([0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126287/355586058.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img).to(torch.float), torch.tensor(sensor).to(torch.float),torch.tensor(label).to(torch.float)\n"
     ]
    }
   ],
   "source": [
    "for (img,sensor,label) in valid_loader:\n",
    "    print(img.shape,sensor.shape,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'embedding_test_dict.pt')\n",
    "# print(model)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"/home/junsheng/ViLT/wandb/offline-run-20220811_120519-nzfb1xoz/files/epoch-best.bin\"))\n",
    "model.eval()\n",
    "device = config.device\n",
    "model.to(device)\n",
    "def infer(img_filename, sensor):\n",
    "    try:\n",
    "        img_path = os.path.join('pictures',img_filename)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img = pixelbert_transform(size=384)(image) # 将图像数据归一化torch.Size([3, 384, 576])\n",
    "        img = torch.tensor(img)\n",
    "        img = torch.unsqueeze(img, 0) # torch.Size([1, 3, 384, 576])\n",
    "        img = img.to(device)\n",
    "        print(\"img.shape:\",img.shape)\n",
    "    except :\n",
    "        print(\"图片加载失败！\")\n",
    "        raise\n",
    "\n",
    "    batch = dict()\n",
    "    batch[\"image\"] = img\n",
    "\n",
    "    batch['sensor_masks'] = torch.ones(1,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch['sensor'] = sensor.to(device)       \n",
    "        infer = model(batch)\n",
    "\n",
    "        print(infer)\n",
    "        sensor_emb, img_emb = infer[\"sensor_feats\"], infer[\"image_feats\"]# torch.Size([1, 23, 768]) torch.Size([1, 217, 768])\n",
    "        cls_output = infer['cls_output']\n",
    "        \n",
    "\n",
    "    return [cls_output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7997, 0.2485, 0.8657, 0.7812, 0.2747, 0.2416, 0.1669, 0.7972, 0.0488,\n",
      "        0.9594, 0.7022, 0.5115, 0.3885, 0.5780, 0.2836, 0.6937, 0.4737, 0.3196,\n",
      "        0.7872])\n",
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 1.9837e-03,  4.6217e-02, -3.0258e-03,  4.8694e-02, -4.0210e-03,\n",
      "          -1.4496e-02, -5.7631e-03, -1.1884e-02, -4.0991e-03,  1.2343e-01,\n",
      "          -3.6471e-02, -7.6417e-02, -2.5354e-03, -1.7514e-02,  9.3113e-03,\n",
      "          -7.2574e-01,  4.1012e-03, -1.0841e-03, -7.2796e-03, -1.5012e-02,\n",
      "           2.9360e-01,  1.2703e-03,  4.3528e-04, -1.0194e-02,  1.8662e-02,\n",
      "           5.0276e-03,  4.1369e-01,  2.2755e-03,  2.1262e-03, -2.1702e-05,\n",
      "          -1.1225e-02, -3.5611e-03, -5.2469e-03,  4.6728e-03, -1.6434e-02,\n",
      "           1.3562e+00, -3.6969e-03,  9.1187e-04, -1.0199e-02, -1.0962e-02,\n",
      "           2.3840e-01, -1.5973e-02, -2.6793e-03, -1.1477e-03, -4.7038e-02,\n",
      "          -1.8323e-03, -3.1893e-03, -3.6522e-04, -2.5440e-02,  1.2644e-01,\n",
      "           4.3254e-03,  1.5189e-03, -2.1334e-02,  6.4191e-04, -6.4353e-03,\n",
      "           2.4517e-02, -2.3220e-03,  2.4411e+00, -3.6682e-02, -5.5442e-03,\n",
      "           8.6023e-03,  1.3674e-01, -1.6904e-03, -8.6058e-02, -5.8487e-03,\n",
      "           1.4185e-02,  3.2303e+00, -1.7587e-01,  2.3291e-02, -5.7395e-03,\n",
      "           7.4572e-03, -4.0686e-04, -7.6169e-04,  1.4364e-03, -2.3802e-04,\n",
      "           1.9352e-03,  7.6270e-03, -6.3665e-03, -1.3034e+00,  5.3466e-02,\n",
      "           3.3903e-04, -1.7301e-02,  1.6477e-02, -5.2748e-03, -1.2973e-04,\n",
      "          -6.8714e-01,  7.4704e-03, -4.2401e-02, -3.4981e-03, -3.5005e-06,\n",
      "           1.4105e+00,  2.7867e-03,  1.2309e-04,  4.6576e-02, -1.3108e-01,\n",
      "          -9.8003e-03, -4.6890e-01,  3.0888e-04, -3.1420e-01,  1.9170e-01,\n",
      "          -1.9668e-01, -1.7651e-02,  1.0807e-02, -1.8299e-04, -2.1448e-03,\n",
      "          -2.5896e-02,  1.7032e-03,  1.3734e-03, -4.6663e-03, -8.3358e-03,\n",
      "          -9.9521e-03, -5.0667e-03,  3.1651e-01,  3.2721e-02,  6.4039e-03,\n",
      "          -3.4588e-03,  1.8386e-02, -3.4394e-02, -8.7865e-04, -1.2191e-02,\n",
      "           3.9401e-02,  1.8414e-01, -2.2838e-03,  3.2196e-02, -2.4321e-03,\n",
      "          -1.6701e-02, -7.6569e-01, -3.1337e-02,  1.3061e-03, -6.9495e-03,\n",
      "          -7.9568e-03,  2.7247e-03,  8.5281e-05,  1.7273e+00, -3.0821e-02,\n",
      "          -1.8882e-04,  2.8974e-02,  4.8482e-03,  3.2082e-03,  1.1188e-02,\n",
      "           1.3844e-02, -3.7361e-02,  9.3148e-02, -5.8730e-01, -7.8975e-03,\n",
      "          -1.4300e-02, -8.6284e-03, -8.1331e-03, -1.4560e-01,  5.6275e-03,\n",
      "          -3.5465e-01, -2.8124e-03, -8.1516e-02, -2.6081e-03, -2.5270e-02,\n",
      "          -2.7216e-03,  2.4242e-04,  1.4163e-02, -1.2159e-02, -8.8843e-03,\n",
      "          -4.8166e-02, -8.2962e-04, -5.6635e-03,  3.9105e-03, -3.2156e-01,\n",
      "          -2.2260e-03,  2.2645e-02, -1.5411e-01,  3.6258e-01, -5.6557e-02,\n",
      "          -7.5801e-03,  3.4235e-04,  4.1745e-03,  1.1632e-02,  9.6625e-03,\n",
      "           8.7152e-03, -3.0913e-01, -1.3841e-05, -2.1345e-03, -2.7807e-01,\n",
      "           7.3172e-02, -2.9576e-03, -9.7723e-03, -7.5042e-03, -2.9927e-03,\n",
      "          -4.1104e-02, -3.1050e-03, -6.0290e-03,  5.6509e-02, -3.5804e-04,\n",
      "          -4.5882e-03,  1.3528e-02, -3.4823e-03, -1.4484e-03, -1.2316e-02,\n",
      "          -1.1309e-03, -1.3150e-02, -6.7661e-03, -4.7468e-04,  2.6440e-03,\n",
      "          -8.3399e-04,  1.1606e-03,  1.3385e-02, -1.3716e-02,  2.5492e-02,\n",
      "          -5.6887e-02, -3.8384e-03, -2.5953e-01, -9.2205e-02, -5.0176e-02,\n",
      "           1.2728e-02, -3.0370e-02, -4.3282e-03, -1.7350e-02,  2.5676e-01,\n",
      "          -2.8721e-02,  2.2039e-02, -4.2963e-03,  4.8464e-02, -1.2210e+00,\n",
      "          -4.9452e-03,  2.5399e-03,  2.2361e-04, -4.7248e-04, -1.6930e-03,\n",
      "          -9.2730e-04, -3.7029e-02,  1.5085e+00, -4.1776e-02,  5.2200e-03,\n",
      "           9.7585e-04,  1.4906e-02,  3.5711e-02, -1.9911e-03,  2.8462e-02,\n",
      "          -1.1169e-02, -1.2924e-01, -1.9080e-04, -2.9924e-02, -3.3551e-03,\n",
      "           7.1063e-03, -5.3613e-03,  4.0596e-03,  3.8042e-03,  2.6935e-03,\n",
      "           4.4951e-04, -1.2442e-02, -4.2365e-02, -5.2252e-02, -6.8430e-03,\n",
      "           5.9492e-04, -1.2804e-03, -9.4352e-03, -2.6022e-03,  3.2827e-03,\n",
      "          -6.4591e-03,  5.4844e-03,  7.9269e-04, -1.4156e-03, -1.1275e-02,\n",
      "           1.1334e-03,  8.9117e-03, -1.3870e-03,  5.7367e-03, -1.8718e-02,\n",
      "           3.3558e+00, -6.7811e-03, -1.1934e-03, -3.0928e-03,  1.5423e-03,\n",
      "           4.0771e-02,  1.0289e+00,  1.7464e-02, -4.1238e-03,  1.5915e-01,\n",
      "           7.4078e-03, -4.6869e-03, -4.0438e-03,  1.7395e-02, -3.6714e-03,\n",
      "          -7.9566e-03, -1.6964e-02,  6.9600e-02, -1.1586e-03, -5.7532e-02,\n",
      "          -1.1377e+00, -1.0194e-02,  3.9660e-03, -1.8660e-03, -2.3218e-03,\n",
      "          -4.6021e-02,  2.2041e-04, -1.6894e-02,  1.0769e-02, -1.5418e-03,\n",
      "          -1.5024e-03, -1.4765e-02, -1.4385e-02,  9.1956e-04, -1.4802e-03,\n",
      "          -1.9522e-03,  3.2395e-03, -7.5023e-03, -5.4219e-03, -4.1581e-04,\n",
      "          -3.9697e-02, -8.3336e-02,  9.2057e-01,  2.7657e-01, -1.9087e-01,\n",
      "           2.2441e+00, -5.4811e-03, -8.0709e-03, -2.1413e-02,  1.4488e-01,\n",
      "          -9.3383e-02,  2.0371e-03, -3.2247e-04, -4.7818e-01,  1.3025e-03,\n",
      "           2.2350e-03,  2.4083e-03,  2.7783e-03,  1.9369e-01,  1.9064e-01,\n",
      "          -1.1178e-01,  5.4607e-04,  5.6811e-04, -3.3402e-02,  5.6972e-03,\n",
      "          -2.0078e-02,  5.0185e-01, -1.0246e-03,  1.1642e-02, -5.1414e-03,\n",
      "          -1.4134e-03, -9.7833e-02, -2.2960e-03,  1.6615e-02, -3.3400e-03,\n",
      "          -4.4910e-03, -1.1875e-03,  3.6313e-01,  5.8834e-04, -5.2402e-04,\n",
      "          -3.7137e-03,  5.8341e-04, -1.5677e-02,  3.2084e-03,  1.4964e-02,\n",
      "           3.1270e-01, -1.6785e-01, -3.0381e-03,  6.1496e-03,  1.5416e-02,\n",
      "          -2.2369e-02, -1.5025e-03, -8.4895e-01, -3.8626e-03, -1.0293e-01,\n",
      "           5.2933e-03, -7.1661e-03,  1.5060e-03, -1.3992e-02,  6.4786e-02,\n",
      "           1.7259e-04, -2.4969e-03,  2.2666e-02,  7.9659e-04, -5.7315e-03,\n",
      "          -8.6742e-04,  1.9871e-02,  6.8686e-03, -1.5742e-03,  2.0383e-03,\n",
      "          -1.0032e-01, -7.9997e-03,  4.7883e-03, -2.0692e-02, -3.0550e-03,\n",
      "          -1.1078e-01,  5.5067e-03,  3.9726e-03, -3.5795e-04, -2.4481e-02,\n",
      "          -9.2606e-02, -1.6984e-02, -1.1505e-01,  1.5262e-03, -2.1731e-03,\n",
      "          -3.6127e-01, -3.9548e-04, -6.4562e-03,  3.0156e-03, -8.6829e-03,\n",
      "          -1.7715e-02,  7.5943e-05, -3.9904e-01, -2.1628e+00, -5.7444e-03,\n",
      "          -1.6821e-01,  4.2221e-03, -7.8495e-03, -3.4038e-01, -1.7501e-02,\n",
      "          -1.6887e-02, -3.7411e-04, -1.0502e-01, -9.6129e-04, -4.7563e-03,\n",
      "          -3.5024e-03,  1.6276e-01,  2.0055e-02,  2.5318e-03,  1.6299e-02,\n",
      "           4.6860e-03, -6.8993e-04,  1.2037e-02,  2.4011e-02, -1.5320e-03,\n",
      "          -9.4426e-04,  6.2485e-02, -3.9349e-02, -2.8798e-01,  2.6349e+00,\n",
      "          -1.4212e-02,  3.7639e-02,  2.2090e-02,  1.5292e-02,  1.1471e-01,\n",
      "           9.5037e-04,  5.3700e-01,  2.7678e-03, -2.1713e-02, -1.9809e-02,\n",
      "           1.5375e-03, -3.2551e-03,  5.7640e-04,  1.1793e-01,  2.9004e-03,\n",
      "           1.0564e-02,  2.7889e-03, -6.1753e-01,  3.0269e-03, -2.5781e-03,\n",
      "          -2.3526e-03,  7.6915e-04, -3.6433e-03, -7.4983e-03,  6.4179e-04,\n",
      "          -3.2517e-02,  8.7918e-03, -1.3578e-02,  4.0756e-03,  3.9633e-03,\n",
      "          -3.5171e-03,  3.8092e-01, -2.0787e-04,  2.6121e-01, -5.4186e-01,\n",
      "           1.4743e-02, -1.0967e-01, -1.8481e-03, -6.7876e-03,  5.0279e-01,\n",
      "          -2.1711e-04, -9.2704e-03, -7.9477e-03, -9.9103e-03, -3.9425e-03,\n",
      "          -5.7466e-01,  4.1506e-03,  3.2023e-01, -1.8629e-04,  1.0300e-02,\n",
      "          -2.8636e-03, -4.4379e-04, -1.5658e+00, -1.6971e-03,  7.9097e-03,\n",
      "          -7.2553e-04,  1.2688e-03,  6.8426e-03, -1.7729e-02, -1.4651e-02,\n",
      "          -6.3581e-03, -5.7497e-01, -1.5854e-03,  1.0539e-02, -2.4659e-01,\n",
      "          -6.8186e-03,  1.0364e-02, -3.7724e-03, -2.9412e-01,  1.6166e-01,\n",
      "          -2.8965e-03,  1.2338e-02, -1.0622e-05, -1.6954e-03,  1.0149e-01,\n",
      "           4.0409e-02,  2.4940e-03, -1.2386e-02,  1.0512e-02,  5.4238e-04,\n",
      "          -9.6325e-04, -2.8328e-02,  3.1430e-01, -7.9318e-04,  4.0648e-01,\n",
      "           4.6454e-03, -1.5395e-03,  1.8001e-02,  8.0727e-02, -1.7540e-02,\n",
      "          -2.0105e-01,  3.4195e-02, -2.6117e-03,  6.6174e-03, -2.9490e-03,\n",
      "          -2.6659e-02, -2.9348e-03, -1.4326e-03, -1.0065e-01, -3.8152e-03,\n",
      "           9.4119e-02,  2.3651e-02, -1.1711e-01,  8.4785e-04,  6.9350e-03,\n",
      "          -1.6155e-03, -2.0487e-02, -4.4754e-03, -9.1077e-03, -9.6275e-03,\n",
      "           1.7915e-01,  7.4937e-02,  2.6815e-03,  5.2777e-02, -3.9888e-03,\n",
      "          -2.4798e-02,  2.3021e-02,  6.4159e-02,  4.1958e-03, -2.3663e-03,\n",
      "           2.9005e-03, -1.5671e-03, -9.1591e-02, -2.0245e-03,  7.2122e-02,\n",
      "          -3.1073e-01,  3.2231e-04, -4.2491e-01,  3.2782e-03, -2.8945e-03,\n",
      "          -3.6879e-05,  2.2672e-03, -2.9478e-02, -2.2656e-02,  5.0973e-03,\n",
      "          -5.2384e-03, -1.8399e-03, -1.0641e-02, -2.2158e-01,  3.6392e-03,\n",
      "          -3.9987e-02,  9.6524e-04, -2.2513e-02, -5.1181e-01, -3.7001e-03,\n",
      "           1.1859e-02,  5.3536e-02, -2.2422e-03,  1.2648e-03,  3.5634e-04,\n",
      "          -2.6924e-02,  3.4376e-01, -3.8054e-03,  7.1198e-05,  8.0287e-04,\n",
      "          -1.3227e-02, -6.1328e-04, -1.5956e-03, -7.0687e-03, -5.2394e-03,\n",
      "          -1.8384e-02,  1.6289e-01, -2.1286e-02, -4.4239e-02, -7.7484e-02,\n",
      "          -1.2263e-02, -7.0094e-03, -1.8447e-02, -8.2680e-03, -3.2591e+00,\n",
      "          -7.3445e-03, -2.6986e-01,  1.7344e-04,  1.1714e-03,  2.4233e-02,\n",
      "           1.9873e-03, -2.3458e-04,  1.3417e-02,  8.4771e-04,  1.2572e-02,\n",
      "          -1.0284e-02,  1.5702e-02,  1.8707e-03, -3.3123e-02,  2.5207e-02,\n",
      "           1.9239e-03, -3.2135e-02,  1.7096e-03, -1.0408e-01,  1.0599e-02,\n",
      "           3.3257e-01, -4.2357e-03, -7.1493e-03, -6.6824e-02,  2.8656e-02,\n",
      "           1.0048e-02,  5.9953e-02, -3.8427e-03, -5.4033e-03, -9.0613e-02,\n",
      "          -7.7945e-03,  1.4075e-03, -6.4806e-03, -1.2148e-03,  4.3019e-03,\n",
      "          -1.5238e+00, -7.8777e-04,  2.9253e-04,  2.8420e+00, -5.7751e-03,\n",
      "           5.6416e-02, -9.1788e-04, -1.7187e-01, -1.6822e-02, -1.5717e-02,\n",
      "           4.2368e-03, -7.7438e-03,  2.6133e-03, -1.9799e-01, -6.1636e-01,\n",
      "          -4.1575e-03,  1.5084e+00,  2.2051e-02,  7.2137e-03, -1.5463e-03,\n",
      "           4.1334e-01, -2.0542e-03, -4.0936e-02, -4.7569e-03,  1.3807e-02,\n",
      "          -9.1939e-02,  4.7250e-03,  4.5029e-02, -6.2601e-02,  2.1493e-02,\n",
      "           3.9888e-05, -1.4360e-03, -1.4647e-03, -7.0422e-04,  1.7657e-03,\n",
      "          -5.4290e-01,  2.9603e-02,  1.8244e-01,  1.3471e-03,  1.3254e-02,\n",
      "          -4.9740e-01,  4.0189e-03,  2.2165e-01,  2.4935e-03,  1.7372e-02,\n",
      "          -2.2034e-03,  1.7625e-02, -2.5429e-03,  7.2952e-02, -9.1224e-03,\n",
      "           9.5973e-02,  3.0677e-03, -1.1595e-02,  4.7705e-02,  3.4800e-03,\n",
      "           5.1260e-04,  2.5329e-03,  1.4752e-03,  4.7557e-02, -6.3601e-02,\n",
      "          -1.1752e-02, -2.6435e-03, -2.3716e-02,  1.5626e+00, -3.9445e-02,\n",
      "          -1.8886e-03, -3.7889e-01,  6.6022e-02,  1.0770e-03,  1.1034e+00,\n",
      "          -4.3177e-03, -2.7808e-01, -9.7727e-02, -3.4623e-03, -2.1748e-04,\n",
      "          -2.5945e-02, -1.4448e-03, -1.1035e-01, -2.7903e-03,  1.4531e-03,\n",
      "          -6.4623e-03,  2.2936e-01,  2.6575e-03,  4.0279e-01,  6.4278e-03,\n",
      "           2.4157e-03,  3.7755e-03, -3.5072e-02, -1.2932e-02,  1.1821e-03,\n",
      "           7.8865e-04, -8.8764e-02,  6.9273e-04, -8.0864e-03, -1.9141e-03,\n",
      "          -3.8827e-04,  1.9281e-03, -9.5316e-03, -1.0604e-03, -3.4693e-04,\n",
      "          -4.5696e-03, -1.7243e-03,  8.9633e-03,  2.5197e-03, -6.8281e-04,\n",
      "          -7.9148e-03, -3.8683e-02, -2.2333e-03,  1.3317e-01, -3.1865e-03,\n",
      "          -6.3308e-03, -1.6871e-03, -1.2553e-01, -1.4317e-02,  4.7499e-02,\n",
      "           9.6776e-02,  4.0690e-02,  2.3352e-02, -5.7105e-03,  1.9834e-04,\n",
      "          -6.5533e-03, -2.7795e-01, -2.4530e-03, -2.1690e-03,  3.9776e-05,\n",
      "           2.8123e-03, -4.2592e-01, -5.1819e-03, -3.3739e-02, -2.2679e-02,\n",
      "          -3.3789e-03, -8.9264e-01, -8.2216e-04,  5.4585e-01,  5.1367e-03,\n",
      "          -1.7192e-03, -6.3656e-03, -5.9796e-03]]], device='cuda:0'), 'image_feats': tensor([[[ 0.0019,  0.0183,  0.0013,  ...,  0.0006, -0.0011,  0.0023],\n",
      "         [ 0.0011,  0.0075,  0.0019,  ...,  0.0021, -0.0006,  0.0070],\n",
      "         [ 0.0025,  0.0202,  0.0014,  ..., -0.0018, -0.0025,  0.0077],\n",
      "         ...,\n",
      "         [ 0.0028,  0.0207,  0.0002,  ..., -0.0014, -0.0020,  0.0027],\n",
      "         [ 0.0034,  0.0228, -0.0020,  ..., -0.0059, -0.0038,  0.0025],\n",
      "         [ 0.0017,  0.0228,  0.0009,  ...,  0.0041,  0.0011,  0.0169]]],\n",
      "       device='cuda:0'), 'cls_feats': tensor([[-5.3377e-03,  5.2661e-02,  1.2068e-02, -5.4623e-01, -4.9665e-01,\n",
      "         -4.6269e-08,  1.6753e-01, -3.8817e-02,  1.7360e-01,  7.9733e-03,\n",
      "         -8.4982e-07,  4.5058e-01,  4.0896e-04,  2.4401e-02,  3.0365e-04,\n",
      "         -2.6960e-01,  1.8894e-01,  1.1567e-03, -6.8586e-04,  5.4189e-01,\n",
      "          6.0348e-01,  2.9472e-01,  5.4300e-03, -6.8249e-02, -7.1796e-06,\n",
      "          3.0101e-01,  6.5082e-04,  7.0566e-02,  1.9033e-02,  2.5250e-02,\n",
      "         -5.7814e-05, -3.5989e-03, -1.4311e-01, -1.0275e-01, -1.4772e-01,\n",
      "          7.5955e-02,  4.9898e-01, -1.6798e-02,  1.0146e-03, -6.4647e-04,\n",
      "         -5.8544e-02, -7.0829e-02, -3.3462e-03, -2.6990e-04,  5.7391e-01,\n",
      "         -5.5152e-04,  7.0339e-04, -1.2052e-04, -7.9936e-03,  1.8239e-01,\n",
      "         -4.2330e-02,  5.8321e-02, -3.3830e-01,  1.5003e-05, -2.2470e-04,\n",
      "         -1.5779e-01, -7.9541e-02, -2.8083e-02, -1.2310e-01, -9.9838e-01,\n",
      "          2.1485e-04, -8.5357e-05, -5.3916e-03, -2.8373e-01, -1.0953e-01,\n",
      "          3.4036e-02, -2.0823e-02, -5.5514e-02,  2.0348e-06,  7.4752e-03,\n",
      "         -5.2093e-02,  8.4540e-06,  4.5825e-02,  5.7334e-02, -1.0718e-07,\n",
      "         -1.6749e-02,  1.3582e-04,  1.7626e-02, -6.0085e-01,  1.0201e-04,\n",
      "          6.5122e-02, -3.2913e-02,  6.3134e-02, -1.1282e-04,  1.2470e-01,\n",
      "          2.3594e-02,  7.8678e-08, -6.5288e-02, -3.0778e-02,  2.2701e-04,\n",
      "          1.4994e-01,  8.8992e-04, -1.5288e-02,  5.2820e-02, -8.7896e-05,\n",
      "         -1.8427e-01, -1.5400e-04, -5.8158e-05,  4.5125e-02, -9.5216e-03,\n",
      "          2.9567e-01,  2.1491e-04, -5.6296e-01,  1.3584e-01, -1.9844e-01,\n",
      "          3.7549e-04, -1.0527e-03,  9.7023e-02, -2.2447e-04, -4.9126e-05,\n",
      "         -1.7129e-01,  1.3328e-02,  1.1203e-01,  1.6087e-01, -5.2941e-01,\n",
      "         -1.9368e-01,  8.1368e-03, -3.0138e-02, -5.7990e-02,  4.2351e-02,\n",
      "         -7.3688e-04, -5.8690e-02, -8.2358e-02, -3.3366e-01,  3.3144e-02,\n",
      "          6.2138e-01,  2.2296e-02, -2.3052e-02,  9.9197e-01,  4.4167e-02,\n",
      "          2.0636e-03, -4.1429e-02,  3.8450e-02, -5.5829e-02,  8.3782e-05,\n",
      "         -3.9381e-01, -4.9891e-01, -9.8411e-01, -9.4254e-04,  1.5923e-01,\n",
      "          1.5619e-01, -4.1158e-02,  1.5105e-01,  5.9432e-01, -7.7682e-05,\n",
      "         -4.4799e-02,  1.5037e-02,  6.5277e-07,  2.6042e-01, -1.5663e-01,\n",
      "          1.8536e-05,  9.9889e-01, -6.2637e-03,  9.9834e-01, -7.1294e-05,\n",
      "         -1.3204e-01,  3.6729e-01,  1.4577e-01, -2.3287e-03,  2.7225e-07,\n",
      "         -1.5243e-01,  3.4126e-02,  2.0917e-03, -3.9592e-01, -2.4719e-02,\n",
      "          9.7888e-01,  9.1085e-02,  1.0322e-03,  8.6366e-02, -1.4770e-01,\n",
      "          5.6357e-04,  3.7067e-04, -9.9070e-01,  1.1552e-02, -9.8343e-04,\n",
      "          8.8300e-03,  1.6026e-02, -1.3830e-02,  2.7184e-02, -7.1285e-04,\n",
      "          1.9178e-04,  4.6451e-01,  5.2300e-01, -5.2968e-03, -1.7609e-05,\n",
      "         -3.6829e-10,  8.9875e-03,  5.2931e-06,  1.0528e-01, -6.3054e-05,\n",
      "          4.0771e-05, -2.0513e-04, -4.3132e-05,  4.8204e-01,  3.6864e-05,\n",
      "          4.1865e-04,  6.6312e-03, -5.0822e-02,  2.6965e-02, -4.7009e-02,\n",
      "          1.5976e-02,  1.4469e-02,  6.9028e-02,  2.2068e-02,  2.7468e-06,\n",
      "          2.8982e-02,  7.8261e-05, -6.1951e-04,  4.7378e-06,  8.4356e-05,\n",
      "          1.4783e-01,  4.5919e-01,  1.3345e-01,  1.7308e-02,  1.0022e-02,\n",
      "         -1.2668e-01,  2.3772e-03,  3.7537e-02, -1.4602e-03,  3.0340e-02,\n",
      "         -1.2100e-02, -5.4407e-04, -6.3185e-01, -3.1143e-02,  1.9241e-05,\n",
      "         -2.4241e-01,  3.8638e-02,  1.4022e-02, -4.9556e-01, -4.4979e-02,\n",
      "          3.5769e-01, -2.6635e-04,  1.5801e-04, -5.4907e-01, -4.7064e-04,\n",
      "         -1.3535e-03, -1.8582e-03,  5.2453e-01,  1.4328e-05,  2.3296e-03,\n",
      "          5.6016e-02, -8.7685e-02,  6.7497e-05,  5.1542e-01,  1.1463e-01,\n",
      "         -1.2605e-01, -5.9693e-02,  1.8356e-01, -9.9299e-01,  3.7857e-04,\n",
      "          9.7356e-01, -9.9804e-01, -1.1910e-01,  5.0536e-06, -1.3428e-01,\n",
      "          1.1336e-01, -7.1862e-03, -2.1117e-04,  6.2957e-02, -2.7963e-04,\n",
      "          1.8242e-01, -6.7076e-03, -1.7136e-03,  1.3683e-01, -5.2258e-01,\n",
      "         -5.3749e-01, -3.6167e-02, -2.1222e-03,  1.3861e-02,  1.9291e-01,\n",
      "          1.2119e-04, -1.6902e-01, -1.7791e-01, -4.4162e-01, -6.4703e-05,\n",
      "          2.6462e-02, -7.0808e-02, -3.4349e-04, -1.2866e-01, -5.6282e-02,\n",
      "          4.3570e-01,  6.1666e-02,  7.8857e-02,  8.6557e-03, -1.1047e-02,\n",
      "          5.2848e-02, -1.6982e-01, -9.8310e-01,  4.3276e-01, -1.9919e-02,\n",
      "         -2.9157e-02,  2.8544e-01,  3.3157e-03, -1.4466e-06, -2.8333e-02,\n",
      "          3.5554e-05, -4.7926e-04, -3.3296e-01, -5.6068e-05, -3.1411e-02,\n",
      "          1.4673e-02, -6.2650e-01, -4.6746e-01,  2.6915e-02,  3.1861e-02,\n",
      "          2.8999e-01, -1.3590e-01, -2.1051e-03, -8.7108e-04,  1.7723e-05,\n",
      "         -1.7831e-01,  9.9698e-01,  1.8078e-01,  1.5222e-01, -2.4161e-03,\n",
      "          9.4548e-02, -4.1539e-06,  1.0090e-01, -6.5995e-02,  8.2342e-11,\n",
      "         -1.4069e-01,  1.0372e-01, -1.5321e-01,  1.8640e-02,  5.0071e-02,\n",
      "          1.0222e-01, -4.6180e-02,  1.1697e-01, -8.3365e-04,  5.2887e-01,\n",
      "         -1.4739e-01,  4.8101e-02, -2.1626e-01,  4.0196e-04, -1.7281e-01,\n",
      "         -4.4038e-05, -5.8329e-02,  4.9082e-05, -1.4593e-01,  6.5082e-02,\n",
      "          5.4184e-01, -2.5368e-02, -1.0197e-01, -4.9696e-02,  8.0638e-02,\n",
      "          1.7689e-01,  1.8172e-05, -9.2650e-04, -2.3362e-02,  2.2361e-05,\n",
      "         -5.3354e-01,  2.1110e-02, -2.4159e-02, -1.3924e-01, -2.0145e-02,\n",
      "          2.5612e-03,  5.1180e-03, -4.5323e-03, -9.0713e-06,  7.3369e-06,\n",
      "         -8.6271e-02,  8.3008e-02,  1.0275e-01,  3.7406e-06, -8.4993e-02,\n",
      "         -2.1791e-03,  3.2108e-01, -3.3179e-03,  1.0832e-01, -2.4965e-08,\n",
      "         -1.9853e-03, -3.3631e-05,  1.0677e-01, -7.5458e-04,  1.3104e-05,\n",
      "          6.9465e-05,  3.8516e-01, -1.8571e-02, -7.1672e-02,  1.7289e-04,\n",
      "         -2.0516e-03, -2.8865e-04, -9.9425e-02,  3.3670e-05, -6.5594e-04,\n",
      "         -7.3241e-03,  3.5792e-04, -5.5659e-02, -8.1540e-03, -1.1814e-01,\n",
      "         -1.9689e-05,  5.3022e-04, -7.0278e-02, -2.3249e-02, -5.2910e-05,\n",
      "         -6.0277e-03, -4.7218e-03,  3.2156e-01,  1.4319e-01, -1.1597e-01,\n",
      "          8.2386e-02,  9.8239e-01,  2.2286e-05,  1.9092e-01,  4.9976e-02,\n",
      "         -1.8010e-01,  4.9386e-04, -2.7991e-03, -1.6495e-02,  4.6164e-01,\n",
      "          4.4670e-02,  1.9476e-06,  7.5764e-02, -2.3509e-02, -5.5319e-02,\n",
      "         -3.8533e-03,  1.1254e-02,  1.6983e-03, -1.7214e-04,  1.2938e-01,\n",
      "          6.0222e-01,  3.2513e-03,  5.5831e-02, -1.9620e-04, -1.3419e-04,\n",
      "         -5.3317e-03, -1.6980e-04, -1.7985e-01, -7.4854e-01, -5.5228e-01,\n",
      "          1.0855e-03,  1.2698e-02,  1.3304e-02,  3.2127e-02,  4.8959e-01,\n",
      "          2.2819e-03, -9.9530e-01,  8.7765e-03,  7.3958e-05,  5.6097e-02,\n",
      "         -5.4835e-02, -4.7910e-04,  2.1124e-04,  5.3076e-03, -3.0005e-04,\n",
      "         -1.6424e-02,  3.9793e-04,  5.3044e-02,  8.2877e-04, -2.7507e-02,\n",
      "          4.5512e-02, -3.4633e-02,  5.3924e-02, -6.0422e-04,  1.5557e-01,\n",
      "         -1.2825e-04, -1.4201e-01,  1.7243e-04,  1.6815e-01,  9.7874e-04,\n",
      "         -3.6930e-01, -1.1668e-01, -3.9293e-01, -3.7074e-04, -5.3913e-01,\n",
      "         -1.0352e-02,  2.0277e-02, -2.0098e-05, -1.5789e-01,  1.6065e-01,\n",
      "          5.0610e-03, -6.7256e-01,  3.8089e-02, -5.1068e-06,  1.9496e-04,\n",
      "          1.7233e-01, -4.4444e-01,  3.1353e-04, -1.8546e-01,  7.3902e-01,\n",
      "         -5.4305e-02, -1.1607e-01, -4.3838e-01,  3.5266e-02,  4.8589e-01,\n",
      "          2.8899e-02, -3.4504e-06, -6.9880e-01, -1.1848e-01, -3.4889e-02,\n",
      "          3.3284e-05,  5.0877e-04, -5.3478e-02, -4.8689e-01,  4.1288e-02,\n",
      "          1.8354e-01, -4.0538e-02, -7.6118e-03, -1.2620e-02, -1.1261e-01,\n",
      "          5.1177e-03,  2.4080e-04, -3.6166e-02, -5.1449e-01, -5.1364e-01,\n",
      "         -2.3644e-02,  2.1584e-01,  1.2935e-01, -2.1157e-02,  1.2428e-04,\n",
      "          6.1816e-03,  7.2106e-01, -2.7537e-01, -5.5552e-01,  4.1438e-01,\n",
      "         -1.0136e-04,  4.5649e-02,  5.1509e-03,  9.9283e-02, -7.0594e-02,\n",
      "          3.9558e-02,  5.4006e-02, -8.1767e-02, -1.3002e-01,  4.7269e-02,\n",
      "         -1.3444e-01, -4.1266e-02,  1.5848e-01, -4.6707e-05,  5.1958e-04,\n",
      "          5.5974e-02, -3.4980e-01, -4.0879e-03,  2.6837e-02,  1.4286e-01,\n",
      "          4.3780e-01,  8.2834e-07, -4.2571e-01, -5.6852e-01,  1.4919e-05,\n",
      "         -4.1577e-02,  3.7498e-03,  2.1796e-03, -4.8518e-01,  1.2284e-05,\n",
      "          4.6110e-01, -1.3618e-02,  2.3314e-04, -6.5052e-02, -3.1120e-02,\n",
      "          3.9638e-04,  1.4962e-01, -1.8447e-01, -6.1874e-01,  5.4475e-02,\n",
      "          7.1698e-03,  5.1877e-01, -2.5224e-02, -1.1259e-01, -3.7520e-02,\n",
      "          5.8149e-03,  1.4983e-01,  1.9068e-01, -3.0908e-03,  5.0923e-03,\n",
      "         -1.3770e-01,  1.2993e-01, -5.8308e-04, -1.2960e-04,  3.1082e-01,\n",
      "         -1.6604e-02,  5.7011e-01, -4.2936e-01,  1.3865e-02,  2.3333e-02,\n",
      "         -8.0839e-06,  1.0068e-01, -1.7369e-04,  5.8272e-03, -4.2725e-01,\n",
      "          2.5488e-02,  1.1621e-03, -7.3529e-02,  1.4387e-01, -1.5492e-02,\n",
      "          1.7533e-02,  1.3898e-01, -3.7139e-02,  3.0677e-03, -4.2441e-03,\n",
      "          2.1308e-03, -4.2977e-02,  9.9311e-01,  2.8530e-03,  2.5465e-02,\n",
      "          3.7380e-02, -2.4878e-03, -3.0767e-02, -2.2215e-02,  1.4239e-01,\n",
      "         -1.8812e-04, -2.8530e-04,  1.3687e-01,  7.8001e-06,  9.9062e-03,\n",
      "         -1.5400e-01, -1.9352e-01,  3.7956e-01,  1.7398e-01, -2.0020e-04,\n",
      "         -1.5651e-04,  3.7809e-02,  4.4828e-03, -9.5426e-02,  1.2925e-02,\n",
      "          1.2830e-03,  2.8441e-04, -1.6474e-01, -1.0164e-02, -1.9015e-04,\n",
      "         -9.9753e-01,  9.4627e-02,  1.3922e-01, -6.5095e-07, -3.9406e-02,\n",
      "         -3.3764e-06,  1.8495e-01, -6.0875e-03,  5.7051e-02,  3.9218e-01,\n",
      "         -9.5887e-02, -1.1931e-01,  1.5925e-01,  1.7763e-04, -2.1009e-02,\n",
      "          2.1241e-04,  6.8218e-05,  7.4600e-03,  2.7709e-02, -8.2521e-02,\n",
      "          2.4447e-02, -1.0464e-04, -5.2140e-01, -1.3095e-02, -3.2027e-01,\n",
      "          2.9741e-01,  3.2213e-02, -1.3045e-02,  8.2432e-04, -9.8703e-01,\n",
      "          6.0834e-04,  6.2796e-05, -4.6495e-01, -7.8908e-02, -4.7152e-02,\n",
      "         -1.2066e-01,  9.2991e-05,  4.5209e-02, -4.3231e-02,  1.9348e-01,\n",
      "          1.1522e-02,  1.9562e-01,  4.1430e-04,  6.7467e-02, -2.1225e-05,\n",
      "         -1.7167e-03, -7.8564e-02,  3.1042e-01, -9.4272e-04, -1.8097e-01,\n",
      "         -1.6356e-01,  5.4789e-03,  3.0200e-03,  6.0928e-04, -3.1026e-01,\n",
      "          1.0584e-03, -5.6018e-04, -7.0516e-03, -1.8934e-01,  1.4302e-02,\n",
      "         -1.7049e-01,  1.1376e-01, -5.5827e-01, -3.7476e-01,  1.2985e-01,\n",
      "          4.6394e-01,  5.7343e-02,  1.1016e-04, -4.7534e-01,  2.6864e-03,\n",
      "          4.1764e-04, -9.9870e-01,  1.3690e-02, -6.6187e-04, -8.3058e-04,\n",
      "         -2.5697e-05,  6.7533e-02,  3.3373e-01,  3.8451e-02,  5.3215e-03,\n",
      "         -2.3418e-01, -2.7300e-04, -1.4208e-01, -1.2816e-01,  1.3162e-01,\n",
      "          6.6139e-02, -4.5003e-03, -5.4344e-01,  2.1168e-04, -1.6722e-01,\n",
      "          1.7441e-04,  1.9294e-01, -1.3461e-05, -2.1954e-01,  5.1294e-02,\n",
      "          2.6190e-02,  1.8663e-01, -3.9694e-02, -1.4917e-01,  1.8020e-03,\n",
      "         -7.7321e-03, -2.1183e-03,  6.5545e-02,  5.4491e-02, -6.8549e-03,\n",
      "         -1.6979e-01, -3.5441e-04, -5.9004e-01, -4.1698e-04,  2.9904e-02,\n",
      "          1.7996e-02, -3.2304e-03, -9.9916e-02,  1.0159e-01,  3.9226e-01,\n",
      "         -1.6991e-04, -1.5456e-01, -4.3228e-01,  1.6658e-03,  4.0342e-01,\n",
      "         -2.7954e-02, -5.5582e-02,  3.7613e-02, -2.1674e-01,  2.0142e-03,\n",
      "          3.4885e-02,  4.7280e-05, -1.0242e-03, -8.7301e-02, -2.7654e-01,\n",
      "         -9.9533e-01, -6.0402e-04,  6.8521e-05,  1.6231e-01, -4.1715e-05,\n",
      "          1.9535e-01,  4.5902e-01,  1.5578e-03, -2.5529e-03, -2.4900e-03,\n",
      "         -3.0925e-01,  3.2815e-02,  3.7209e-05, -1.7382e-01,  3.1550e-04,\n",
      "          1.5747e-01, -5.3189e-01, -1.9636e-05]], device='cuda:0'), 'raw_cls_feats': tensor([[ 1.9837e-03,  4.6217e-02, -3.0258e-03,  4.8694e-02, -4.0210e-03,\n",
      "         -1.4496e-02, -5.7631e-03, -1.1884e-02, -4.0991e-03,  1.2343e-01,\n",
      "         -3.6471e-02, -7.6417e-02, -2.5354e-03, -1.7514e-02,  9.3113e-03,\n",
      "         -7.2574e-01,  4.1012e-03, -1.0841e-03, -7.2796e-03, -1.5012e-02,\n",
      "          2.9360e-01,  1.2703e-03,  4.3528e-04, -1.0194e-02,  1.8662e-02,\n",
      "          5.0276e-03,  4.1369e-01,  2.2755e-03,  2.1262e-03, -2.1702e-05,\n",
      "         -1.1225e-02, -3.5611e-03, -5.2469e-03,  4.6728e-03, -1.6434e-02,\n",
      "          1.3562e+00, -3.6969e-03,  9.1187e-04, -1.0199e-02, -1.0962e-02,\n",
      "          2.3840e-01, -1.5973e-02, -2.6793e-03, -1.1477e-03, -4.7038e-02,\n",
      "         -1.8323e-03, -3.1893e-03, -3.6522e-04, -2.5440e-02,  1.2644e-01,\n",
      "          4.3254e-03,  1.5189e-03, -2.1334e-02,  6.4191e-04, -6.4353e-03,\n",
      "          2.4517e-02, -2.3220e-03,  2.4411e+00, -3.6682e-02, -5.5442e-03,\n",
      "          8.6023e-03,  1.3674e-01, -1.6904e-03, -8.6058e-02, -5.8487e-03,\n",
      "          1.4185e-02,  3.2303e+00, -1.7587e-01,  2.3291e-02, -5.7395e-03,\n",
      "          7.4572e-03, -4.0686e-04, -7.6169e-04,  1.4364e-03, -2.3802e-04,\n",
      "          1.9352e-03,  7.6270e-03, -6.3665e-03, -1.3034e+00,  5.3466e-02,\n",
      "          3.3903e-04, -1.7301e-02,  1.6477e-02, -5.2748e-03, -1.2973e-04,\n",
      "         -6.8714e-01,  7.4704e-03, -4.2401e-02, -3.4981e-03, -3.5005e-06,\n",
      "          1.4105e+00,  2.7867e-03,  1.2309e-04,  4.6576e-02, -1.3108e-01,\n",
      "         -9.8003e-03, -4.6890e-01,  3.0888e-04, -3.1420e-01,  1.9170e-01,\n",
      "         -1.9668e-01, -1.7651e-02,  1.0807e-02, -1.8299e-04, -2.1448e-03,\n",
      "         -2.5896e-02,  1.7032e-03,  1.3734e-03, -4.6663e-03, -8.3358e-03,\n",
      "         -9.9521e-03, -5.0667e-03,  3.1651e-01,  3.2721e-02,  6.4039e-03,\n",
      "         -3.4588e-03,  1.8386e-02, -3.4394e-02, -8.7865e-04, -1.2191e-02,\n",
      "          3.9401e-02,  1.8414e-01, -2.2838e-03,  3.2196e-02, -2.4321e-03,\n",
      "         -1.6701e-02, -7.6569e-01, -3.1337e-02,  1.3061e-03, -6.9495e-03,\n",
      "         -7.9568e-03,  2.7247e-03,  8.5281e-05,  1.7273e+00, -3.0821e-02,\n",
      "         -1.8882e-04,  2.8974e-02,  4.8482e-03,  3.2082e-03,  1.1188e-02,\n",
      "          1.3844e-02, -3.7361e-02,  9.3148e-02, -5.8730e-01, -7.8975e-03,\n",
      "         -1.4300e-02, -8.6284e-03, -8.1331e-03, -1.4560e-01,  5.6275e-03,\n",
      "         -3.5465e-01, -2.8124e-03, -8.1516e-02, -2.6081e-03, -2.5270e-02,\n",
      "         -2.7216e-03,  2.4242e-04,  1.4163e-02, -1.2159e-02, -8.8843e-03,\n",
      "         -4.8166e-02, -8.2962e-04, -5.6635e-03,  3.9105e-03, -3.2156e-01,\n",
      "         -2.2260e-03,  2.2645e-02, -1.5411e-01,  3.6258e-01, -5.6557e-02,\n",
      "         -7.5801e-03,  3.4235e-04,  4.1745e-03,  1.1632e-02,  9.6625e-03,\n",
      "          8.7152e-03, -3.0913e-01, -1.3841e-05, -2.1345e-03, -2.7807e-01,\n",
      "          7.3172e-02, -2.9576e-03, -9.7723e-03, -7.5042e-03, -2.9927e-03,\n",
      "         -4.1104e-02, -3.1050e-03, -6.0290e-03,  5.6509e-02, -3.5804e-04,\n",
      "         -4.5882e-03,  1.3528e-02, -3.4823e-03, -1.4484e-03, -1.2316e-02,\n",
      "         -1.1309e-03, -1.3150e-02, -6.7661e-03, -4.7468e-04,  2.6440e-03,\n",
      "         -8.3399e-04,  1.1606e-03,  1.3385e-02, -1.3716e-02,  2.5492e-02,\n",
      "         -5.6887e-02, -3.8384e-03, -2.5953e-01, -9.2205e-02, -5.0176e-02,\n",
      "          1.2728e-02, -3.0370e-02, -4.3282e-03, -1.7350e-02,  2.5676e-01,\n",
      "         -2.8721e-02,  2.2039e-02, -4.2963e-03,  4.8464e-02, -1.2210e+00,\n",
      "         -4.9452e-03,  2.5399e-03,  2.2361e-04, -4.7248e-04, -1.6930e-03,\n",
      "         -9.2730e-04, -3.7029e-02,  1.5085e+00, -4.1776e-02,  5.2200e-03,\n",
      "          9.7585e-04,  1.4906e-02,  3.5711e-02, -1.9911e-03,  2.8462e-02,\n",
      "         -1.1169e-02, -1.2924e-01, -1.9080e-04, -2.9924e-02, -3.3551e-03,\n",
      "          7.1063e-03, -5.3613e-03,  4.0596e-03,  3.8042e-03,  2.6935e-03,\n",
      "          4.4951e-04, -1.2442e-02, -4.2365e-02, -5.2252e-02, -6.8430e-03,\n",
      "          5.9492e-04, -1.2804e-03, -9.4352e-03, -2.6022e-03,  3.2827e-03,\n",
      "         -6.4591e-03,  5.4844e-03,  7.9269e-04, -1.4156e-03, -1.1275e-02,\n",
      "          1.1334e-03,  8.9117e-03, -1.3870e-03,  5.7367e-03, -1.8718e-02,\n",
      "          3.3558e+00, -6.7811e-03, -1.1934e-03, -3.0928e-03,  1.5423e-03,\n",
      "          4.0771e-02,  1.0289e+00,  1.7464e-02, -4.1238e-03,  1.5915e-01,\n",
      "          7.4078e-03, -4.6869e-03, -4.0438e-03,  1.7395e-02, -3.6714e-03,\n",
      "         -7.9566e-03, -1.6964e-02,  6.9600e-02, -1.1586e-03, -5.7532e-02,\n",
      "         -1.1377e+00, -1.0194e-02,  3.9660e-03, -1.8660e-03, -2.3218e-03,\n",
      "         -4.6021e-02,  2.2041e-04, -1.6894e-02,  1.0769e-02, -1.5418e-03,\n",
      "         -1.5024e-03, -1.4765e-02, -1.4385e-02,  9.1956e-04, -1.4802e-03,\n",
      "         -1.9522e-03,  3.2395e-03, -7.5023e-03, -5.4219e-03, -4.1581e-04,\n",
      "         -3.9697e-02, -8.3336e-02,  9.2057e-01,  2.7657e-01, -1.9087e-01,\n",
      "          2.2441e+00, -5.4811e-03, -8.0709e-03, -2.1413e-02,  1.4488e-01,\n",
      "         -9.3383e-02,  2.0371e-03, -3.2247e-04, -4.7818e-01,  1.3025e-03,\n",
      "          2.2350e-03,  2.4083e-03,  2.7783e-03,  1.9369e-01,  1.9064e-01,\n",
      "         -1.1178e-01,  5.4607e-04,  5.6811e-04, -3.3402e-02,  5.6972e-03,\n",
      "         -2.0078e-02,  5.0185e-01, -1.0246e-03,  1.1642e-02, -5.1414e-03,\n",
      "         -1.4134e-03, -9.7833e-02, -2.2960e-03,  1.6615e-02, -3.3400e-03,\n",
      "         -4.4910e-03, -1.1875e-03,  3.6313e-01,  5.8834e-04, -5.2402e-04,\n",
      "         -3.7137e-03,  5.8341e-04, -1.5677e-02,  3.2084e-03,  1.4964e-02,\n",
      "          3.1270e-01, -1.6785e-01, -3.0381e-03,  6.1496e-03,  1.5416e-02,\n",
      "         -2.2369e-02, -1.5025e-03, -8.4895e-01, -3.8626e-03, -1.0293e-01,\n",
      "          5.2933e-03, -7.1661e-03,  1.5060e-03, -1.3992e-02,  6.4786e-02,\n",
      "          1.7259e-04, -2.4969e-03,  2.2666e-02,  7.9659e-04, -5.7315e-03,\n",
      "         -8.6742e-04,  1.9871e-02,  6.8686e-03, -1.5742e-03,  2.0383e-03,\n",
      "         -1.0032e-01, -7.9997e-03,  4.7883e-03, -2.0692e-02, -3.0550e-03,\n",
      "         -1.1078e-01,  5.5067e-03,  3.9726e-03, -3.5795e-04, -2.4481e-02,\n",
      "         -9.2606e-02, -1.6984e-02, -1.1505e-01,  1.5262e-03, -2.1731e-03,\n",
      "         -3.6127e-01, -3.9548e-04, -6.4562e-03,  3.0156e-03, -8.6829e-03,\n",
      "         -1.7715e-02,  7.5943e-05, -3.9904e-01, -2.1628e+00, -5.7444e-03,\n",
      "         -1.6821e-01,  4.2221e-03, -7.8495e-03, -3.4038e-01, -1.7501e-02,\n",
      "         -1.6887e-02, -3.7411e-04, -1.0502e-01, -9.6129e-04, -4.7563e-03,\n",
      "         -3.5024e-03,  1.6276e-01,  2.0055e-02,  2.5318e-03,  1.6299e-02,\n",
      "          4.6860e-03, -6.8993e-04,  1.2037e-02,  2.4011e-02, -1.5320e-03,\n",
      "         -9.4426e-04,  6.2485e-02, -3.9349e-02, -2.8798e-01,  2.6349e+00,\n",
      "         -1.4212e-02,  3.7639e-02,  2.2090e-02,  1.5292e-02,  1.1471e-01,\n",
      "          9.5037e-04,  5.3700e-01,  2.7678e-03, -2.1713e-02, -1.9809e-02,\n",
      "          1.5375e-03, -3.2551e-03,  5.7640e-04,  1.1793e-01,  2.9004e-03,\n",
      "          1.0564e-02,  2.7889e-03, -6.1753e-01,  3.0269e-03, -2.5781e-03,\n",
      "         -2.3526e-03,  7.6915e-04, -3.6433e-03, -7.4983e-03,  6.4179e-04,\n",
      "         -3.2517e-02,  8.7918e-03, -1.3578e-02,  4.0756e-03,  3.9633e-03,\n",
      "         -3.5171e-03,  3.8092e-01, -2.0787e-04,  2.6121e-01, -5.4186e-01,\n",
      "          1.4743e-02, -1.0967e-01, -1.8481e-03, -6.7876e-03,  5.0279e-01,\n",
      "         -2.1711e-04, -9.2704e-03, -7.9477e-03, -9.9103e-03, -3.9425e-03,\n",
      "         -5.7466e-01,  4.1506e-03,  3.2023e-01, -1.8629e-04,  1.0300e-02,\n",
      "         -2.8636e-03, -4.4379e-04, -1.5658e+00, -1.6971e-03,  7.9097e-03,\n",
      "         -7.2553e-04,  1.2688e-03,  6.8426e-03, -1.7729e-02, -1.4651e-02,\n",
      "         -6.3581e-03, -5.7497e-01, -1.5854e-03,  1.0539e-02, -2.4659e-01,\n",
      "         -6.8186e-03,  1.0364e-02, -3.7724e-03, -2.9412e-01,  1.6166e-01,\n",
      "         -2.8965e-03,  1.2338e-02, -1.0622e-05, -1.6954e-03,  1.0149e-01,\n",
      "          4.0409e-02,  2.4940e-03, -1.2386e-02,  1.0512e-02,  5.4238e-04,\n",
      "         -9.6325e-04, -2.8328e-02,  3.1430e-01, -7.9318e-04,  4.0648e-01,\n",
      "          4.6454e-03, -1.5395e-03,  1.8001e-02,  8.0727e-02, -1.7540e-02,\n",
      "         -2.0105e-01,  3.4195e-02, -2.6117e-03,  6.6174e-03, -2.9490e-03,\n",
      "         -2.6659e-02, -2.9348e-03, -1.4326e-03, -1.0065e-01, -3.8152e-03,\n",
      "          9.4119e-02,  2.3651e-02, -1.1711e-01,  8.4785e-04,  6.9350e-03,\n",
      "         -1.6155e-03, -2.0487e-02, -4.4754e-03, -9.1077e-03, -9.6275e-03,\n",
      "          1.7915e-01,  7.4937e-02,  2.6815e-03,  5.2777e-02, -3.9888e-03,\n",
      "         -2.4798e-02,  2.3021e-02,  6.4159e-02,  4.1958e-03, -2.3663e-03,\n",
      "          2.9005e-03, -1.5671e-03, -9.1591e-02, -2.0245e-03,  7.2122e-02,\n",
      "         -3.1073e-01,  3.2231e-04, -4.2491e-01,  3.2782e-03, -2.8945e-03,\n",
      "         -3.6879e-05,  2.2672e-03, -2.9478e-02, -2.2656e-02,  5.0973e-03,\n",
      "         -5.2384e-03, -1.8399e-03, -1.0641e-02, -2.2158e-01,  3.6392e-03,\n",
      "         -3.9987e-02,  9.6524e-04, -2.2513e-02, -5.1181e-01, -3.7001e-03,\n",
      "          1.1859e-02,  5.3536e-02, -2.2422e-03,  1.2648e-03,  3.5634e-04,\n",
      "         -2.6924e-02,  3.4376e-01, -3.8054e-03,  7.1198e-05,  8.0287e-04,\n",
      "         -1.3227e-02, -6.1328e-04, -1.5956e-03, -7.0687e-03, -5.2394e-03,\n",
      "         -1.8384e-02,  1.6289e-01, -2.1286e-02, -4.4239e-02, -7.7484e-02,\n",
      "         -1.2263e-02, -7.0094e-03, -1.8447e-02, -8.2680e-03, -3.2591e+00,\n",
      "         -7.3445e-03, -2.6986e-01,  1.7344e-04,  1.1714e-03,  2.4233e-02,\n",
      "          1.9873e-03, -2.3458e-04,  1.3417e-02,  8.4771e-04,  1.2572e-02,\n",
      "         -1.0284e-02,  1.5702e-02,  1.8707e-03, -3.3123e-02,  2.5207e-02,\n",
      "          1.9239e-03, -3.2135e-02,  1.7096e-03, -1.0408e-01,  1.0599e-02,\n",
      "          3.3257e-01, -4.2357e-03, -7.1493e-03, -6.6824e-02,  2.8656e-02,\n",
      "          1.0048e-02,  5.9953e-02, -3.8427e-03, -5.4033e-03, -9.0613e-02,\n",
      "         -7.7945e-03,  1.4075e-03, -6.4806e-03, -1.2148e-03,  4.3019e-03,\n",
      "         -1.5238e+00, -7.8777e-04,  2.9253e-04,  2.8420e+00, -5.7751e-03,\n",
      "          5.6416e-02, -9.1788e-04, -1.7187e-01, -1.6822e-02, -1.5717e-02,\n",
      "          4.2368e-03, -7.7438e-03,  2.6133e-03, -1.9799e-01, -6.1636e-01,\n",
      "         -4.1575e-03,  1.5084e+00,  2.2051e-02,  7.2137e-03, -1.5463e-03,\n",
      "          4.1334e-01, -2.0542e-03, -4.0936e-02, -4.7569e-03,  1.3807e-02,\n",
      "         -9.1939e-02,  4.7250e-03,  4.5029e-02, -6.2601e-02,  2.1493e-02,\n",
      "          3.9888e-05, -1.4360e-03, -1.4647e-03, -7.0422e-04,  1.7657e-03,\n",
      "         -5.4290e-01,  2.9603e-02,  1.8244e-01,  1.3471e-03,  1.3254e-02,\n",
      "         -4.9740e-01,  4.0189e-03,  2.2165e-01,  2.4935e-03,  1.7372e-02,\n",
      "         -2.2034e-03,  1.7625e-02, -2.5429e-03,  7.2952e-02, -9.1224e-03,\n",
      "          9.5973e-02,  3.0677e-03, -1.1595e-02,  4.7705e-02,  3.4800e-03,\n",
      "          5.1260e-04,  2.5329e-03,  1.4752e-03,  4.7557e-02, -6.3601e-02,\n",
      "         -1.1752e-02, -2.6435e-03, -2.3716e-02,  1.5626e+00, -3.9445e-02,\n",
      "         -1.8886e-03, -3.7889e-01,  6.6022e-02,  1.0770e-03,  1.1034e+00,\n",
      "         -4.3177e-03, -2.7808e-01, -9.7727e-02, -3.4623e-03, -2.1748e-04,\n",
      "         -2.5945e-02, -1.4448e-03, -1.1035e-01, -2.7903e-03,  1.4531e-03,\n",
      "         -6.4623e-03,  2.2936e-01,  2.6575e-03,  4.0279e-01,  6.4278e-03,\n",
      "          2.4157e-03,  3.7755e-03, -3.5072e-02, -1.2932e-02,  1.1821e-03,\n",
      "          7.8865e-04, -8.8764e-02,  6.9273e-04, -8.0864e-03, -1.9141e-03,\n",
      "         -3.8827e-04,  1.9281e-03, -9.5316e-03, -1.0604e-03, -3.4693e-04,\n",
      "         -4.5696e-03, -1.7243e-03,  8.9633e-03,  2.5197e-03, -6.8281e-04,\n",
      "         -7.9148e-03, -3.8683e-02, -2.2333e-03,  1.3317e-01, -3.1865e-03,\n",
      "         -6.3308e-03, -1.6871e-03, -1.2553e-01, -1.4317e-02,  4.7499e-02,\n",
      "          9.6776e-02,  4.0690e-02,  2.3352e-02, -5.7105e-03,  1.9834e-04,\n",
      "         -6.5533e-03, -2.7795e-01, -2.4530e-03, -2.1690e-03,  3.9776e-05,\n",
      "          2.8123e-03, -4.2592e-01, -5.1819e-03, -3.3739e-02, -2.2679e-02,\n",
      "         -3.3789e-03, -8.9264e-01, -8.2216e-04,  5.4585e-01,  5.1367e-03,\n",
      "         -1.7192e-03, -6.3656e-03, -5.9796e-03]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 5,  4],\n",
      "         [ 1,  7],\n",
      "         [ 9, 16],\n",
      "         [ 6,  3],\n",
      "         [ 1, 16],\n",
      "         [ 0, 15],\n",
      "         [ 7, 11],\n",
      "         [ 9, 14],\n",
      "         [ 1,  1],\n",
      "         [ 0,  2],\n",
      "         [ 9,  5],\n",
      "         [ 4, 16],\n",
      "         [ 6,  1],\n",
      "         [ 4, 13],\n",
      "         [ 7,  4],\n",
      "         [ 2,  6],\n",
      "         [ 0, 18],\n",
      "         [ 4, 17],\n",
      "         [ 4, 15],\n",
      "         [ 4,  2],\n",
      "         [ 8, 15],\n",
      "         [ 1, 18],\n",
      "         [ 3,  9],\n",
      "         [ 7,  7],\n",
      "         [ 5,  5],\n",
      "         [ 1, 11],\n",
      "         [ 9,  7],\n",
      "         [ 6,  9],\n",
      "         [ 1, 14],\n",
      "         [ 2, 14],\n",
      "         [ 9,  3],\n",
      "         [ 8, 11],\n",
      "         [ 8,  7],\n",
      "         [ 6,  7],\n",
      "         [ 8, 13],\n",
      "         [ 3, 18],\n",
      "         [ 2, 15],\n",
      "         [ 5, 15],\n",
      "         [ 4, 11],\n",
      "         [ 0, 10],\n",
      "         [ 9,  0],\n",
      "         [ 5,  9],\n",
      "         [ 3,  2],\n",
      "         [10,  3],\n",
      "         [ 6, 11],\n",
      "         [ 3, 14],\n",
      "         [ 1,  5],\n",
      "         [ 8, 10],\n",
      "         [ 0, 16],\n",
      "         [10, 11],\n",
      "         [ 9, 11],\n",
      "         [ 1,  4],\n",
      "         [ 9, 18],\n",
      "         [ 5, 12],\n",
      "         [10,  0],\n",
      "         [ 6, 10],\n",
      "         [ 9, 13],\n",
      "         [ 0,  9],\n",
      "         [10,  2],\n",
      "         [10,  5],\n",
      "         [ 9,  4],\n",
      "         [ 6,  6],\n",
      "         [ 1, 13],\n",
      "         [ 2,  4],\n",
      "         [ 0,  7],\n",
      "         [ 1,  2],\n",
      "         [ 6, 13],\n",
      "         [ 2,  3],\n",
      "         [ 5, 17],\n",
      "         [ 2, 12],\n",
      "         [ 4,  5],\n",
      "         [ 2, 18],\n",
      "         [ 2,  1],\n",
      "         [ 8, 12],\n",
      "         [ 6, 12],\n",
      "         [ 6, 17],\n",
      "         [ 1, 10],\n",
      "         [ 9,  2],\n",
      "         [ 8,  3],\n",
      "         [ 0,  6],\n",
      "         [10, 16],\n",
      "         [ 0, 14],\n",
      "         [ 0,  1],\n",
      "         [ 3,  3],\n",
      "         [ 3,  4],\n",
      "         [ 5,  2],\n",
      "         [ 5,  1],\n",
      "         [ 5,  7],\n",
      "         [ 1,  0],\n",
      "         [ 7,  9],\n",
      "         [ 8,  6],\n",
      "         [ 5,  8],\n",
      "         [ 0,  0],\n",
      "         [ 1,  3],\n",
      "         [ 0,  8],\n",
      "         [ 6, 14],\n",
      "         [ 0,  5],\n",
      "         [ 1, 15],\n",
      "         [10, 18],\n",
      "         [ 5, 10],\n",
      "         [ 2,  0],\n",
      "         [ 9,  1],\n",
      "         [ 5, 13],\n",
      "         [ 7,  0],\n",
      "         [ 8,  5],\n",
      "         [ 9, 10],\n",
      "         [ 8,  1],\n",
      "         [ 3, 10],\n",
      "         [ 3,  5],\n",
      "         [ 3, 11],\n",
      "         [10, 12],\n",
      "         [ 9, 17],\n",
      "         [ 7,  3],\n",
      "         [ 8, 14],\n",
      "         [ 4, 14],\n",
      "         [ 2, 11],\n",
      "         [10, 13],\n",
      "         [ 8,  4],\n",
      "         [ 4,  7],\n",
      "         [ 0, 13],\n",
      "         [ 2, 16],\n",
      "         [10, 17],\n",
      "         [ 7,  1],\n",
      "         [10,  4],\n",
      "         [ 6, 18],\n",
      "         [ 6,  2],\n",
      "         [ 5, 14],\n",
      "         [ 8,  2],\n",
      "         [ 3, 13],\n",
      "         [ 5,  6],\n",
      "         [ 2, 17],\n",
      "         [ 0, 17],\n",
      "         [ 8, 16],\n",
      "         [ 4,  3],\n",
      "         [ 4, 18],\n",
      "         [ 4, 10],\n",
      "         [ 4,  1],\n",
      "         [ 3, 17],\n",
      "         [ 7, 10],\n",
      "         [ 9, 15],\n",
      "         [ 1,  8],\n",
      "         [ 5, 16],\n",
      "         [10, 15],\n",
      "         [ 4,  9],\n",
      "         [ 7,  5],\n",
      "         [ 0,  4],\n",
      "         [ 8,  0],\n",
      "         [10,  7],\n",
      "         [ 4,  8],\n",
      "         [ 0, 12],\n",
      "         [ 3,  6],\n",
      "         [ 6, 16],\n",
      "         [ 3,  0],\n",
      "         [ 5,  0],\n",
      "         [ 6, 15],\n",
      "         [ 1, 12],\n",
      "         [10,  6],\n",
      "         [10, 10],\n",
      "         [ 3,  1],\n",
      "         [ 1, 17],\n",
      "         [ 3, 12],\n",
      "         [ 5, 11],\n",
      "         [ 4,  4],\n",
      "         [ 7, 16],\n",
      "         [ 2,  2],\n",
      "         [ 4,  6],\n",
      "         [ 6,  5],\n",
      "         [ 2, 13],\n",
      "         [10,  1],\n",
      "         [10,  8],\n",
      "         [ 6,  8],\n",
      "         [ 9, 12],\n",
      "         [ 9,  6],\n",
      "         [ 0, 11],\n",
      "         [ 6,  0],\n",
      "         [ 7, 15],\n",
      "         [ 2,  9],\n",
      "         [ 3,  7],\n",
      "         [ 8,  8],\n",
      "         [ 3, 15],\n",
      "         [ 7, 13],\n",
      "         [ 7, 12],\n",
      "         [ 1,  6],\n",
      "         [ 9,  9],\n",
      "         [ 8,  9],\n",
      "         [ 8, 17],\n",
      "         [ 7, 18],\n",
      "         [10,  9],\n",
      "         [ 0,  3],\n",
      "         [ 3,  8],\n",
      "         [ 8, 18],\n",
      "         [ 3, 16],\n",
      "         [ 7, 14],\n",
      "         [10, 14],\n",
      "         [ 2, 10],\n",
      "         [ 6,  4],\n",
      "         [ 7,  6],\n",
      "         [ 1,  9],\n",
      "         [ 7, 17],\n",
      "         [ 4, 12],\n",
      "         [ 7,  2],\n",
      "         [ 7,  8],\n",
      "         [ 9,  8],\n",
      "         [ 4,  0],\n",
      "         [ 5, 18],\n",
      "         [ 2,  7],\n",
      "         [ 2,  5],\n",
      "         [ 2,  8],\n",
      "         [ 5,  3]]]), (11, 19)), 'cls_output': tensor([[0.1432]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126287/3499233738.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples=[\n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-05-24-10-00-25.jpeg\", #0\n",
    "            \n",
    "            \"/home/junsheng/data/xiangguan/pic/xiangguanD4-2021-07-18-04-22-30-preset-18.jpeg\", # 3\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "n = 1\n",
    "sensor = torch.rand(config.senser_input_num)\n",
    "# sensor = torch.ones(config.senser_input_num)\n",
    "print(sensor)\n",
    "sensor =  torch.tensor(sensor).unsqueeze(0).unsqueeze(0) # torch.Size([1, 1, 3])\n",
    "out = infer(examples[0],sensor)\n",
    "# print(\"out:\",out,\"000\\n\")\n",
    "# print(\"out0.shape:\",out[0].shape)\n",
    "# cv2.imwrite('output.png',out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1432]], device='cuda:0')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14320478\n"
     ]
    }
   ],
   "source": [
    "print(out[0].cpu().numpy()[0][0])\n",
    "#0.00031266143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test by valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择三组生长期不同的数据去验证训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.query(\"fold==0\").reset_index(drop=True)\n",
    "df_test.to_csv(\"test_by_valid.csv\",index=False)\n",
    "sensor_test_list = df_test.sensor.tolist()\n",
    "image_test_list = df_test.image_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([1, 3, 352, 608])\n",
      "{'sensor_feats': tensor([[[ 3.6833e-03, -4.9005e-03, -9.9788e-05,  3.6299e-02, -3.6671e-03,\n",
      "           4.1508e-03, -6.2636e-03,  2.1828e-02, -2.5868e-04,  8.0162e-02,\n",
      "           9.6598e-03, -8.7495e-02,  1.3239e-03,  4.9599e-03,  5.1039e-03,\n",
      "          -7.6559e-01, -3.5120e-03, -2.0407e-02,  3.0221e-03, -3.0267e-02,\n",
      "           3.0537e-01,  3.8095e-03,  1.9602e-02, -6.4973e-01,  6.2311e-02,\n",
      "           1.6371e-03,  1.1590e-01,  1.3999e-02, -3.1275e-03, -2.6060e-02,\n",
      "          -1.6441e-02,  3.9582e-03,  4.1413e-03, -1.2527e-02,  1.4861e-02,\n",
      "           4.1932e-01, -1.4594e-03,  3.7371e-03, -1.9222e-03,  1.1926e-02,\n",
      "           3.3082e-02, -3.3947e-02, -1.6935e-03, -1.2576e-03,  5.6364e-03,\n",
      "           1.8582e-04, -1.1023e-04, -2.1258e-03, -2.7175e-02,  2.8841e-02,\n",
      "           2.2490e-03, -1.0049e-03, -1.7248e-02, -4.9888e-03,  2.7580e-03,\n",
      "          -1.9249e-01, -4.7089e-04,  1.4201e-01, -2.6604e-03, -7.4962e-04,\n",
      "           6.6511e-03,  1.1201e-01, -2.5767e-02, -5.7611e-03,  2.1519e-03,\n",
      "           1.4366e-02,  1.1721e+00,  4.8073e-02, -2.8932e-03,  1.3595e-02,\n",
      "           5.0900e-03,  6.9952e-03, -2.1163e-02,  4.2687e-03,  2.1855e-03,\n",
      "          -8.3365e-04,  1.3019e-02,  1.0598e-02, -2.3664e-01,  9.0880e-03,\n",
      "           1.7480e-02,  1.1236e-02, -1.3321e-02, -1.0134e-02,  1.7697e-03,\n",
      "          -1.8115e-01,  9.2105e-03,  3.7867e-02,  6.6494e-04,  2.7941e-03,\n",
      "          -3.1677e-01,  1.8016e-03, -4.0626e-03, -1.3554e-02, -5.1473e+00,\n",
      "           1.7827e-03, -1.1636e-01, -1.1691e-03, -5.9526e-01,  3.9782e-02,\n",
      "          -1.9324e-03, -1.1034e-02,  8.7094e-03,  3.1302e-04, -5.5547e-03,\n",
      "          -2.9796e-02,  2.8098e-03, -1.9095e-03,  1.1127e-03, -4.8545e-03,\n",
      "           1.5476e-03,  8.4391e-04,  1.0267e-01,  8.6120e-03,  1.2856e-02,\n",
      "           4.0304e-03, -1.3702e-02,  7.3584e-01,  4.2126e-04,  3.4126e-03,\n",
      "           1.6682e-02,  2.3869e+00,  5.0811e-02, -1.4469e-02,  4.0613e-03,\n",
      "           2.2397e-02, -4.3171e-01, -9.5001e-03, -1.0224e-03, -9.4322e-03,\n",
      "          -7.3053e-03,  5.3709e-02, -6.0473e-04,  6.9486e-01, -4.2790e-03,\n",
      "          -2.0589e-04, -9.1527e-03, -1.9449e-02,  2.2969e-03,  4.4052e-03,\n",
      "           2.7368e-02,  2.8137e-03, -6.5430e-03,  9.6485e-01, -1.7208e-03,\n",
      "          -4.2685e-02,  4.9128e-05, -1.1218e-01, -8.2130e-02, -2.7226e-02,\n",
      "          -7.2998e-02, -1.5109e-03,  1.9522e-02,  1.3986e-02, -5.5294e-03,\n",
      "          -8.9993e-03, -7.3813e-03,  1.0937e-02,  1.5121e-03, -1.9621e-02,\n",
      "          -4.6829e-02,  3.3130e-03, -4.2380e-04,  9.7657e-04,  6.8600e-01,\n",
      "           1.2016e-03, -7.0382e-03, -1.3357e-03,  4.8855e-02,  1.5122e-01,\n",
      "           4.9568e-03,  1.0617e-02,  4.3703e-03, -1.2044e-02,  2.0626e-02,\n",
      "          -1.0750e-02,  6.5604e-01, -4.4124e-03, -2.2425e-03, -1.0785e-01,\n",
      "          -1.5082e-02,  5.6466e-03, -3.4007e-03, -3.3809e-02,  3.0380e-02,\n",
      "           8.7661e-03, -1.4940e-03, -1.9859e-03,  3.0883e-01,  2.6224e-03,\n",
      "          -5.5577e-02, -3.2461e-02, -3.3140e-04, -5.5035e-03, -4.5260e-03,\n",
      "           1.2039e-02, -8.8199e-03, -1.2029e-03,  1.0151e-03, -2.0289e-03,\n",
      "          -6.1277e-05,  1.9273e-03,  1.5452e-02,  6.7439e-04, -1.6044e-02,\n",
      "          -8.1263e-02, -7.8397e-04, -9.5217e-02,  6.8289e-02, -4.6909e-02,\n",
      "           1.4775e-02, -4.1742e-02,  8.2675e-03, -1.2544e-02,  9.4689e-03,\n",
      "          -8.2523e-02,  5.2151e-03,  9.4574e-03,  2.1791e-02, -1.9903e+00,\n",
      "           1.0332e-02,  1.2560e-03,  4.0834e-03, -1.6078e-03, -7.4492e-03,\n",
      "           3.5942e-03,  2.2770e-02,  1.2452e+00,  1.1643e-02, -1.4150e-03,\n",
      "          -1.5489e-03,  2.8636e-02,  1.5510e+00,  2.7801e-03, -2.6768e-02,\n",
      "          -5.2702e-03, -5.1134e-01,  3.3995e-03, -5.8609e-03,  4.4562e-04,\n",
      "          -7.2952e-03, -5.1517e-04,  5.4095e-03, -1.9797e-02, -5.6789e-03,\n",
      "           5.2543e-04, -1.0630e-02,  9.4929e-03,  1.5799e-02,  4.6391e-03,\n",
      "          -6.2454e-04,  1.7531e-03,  4.1862e-04, -1.8626e-04, -3.1422e-03,\n",
      "           2.9538e-03,  1.1773e-03, -4.2668e-03,  3.8824e-04,  1.6806e-03,\n",
      "           1.9233e-03, -9.6224e-04,  2.9963e-03, -1.2141e-02,  6.9290e-03,\n",
      "           5.5292e-01,  5.9366e-03, -2.5660e-03, -1.8997e-03, -1.5765e-03,\n",
      "          -4.7816e-02,  9.8767e-01,  1.5281e-02,  2.2410e-03,  1.6648e-02,\n",
      "          -1.5916e-02,  3.1840e-03, -4.9989e-04, -2.1853e-02, -1.8575e-03,\n",
      "           1.3241e-02, -2.8170e-03,  4.5313e-02, -2.2320e-04, -3.3018e-03,\n",
      "          -1.0537e+00,  6.0650e-02, -7.7986e-04, -8.9939e-03,  7.2916e-03,\n",
      "          -1.7176e-02,  3.0293e-03, -2.5278e-02, -3.8027e-02,  3.6793e-03,\n",
      "           1.5338e-03,  2.1752e-03, -5.9254e-03,  3.8138e-03, -4.5675e-02,\n",
      "           7.8394e-03, -1.6127e-02,  2.9492e-03,  1.6392e-02, -2.1683e-03,\n",
      "           1.5489e-01,  5.3163e-03,  1.6077e-01, -7.0826e-02,  6.2154e-01,\n",
      "           7.2546e-01,  1.1057e-03,  1.2996e-02,  2.1748e-03,  1.8108e-01,\n",
      "          -7.3285e-02,  3.5479e-02,  2.9239e-03, -7.2292e-02,  6.5171e-04,\n",
      "          -1.5250e-03, -7.8684e-04,  4.3017e-03,  1.0411e-01,  4.3717e-01,\n",
      "          -9.3799e-02,  1.9192e-04, -1.6389e-02, -2.8160e-02, -4.2248e-03,\n",
      "           1.2453e-02,  1.3254e-01,  1.1750e-03, -1.0673e-02, -2.7029e-02,\n",
      "          -4.3014e-04,  2.5367e-02,  1.5894e-04,  4.4905e-02,  1.2061e-03,\n",
      "           3.1100e-03, -2.1152e-03,  7.9359e-03,  1.3091e-02,  1.6735e-03,\n",
      "           2.4134e-02, -2.3015e-03, -1.4434e-03,  4.4248e-03,  1.7538e-03,\n",
      "          -7.6931e-01, -4.9669e-02, -9.1375e-03,  8.1806e-04,  1.0806e-02,\n",
      "           1.5566e+00, -5.3586e-03, -4.4451e-02, -1.7672e-03,  8.6276e-02,\n",
      "           1.1928e-02,  8.6145e-03, -9.1155e-03,  8.8055e-03,  2.5191e-02,\n",
      "          -2.4536e-03,  1.0168e-02, -8.8250e-02, -3.4801e-03, -6.6460e-04,\n",
      "          -1.3456e-02,  1.8973e-02,  3.0237e-02,  3.5390e-03,  2.2361e-03,\n",
      "          -1.5141e-01, -4.7833e-03,  3.1169e-04, -2.2052e-02,  1.1126e-03,\n",
      "          -4.7865e-02, -6.4741e-03, -7.6515e-03, -5.5818e-04, -8.1276e-03,\n",
      "           5.9970e-02, -2.1623e-02, -2.8346e-03, -1.9104e-03, -1.6598e-03,\n",
      "          -1.0255e-01,  4.6174e-03, -2.0805e-02,  2.2115e-03, -5.7564e-02,\n",
      "           1.2046e-03,  1.4038e-03, -3.9038e-01, -7.4103e-01, -2.4924e-03,\n",
      "          -1.9821e-02, -8.8343e-03, -9.4008e-03, -3.4048e-01, -1.5052e-02,\n",
      "          -5.6349e-02, -6.5185e-03, -4.7837e-03,  2.6683e-03, -6.2063e-03,\n",
      "          -4.0136e-03,  9.0357e-02, -1.5284e-02, -5.2749e-03, -2.0534e-02,\n",
      "           5.5996e-03, -2.0211e-03, -9.6404e-04,  4.6754e-02, -9.3116e-04,\n",
      "          -1.4676e-03,  1.8890e-02,  4.6220e-03,  7.5397e-01,  9.1770e-01,\n",
      "          -4.3833e-03, -9.8877e-03, -1.8822e-03,  6.4089e-02, -3.3999e-01,\n",
      "          -3.1487e-03,  2.2021e-01, -7.5299e-04, -1.6731e-02, -1.1501e-03,\n",
      "           8.6087e-03, -4.1561e-03,  3.2786e-03,  7.2021e-02, -3.3309e-03,\n",
      "           2.9286e-02,  3.1519e-03,  1.6632e+00,  6.5945e-04, -1.5206e-04,\n",
      "           8.4661e-03,  4.2623e-03, -4.0298e-04,  2.6665e-03, -2.4387e-04,\n",
      "          -2.1276e-01, -2.9301e-03, -2.8758e-03, -1.3766e-03,  5.2962e-03,\n",
      "          -2.2518e-04, -1.1620e+00,  1.2909e-02,  9.0671e-02, -2.0964e-01,\n",
      "           1.7341e-02,  2.1475e-02,  7.2913e-03,  1.4896e-02,  2.1154e+00,\n",
      "           8.5229e-05,  3.5324e-03, -9.6072e-03,  1.0066e-02,  1.4804e-03,\n",
      "          -2.0418e-01, -4.2747e-01,  7.6402e-02,  6.5331e-03, -3.7908e-02,\n",
      "           4.6993e-03, -1.4324e-02, -3.1262e-01,  9.5928e-03, -4.0582e-03,\n",
      "           5.7464e-03,  2.4961e-03,  9.6939e-03, -1.8270e-03,  2.0262e-03,\n",
      "           1.4083e-03, -3.7067e-01, -1.8939e-03,  9.4532e-04, -6.4903e+00,\n",
      "          -5.0424e-04, -1.7766e-02, -2.2682e-03, -4.6391e-01,  5.7402e-02,\n",
      "           1.0472e-03,  5.1468e-03,  4.1589e-03, -2.5601e-04,  3.6812e-01,\n",
      "           3.0221e-02, -4.4640e-03,  7.2796e-03, -3.2433e-02, -6.8215e-03,\n",
      "           8.8898e-03,  2.7931e-02,  3.6651e-01, -1.2331e-03, -1.4431e+00,\n",
      "           1.0357e-02, -1.8810e-03, -1.0923e-02,  1.1628e-01, -4.1694e-02,\n",
      "          -6.2771e-01, -2.9700e-02,  1.5704e-03,  1.6831e-04,  3.4004e-03,\n",
      "           2.8992e-02,  3.6517e-03, -5.9099e-04, -6.8661e-02,  8.7162e-03,\n",
      "           1.4568e-02,  5.9898e-02, -1.1567e-01, -5.2588e-04,  5.5530e-02,\n",
      "           5.3345e-04, -2.9247e-01,  4.8096e-03, -3.8915e-02,  2.8497e-03,\n",
      "           7.1442e-02,  4.9828e-02,  1.3329e-04, -9.6670e-03, -1.5869e-03,\n",
      "          -2.6747e-02, -6.2327e-02,  7.9214e-02, -1.1057e-02, -3.4103e-04,\n",
      "          -5.5541e-03,  7.3643e-04, -5.8452e-02,  1.1224e-02,  4.0737e-02,\n",
      "          -1.2315e-01, -2.5649e-03, -3.5762e-01,  7.0592e-03, -2.0113e-04,\n",
      "          -1.4258e-03, -3.8466e-03,  7.2699e-02, -1.4747e-02,  4.2848e-03,\n",
      "          -1.5047e-03,  6.8938e-04,  1.7226e-02, -2.7157e+00,  2.4241e-03,\n",
      "          -1.8460e-02, -3.3473e-03, -2.4788e-03, -2.7722e-01, -8.3065e-04,\n",
      "          -8.6895e-03,  1.9576e-02,  1.1694e-03, -3.9972e-04,  3.5822e-03,\n",
      "          -1.2517e-02,  1.1614e+00,  8.7096e-04,  1.2841e-03,  3.5633e-03,\n",
      "          -3.7490e-03,  8.5349e-03,  3.1195e-05,  5.8892e-03, -2.3143e-03,\n",
      "           6.3320e-04,  1.8509e-01,  2.3857e-02, -8.6873e-02,  4.1586e-01,\n",
      "           8.1331e-03, -8.7256e-03,  1.0616e-02,  8.1290e-03, -9.2292e-01,\n",
      "          -4.0571e-03, -7.6344e-01, -7.2179e-03, -3.3374e-03,  1.8318e-02,\n",
      "           6.4270e-03,  7.2099e-04,  1.3693e-03, -4.4249e-03,  4.8394e-03,\n",
      "          -7.5061e-03, -4.0756e-03,  5.3435e-03, -5.8249e-03, -1.0200e-02,\n",
      "          -5.8616e-03, -7.2311e-03,  1.6423e-03, -1.2705e-01, -2.0827e-02,\n",
      "           1.1971e-01, -4.7110e-03,  1.2819e-02, -4.5109e-02,  4.3412e-02,\n",
      "           1.1373e-01,  5.8748e-02,  2.1734e-03,  1.2118e-03,  3.3722e-01,\n",
      "          -3.1428e-03, -2.3842e-03, -5.9738e-03,  2.8021e-02, -1.1388e-03,\n",
      "          -2.8559e+00,  3.4629e-03,  5.4082e-04,  4.9422e-01, -2.8342e-03,\n",
      "           6.6212e-02,  8.7127e-03, -4.7402e-02,  1.5571e-02,  2.1465e-03,\n",
      "          -3.4422e-04,  3.5100e-03, -5.2707e-02, -9.5644e-02, -2.9153e-01,\n",
      "          -2.4602e-03,  5.3703e+00,  2.5943e-02, -1.1562e-02, -1.0243e-03,\n",
      "           2.1908e-01, -1.3487e-03, -3.1837e-02,  1.3221e-03,  3.1070e-03,\n",
      "          -6.0830e-02,  1.9214e-04, -1.4576e-02, -1.3284e-02, -2.1499e-02,\n",
      "           4.4109e-02,  9.7950e-04,  4.2135e-03,  5.1963e-03, -7.9736e-03,\n",
      "           6.9294e-02,  1.8987e-01,  2.0903e-01, -5.5234e-04, -3.1195e-03,\n",
      "           6.8719e-01,  4.1520e-03,  1.5851e-01,  5.4785e-04, -7.5185e-04,\n",
      "           2.9207e-03, -2.5125e-02,  6.5018e-03, -1.2699e-02,  4.5626e-03,\n",
      "           1.2377e-01, -1.0190e-02, -3.2203e-02,  9.7058e-02,  3.9779e-03,\n",
      "          -5.3531e-03, -5.0306e-03, -1.4516e-03, -3.9261e-02,  1.5475e-02,\n",
      "          -9.1718e-03, -1.7258e-03, -1.8597e-02,  2.9439e-01,  8.7535e-04,\n",
      "          -1.3654e-03, -1.3194e+00,  4.8133e-01,  2.3145e-03,  3.4747e-01,\n",
      "          -1.1968e-03, -4.5791e-01,  5.3743e-02, -4.3847e-03,  1.6801e-04,\n",
      "          -1.6864e-03, -1.5339e-04, -3.6864e-02,  2.9741e-04, -9.2415e-02,\n",
      "          -2.0066e-02,  9.1343e-02,  4.2571e-03,  2.0516e-01, -4.8439e-03,\n",
      "          -2.3455e-03,  6.4894e-03, -1.9090e-02,  9.4274e-03,  8.3690e-03,\n",
      "          -2.4663e-04,  1.7131e-01, -1.7711e-03, -2.5631e-04, -9.8231e-04,\n",
      "           1.8127e-03, -1.6789e-03,  1.5413e-03,  9.4254e-04, -1.7049e-02,\n",
      "           1.3306e-03,  4.7457e-03, -7.1447e-03,  3.9672e-03, -3.8861e-03,\n",
      "          -3.9203e-03,  2.5418e-02,  1.4856e-02,  7.3798e-02,  1.9554e-03,\n",
      "          -2.4449e-03, -1.5715e-03, -6.1131e-02, -1.0510e-02,  8.1823e-02,\n",
      "           9.7128e-02,  4.7417e-02,  5.9413e-02,  3.8188e-04,  5.7529e-04,\n",
      "           7.8862e-04, -4.3391e-01, -9.4078e-04, -6.2663e-03,  1.4656e-03,\n",
      "          -5.9200e-04, -2.6261e-01,  4.0537e-03,  3.4689e-02,  4.2401e-03,\n",
      "          -1.4853e-03, -1.7669e-01,  1.0320e-03,  1.6695e-01,  6.8721e-04,\n",
      "           1.0225e-03, -8.0961e-03,  4.9610e-03]]], device='cuda:0'), 'image_feats': tensor([[[ 4.7991e-03,  1.2483e-02,  1.8870e-03,  ...,  3.0614e-05,\n",
      "           8.8470e-04,  8.4735e-03],\n",
      "         [ 2.0433e-03,  9.2291e-03,  1.2973e-03,  ...,  2.3213e-03,\n",
      "           1.3718e-03,  3.7129e-02],\n",
      "         [-7.3007e-06, -1.9850e-02, -5.7412e-03,  ..., -4.8693e-03,\n",
      "          -3.9286e-04, -2.2692e-02],\n",
      "         ...,\n",
      "         [ 2.8039e-03, -1.3707e-02, -4.6585e-03,  ..., -7.1467e-03,\n",
      "          -2.3589e-03, -1.9431e-02],\n",
      "         [-7.0018e-03, -8.5165e-03,  5.8966e-03,  ...,  6.0484e-03,\n",
      "          -3.2193e-04,  2.5789e-02],\n",
      "         [-2.5470e-03, -2.1345e-02,  3.7049e-04,  ..., -6.1024e-03,\n",
      "           3.6453e-04, -5.3582e-02]]], device='cuda:0'), 'cls_feats': tensor([[-1.4505e-02, -7.1894e-02, -1.7275e-02, -9.2963e-01, -9.0931e-01,\n",
      "         -4.1554e-07,  5.3211e-01,  6.5123e-02,  7.1169e-01,  8.0875e-01,\n",
      "         -5.8021e-07,  8.9401e-01,  3.6435e-03,  7.9278e-01, -1.6374e-03,\n",
      "         -7.9420e-01, -9.5128e-01,  6.3663e-04,  6.6247e-04,  9.2765e-01,\n",
      "          9.4582e-01,  8.0916e-01,  1.2985e-02, -7.2782e-01, -5.8546e-05,\n",
      "          8.1239e-01,  5.8955e-04,  8.7006e-02,  4.9514e-01,  2.8212e-02,\n",
      "         -4.4469e-04, -8.9761e-03, -2.3888e-01, -6.7280e-01, -3.2500e-01,\n",
      "         -1.7414e-02,  9.1349e-01, -4.3748e-02, -2.1852e-05,  8.2348e-03,\n",
      "         -7.3526e-01, -8.7637e-02, -5.2579e-03, -3.7334e-05,  9.3725e-01,\n",
      "          2.9699e-03, -1.0370e-02, -3.3078e-04,  1.1535e-02,  3.6454e-01,\n",
      "         -8.2101e-02, -1.1227e-01, -8.3631e-01,  7.0465e-06, -4.3474e-03,\n",
      "         -2.5591e-01,  7.6203e-02,  3.6848e-01, -6.2518e-01, -7.1305e-01,\n",
      "          4.1987e-05,  7.6182e-05, -3.4333e-03, -8.0153e-01, -1.7840e-01,\n",
      "          1.4784e-02,  8.6199e-03,  1.5947e-01, -4.7242e-06,  7.9444e-01,\n",
      "         -7.3566e-01, -1.1786e-05, -2.4253e-01, -8.4819e-01,  3.4264e-07,\n",
      "         -1.8460e-02,  3.6723e-03,  1.6453e-02, -9.4553e-01,  5.3297e-05,\n",
      "         -1.0967e-01,  8.3935e-01,  7.4089e-01,  4.2579e-05,  4.9508e-01,\n",
      "          5.2659e-01, -1.9335e-08,  6.3562e-02, -7.8286e-01,  7.0035e-05,\n",
      "          5.7268e-01, -6.7479e-03,  4.0676e-01,  7.9104e-02,  1.3996e-03,\n",
      "          9.5124e-01,  9.2808e-05,  5.8421e-04, -8.1708e-02, -8.1152e-01,\n",
      "          8.0973e-01,  9.2680e-05, -9.3488e-01,  6.7354e-01, -4.6083e-01,\n",
      "         -1.7635e-03, -8.8604e-04, -8.7574e-01,  1.6442e-03,  1.1202e-04,\n",
      "         -4.1914e-01, -8.4250e-01,  1.6191e-01,  4.7548e-01, -9.2524e-01,\n",
      "         -7.3112e-01, -2.5160e-02,  6.0732e-02,  1.8651e-01,  1.3536e-01,\n",
      "         -4.8698e-04, -5.7220e-01,  8.0401e-02, -8.3393e-01,  9.0542e-02,\n",
      "          9.5047e-01, -9.0012e-03, -3.0675e-02,  6.0895e-01, -8.5841e-01,\n",
      "         -3.9501e-03,  2.0262e-01,  6.0909e-02,  8.7555e-02, -4.2230e-04,\n",
      "         -8.6607e-01, -9.1332e-01, -5.2556e-01, -3.3402e-03,  2.6177e-01,\n",
      "          5.5142e-01,  8.4970e-02,  3.2855e-01,  9.4413e-01,  1.0513e-03,\n",
      "         -5.9422e-02,  2.7156e-02,  3.1262e-07,  7.8508e-01, -6.9633e-01,\n",
      "         -1.7430e-04,  7.5766e-01,  1.2209e-02,  7.3779e-01,  2.2066e-05,\n",
      "         -6.1671e-01,  8.5345e-01, -9.3354e-01,  3.4012e-03,  4.0515e-07,\n",
      "         -2.5035e-01,  3.1061e-02, -9.1909e-03, -8.6814e-01,  8.3190e-01,\n",
      "          5.1221e-01, -1.9428e-01, -3.9014e-05,  7.0733e-01, -6.8775e-01,\n",
      "         -8.1584e-01, -1.2191e-03, -5.9080e-01,  4.8718e-01, -1.3261e-03,\n",
      "         -1.2666e-02, -1.6847e-02, -2.7245e-02, -3.4595e-02,  7.1858e-03,\n",
      "          5.7158e-03,  8.9959e-01,  9.2180e-01,  1.1767e-01, -8.2692e-06,\n",
      "          4.6360e-10,  1.0578e-02,  1.3524e-06,  1.5641e-01,  1.5676e-04,\n",
      "          7.1883e-04, -2.0636e-04,  2.8978e-05,  9.0771e-01,  5.4191e-03,\n",
      "         -2.7271e-03,  1.2294e-02,  1.3220e-01, -2.0733e-02,  5.9836e-02,\n",
      "         -1.7552e-02, -4.2663e-01, -8.0063e-02,  3.4514e-02, -5.7960e-06,\n",
      "         -8.5151e-01, -1.1673e-04, -1.1781e-04,  1.9237e-06, -1.0031e-03,\n",
      "          2.3216e-01,  8.9758e-01, -9.3281e-01, -1.0102e-01,  2.1331e-02,\n",
      "         -5.6987e-01,  2.5730e-03,  7.5324e-01, -7.1501e-04, -8.4945e-01,\n",
      "          8.0533e-01,  7.3221e-04, -9.5339e-01, -1.2114e-01,  5.7122e-06,\n",
      "         -7.7289e-01,  3.3184e-02,  4.8083e-01, -9.1204e-01,  3.1580e-01,\n",
      "          8.4792e-01,  1.6278e-04,  5.8044e-04, -9.3130e-01, -1.1707e-03,\n",
      "          4.6410e-04, -2.6413e-03,  9.2266e-01, -2.3438e-03, -4.3362e-04,\n",
      "         -8.7969e-01,  8.9139e-01,  5.5334e-05,  9.1989e-01,  6.4666e-01,\n",
      "         -5.8673e-01,  1.7812e-01,  3.8765e-01, -6.1465e-01,  3.4725e-04,\n",
      "          4.8530e-01, -7.1563e-01,  9.1741e-01,  4.6334e-06,  9.3355e-01,\n",
      "          6.5523e-01,  1.3858e-02, -4.8926e-05,  5.7726e-01, -7.6317e-04,\n",
      "          2.5999e-01, -1.1144e-02, -2.3934e-03,  5.5628e-01, -9.2219e-01,\n",
      "         -9.2728e-01,  3.5191e-01, -3.8028e-03,  2.0088e-02,  3.7347e-01,\n",
      "          1.7987e-03, -2.9954e-01, -7.1787e-01, -8.8898e-01, -2.8579e-04,\n",
      "          3.4720e-02,  8.4959e-01, -6.3400e-04,  9.0924e-01, -6.8193e-02,\n",
      "          8.8805e-01, -1.7017e-01,  1.0817e-01, -8.0190e-03, -2.2195e-02,\n",
      "         -5.5436e-02, -2.7591e-01, -5.2099e-01,  8.8578e-01, -7.8323e-01,\n",
      "         -7.8953e-01,  8.0298e-01, -7.9898e-01, -1.4605e-06,  8.3600e-01,\n",
      "         -5.3503e-05, -1.1562e-03, -8.3352e-01,  8.4938e-05,  3.5456e-01,\n",
      "         -1.5358e-02, -9.5133e-01, -9.0106e-01, -7.9055e-01,  7.8592e-01,\n",
      "          8.0601e-01,  9.3699e-01,  1.3711e-02, -4.2138e-03, -5.6836e-05,\n",
      "         -4.9921e-01,  6.9854e-01,  3.5365e-01,  4.6964e-01, -4.2865e-04,\n",
      "          1.3595e-01,  2.9050e-06,  1.6742e-01,  8.8918e-01, -2.9425e-09,\n",
      "         -2.3819e-01,  1.5252e-01,  9.4095e-01,  1.4141e-02, -1.1716e-01,\n",
      "         -1.5815e-01,  8.6769e-01,  1.7980e-01, -1.5607e-02,  9.2474e-01,\n",
      "          2.2244e-01, -2.6930e-01, -7.5122e-01,  4.1809e-03, -5.2544e-01,\n",
      "         -8.7788e-05,  1.8812e-01,  5.9811e-04, -2.5208e-01, -1.0691e-01,\n",
      "          9.2841e-01, -3.0695e-02, -6.4275e-01, -7.0646e-01,  6.3202e-01,\n",
      "          4.9178e-01, -1.4084e-04,  3.9795e-03, -1.4687e-01, -4.4415e-04,\n",
      "         -9.2495e-01,  7.9079e-01,  2.7242e-01, -6.0600e-01,  8.2298e-01,\n",
      "         -1.3630e-02,  1.2545e-02, -6.4503e-03, -4.0243e-06, -9.0049e-06,\n",
      "          9.0253e-01,  6.0543e-01,  6.3281e-01, -1.3689e-05,  8.8848e-01,\n",
      "          1.1455e-02,  8.2623e-01, -1.1084e-02,  6.6504e-01, -9.4682e-08,\n",
      "          1.2822e-02,  5.5884e-04,  1.9075e-01,  7.2713e-04,  2.2651e-04,\n",
      "         -2.6134e-04,  8.6283e-01, -6.9602e-02,  1.9799e-01, -4.9681e-04,\n",
      "          1.0864e-02, -7.8079e-04, -1.4803e-01,  1.6765e-03,  1.0982e-03,\n",
      "          1.4617e-03,  6.3492e-04,  2.1786e-01,  8.3464e-01, -5.6693e-01,\n",
      "         -2.0173e-05,  6.7996e-03,  8.7472e-01, -3.7001e-02,  8.9822e-05,\n",
      "         -8.7278e-03,  3.1993e-03,  8.2672e-01, -9.0083e-01, -1.8220e-01,\n",
      "         -9.0092e-01,  5.1231e-01, -1.1910e-06,  3.7654e-01,  6.6536e-01,\n",
      "         -3.7565e-01,  1.0082e-04,  5.6626e-03,  1.7433e-02,  8.9791e-01,\n",
      "          5.3722e-02, -7.8308e-05, -2.9054e-01, -5.0976e-01, -7.7341e-02,\n",
      "          3.6191e-03, -1.8620e-02, -3.0037e-04,  5.1537e-04,  6.3498e-01,\n",
      "          9.4569e-01,  4.6428e-01,  7.4665e-01,  5.2988e-05,  2.1099e-05,\n",
      "          8.1685e-01,  3.2307e-03, -4.5097e-01, -9.7286e-01,  4.6466e-02,\n",
      "         -1.8113e-04,  1.6870e-02,  2.5760e-02, -8.4249e-01,  9.1023e-01,\n",
      "          6.2924e-03, -6.6016e-01, -1.7136e-02,  7.4179e-06, -2.1023e-01,\n",
      "          8.7819e-01, -4.5653e-05, -1.0034e-03,  1.2152e-02,  5.9679e-04,\n",
      "         -1.8855e-02,  8.3504e-04, -2.1136e-01,  1.5702e-04, -7.9124e-01,\n",
      "         -4.3222e-01,  8.2425e-01, -8.4347e-01, -5.1147e-04,  4.1067e-01,\n",
      "         -5.5344e-06, -5.7843e-01, -6.9257e-04,  3.4952e-01, -3.4654e-03,\n",
      "         -8.5465e-01, -5.8862e-01, -8.6676e-01, -7.7587e-04, -9.2759e-01,\n",
      "         -1.3129e-02,  3.9997e-02, -1.2853e-06, -5.2807e-01, -9.2180e-01,\n",
      "          8.7657e-03, -9.6195e-01, -4.4383e-02, -2.9560e-07,  2.2138e-04,\n",
      "          4.7383e-01, -8.9134e-01, -1.0991e-03, -3.9709e-01,  9.7521e-01,\n",
      "         -7.5800e-01, -1.6782e-01, -8.8665e-01, -3.2294e-02,  9.0850e-01,\n",
      "          4.8162e-02,  7.6363e-05,  8.4513e-02, -1.7292e-01,  6.2218e-02,\n",
      "          1.3251e-05,  2.3288e-04, -7.0352e-02, -9.0894e-01, -6.5589e-02,\n",
      "          4.7402e-01,  4.5864e-02, -5.7238e-03,  1.1852e-02, -5.5004e-01,\n",
      "          6.9308e-03,  3.8282e-03,  7.0087e-02, -9.1921e-01, -9.1868e-01,\n",
      "         -7.9639e-01,  7.5135e-01,  2.0984e-01, -2.1841e-02, -1.5637e-04,\n",
      "         -5.4636e-03,  9.7085e-01, -7.9576e-01, -9.3260e-01,  8.7734e-01,\n",
      "          1.1245e-03,  7.2456e-01,  3.4840e-02,  5.8036e-01,  8.7754e-01,\n",
      "          5.4032e-01, -2.6241e-01,  1.8971e-01, -1.7813e-01, -2.9918e-01,\n",
      "         -2.0157e-01, -7.5863e-01,  2.5967e-01,  1.0550e-05,  5.0162e-04,\n",
      "          7.3666e-02, -8.4518e-01,  9.4608e-03,  3.1647e-02,  6.9049e-01,\n",
      "          8.8750e-01, -2.9973e-06, -8.8303e-01, -9.3133e-01, -1.9994e-04,\n",
      "         -7.6780e-01, -6.7363e-03,  2.5657e-04, -9.0811e-01, -6.6343e-07,\n",
      "          8.9850e-01,  2.9210e-02,  4.6895e-03, -6.5009e-01,  6.2537e-02,\n",
      "          5.4541e-05,  3.3857e-01,  9.4444e-01, -9.4914e-01, -2.8796e-01,\n",
      "          1.7369e-02,  9.2137e-01, -4.4277e-03, -1.5332e-01,  3.1081e-02,\n",
      "          1.0880e-03,  5.2845e-01,  4.4759e-01, -1.3427e-02,  2.0496e-03,\n",
      "         -2.1559e-01,  2.1391e-01, -3.5653e-04,  1.4040e-04,  8.2233e-01,\n",
      "         -4.5506e-02,  9.3690e-01, -8.8335e-01,  1.7847e-03, -4.1985e-02,\n",
      "          3.7758e-05,  6.2979e-01,  1.5017e-04,  3.1335e-02, -8.8369e-01,\n",
      "         -8.3596e-01, -1.1283e-03,  6.8102e-02,  6.8502e-01, -4.9331e-01,\n",
      "         -1.6267e-02,  5.8988e-01, -5.3437e-01,  6.5072e-03, -4.1956e-03,\n",
      "         -2.1709e-03,  8.4709e-01,  6.3196e-01, -1.0108e-02, -3.6560e-02,\n",
      "         -5.6482e-02,  3.3754e-03,  3.7683e-01, -3.3169e-02,  2.3900e-01,\n",
      "         -5.0768e-05, -2.3170e-04,  6.7598e-01, -2.2019e-05,  4.8133e-01,\n",
      "         -3.0251e-01, -7.3006e-01,  8.5994e-01,  4.6632e-01, -7.1211e-03,\n",
      "          7.6418e-04, -3.2909e-02,  7.1060e-03, -6.8303e-01, -2.1500e-02,\n",
      "          3.4263e-03,  9.8219e-04, -3.3642e-01,  8.8334e-03,  7.5632e-04,\n",
      "         -6.9989e-01, -9.0396e-01,  2.3795e-01,  1.0778e-06, -5.4601e-01,\n",
      "          5.9995e-04,  4.1383e-01, -1.6183e-02,  7.9356e-02,  8.6688e-01,\n",
      "         -6.2374e-01, -1.7741e-01,  5.0588e-01,  6.5108e-04,  4.8743e-02,\n",
      "         -7.3564e-05, -1.0317e-04, -1.2213e-03,  4.8344e-02,  9.0576e-01,\n",
      "         -5.6114e-02,  1.6516e-03, -9.2072e-01,  8.2523e-01,  9.6542e-01,\n",
      "          8.1112e-01, -3.5722e-01,  8.3154e-03,  2.3375e-03, -5.9422e-01,\n",
      "         -1.1441e-03, -8.3569e-04, -9.0022e-01,  8.9906e-01, -6.3866e-02,\n",
      "         -1.8947e-01, -5.7234e-03, -8.5834e-01, -7.4527e-01,  3.6611e-01,\n",
      "         -2.2913e-02,  5.3464e-01,  1.9253e-03,  7.1309e-01, -1.8299e-03,\n",
      "          1.0988e-02,  8.9545e-01,  8.1630e-01, -1.5284e-03, -4.6159e-01,\n",
      "          9.2663e-01, -1.3020e-02, -3.0147e-03,  1.8685e-04, -8.1803e-01,\n",
      "         -1.8697e-03,  8.2544e-01,  2.2843e-02, -4.1499e-01,  3.9276e-02,\n",
      "          9.4349e-01,  1.7724e-01, -9.3358e-01, -8.5648e-01,  1.6391e-01,\n",
      "          8.9957e-01, -1.8085e-01, -1.5444e-03, -9.0442e-01,  1.3730e-03,\n",
      "          2.4354e-03, -7.3451e-01, -1.2499e-02, -1.3807e-04, -3.8089e-03,\n",
      "          4.4594e-05,  6.4353e-01,  8.3921e-01, -3.7072e-01,  9.0478e-03,\n",
      "         -7.6744e-01,  5.8446e-05, -5.0102e-01, -5.6719e-01,  6.0967e-01,\n",
      "          7.3236e-01, -3.6108e-03, -9.2855e-01,  2.2194e-04, -7.0744e-01,\n",
      "         -1.5355e-04,  3.9986e-01, -6.7855e-06, -7.5742e-01,  7.5470e-01,\n",
      "         -8.5039e-01,  4.3112e-01,  7.5856e-02,  9.3070e-01,  6.0703e-04,\n",
      "         -6.2092e-03, -5.5404e-04, -1.9103e-01, -2.4685e-01,  8.3829e-01,\n",
      "         -3.2744e-01,  5.1991e-04, -9.4435e-01, -1.5271e-03,  4.9260e-02,\n",
      "          7.9963e-01,  1.0397e-02,  8.8166e-01, -8.9889e-01,  8.6659e-01,\n",
      "          2.7946e-04, -2.7091e-01, -8.8609e-01,  6.3151e-03,  8.7193e-01,\n",
      "          3.7963e-02, -7.2552e-02, -9.0234e-02, -7.5417e-01,  2.3065e-03,\n",
      "         -1.2720e-02,  1.0442e-04, -4.5007e-01,  8.8732e-01, -7.9442e-01,\n",
      "         -6.5053e-01,  3.2761e-04,  6.2310e-06,  5.2825e-01, -4.3702e-05,\n",
      "         -9.3313e-01,  8.9764e-01, -1.7139e-02, -1.0329e-03, -1.4235e-02,\n",
      "         -8.1684e-01,  5.5487e-01,  2.8138e-06, -3.9387e-01,  5.1186e-05,\n",
      "          3.9720e-01, -9.2500e-01, -4.9341e-04]], device='cuda:0'), 'raw_cls_feats': tensor([[ 3.6833e-03, -4.9005e-03, -9.9788e-05,  3.6299e-02, -3.6671e-03,\n",
      "          4.1508e-03, -6.2636e-03,  2.1828e-02, -2.5868e-04,  8.0162e-02,\n",
      "          9.6598e-03, -8.7495e-02,  1.3239e-03,  4.9599e-03,  5.1039e-03,\n",
      "         -7.6559e-01, -3.5120e-03, -2.0407e-02,  3.0221e-03, -3.0267e-02,\n",
      "          3.0537e-01,  3.8095e-03,  1.9602e-02, -6.4973e-01,  6.2311e-02,\n",
      "          1.6371e-03,  1.1590e-01,  1.3999e-02, -3.1275e-03, -2.6060e-02,\n",
      "         -1.6441e-02,  3.9582e-03,  4.1413e-03, -1.2527e-02,  1.4861e-02,\n",
      "          4.1932e-01, -1.4594e-03,  3.7371e-03, -1.9222e-03,  1.1926e-02,\n",
      "          3.3082e-02, -3.3947e-02, -1.6935e-03, -1.2576e-03,  5.6364e-03,\n",
      "          1.8582e-04, -1.1023e-04, -2.1258e-03, -2.7175e-02,  2.8841e-02,\n",
      "          2.2490e-03, -1.0049e-03, -1.7248e-02, -4.9888e-03,  2.7580e-03,\n",
      "         -1.9249e-01, -4.7089e-04,  1.4201e-01, -2.6604e-03, -7.4962e-04,\n",
      "          6.6511e-03,  1.1201e-01, -2.5767e-02, -5.7611e-03,  2.1519e-03,\n",
      "          1.4366e-02,  1.1721e+00,  4.8073e-02, -2.8932e-03,  1.3595e-02,\n",
      "          5.0900e-03,  6.9952e-03, -2.1163e-02,  4.2687e-03,  2.1855e-03,\n",
      "         -8.3365e-04,  1.3019e-02,  1.0598e-02, -2.3664e-01,  9.0880e-03,\n",
      "          1.7480e-02,  1.1236e-02, -1.3321e-02, -1.0134e-02,  1.7697e-03,\n",
      "         -1.8115e-01,  9.2105e-03,  3.7867e-02,  6.6494e-04,  2.7941e-03,\n",
      "         -3.1677e-01,  1.8016e-03, -4.0626e-03, -1.3554e-02, -5.1473e+00,\n",
      "          1.7827e-03, -1.1636e-01, -1.1691e-03, -5.9526e-01,  3.9782e-02,\n",
      "         -1.9324e-03, -1.1034e-02,  8.7094e-03,  3.1302e-04, -5.5547e-03,\n",
      "         -2.9796e-02,  2.8098e-03, -1.9095e-03,  1.1127e-03, -4.8545e-03,\n",
      "          1.5476e-03,  8.4391e-04,  1.0267e-01,  8.6120e-03,  1.2856e-02,\n",
      "          4.0304e-03, -1.3702e-02,  7.3584e-01,  4.2126e-04,  3.4126e-03,\n",
      "          1.6682e-02,  2.3869e+00,  5.0811e-02, -1.4469e-02,  4.0613e-03,\n",
      "          2.2397e-02, -4.3171e-01, -9.5001e-03, -1.0224e-03, -9.4322e-03,\n",
      "         -7.3053e-03,  5.3709e-02, -6.0473e-04,  6.9486e-01, -4.2790e-03,\n",
      "         -2.0589e-04, -9.1527e-03, -1.9449e-02,  2.2969e-03,  4.4052e-03,\n",
      "          2.7368e-02,  2.8137e-03, -6.5430e-03,  9.6485e-01, -1.7208e-03,\n",
      "         -4.2685e-02,  4.9128e-05, -1.1218e-01, -8.2130e-02, -2.7226e-02,\n",
      "         -7.2998e-02, -1.5109e-03,  1.9522e-02,  1.3986e-02, -5.5294e-03,\n",
      "         -8.9993e-03, -7.3813e-03,  1.0937e-02,  1.5121e-03, -1.9621e-02,\n",
      "         -4.6829e-02,  3.3130e-03, -4.2380e-04,  9.7657e-04,  6.8600e-01,\n",
      "          1.2016e-03, -7.0382e-03, -1.3357e-03,  4.8855e-02,  1.5122e-01,\n",
      "          4.9568e-03,  1.0617e-02,  4.3703e-03, -1.2044e-02,  2.0626e-02,\n",
      "         -1.0750e-02,  6.5604e-01, -4.4124e-03, -2.2425e-03, -1.0785e-01,\n",
      "         -1.5082e-02,  5.6466e-03, -3.4007e-03, -3.3809e-02,  3.0380e-02,\n",
      "          8.7661e-03, -1.4940e-03, -1.9859e-03,  3.0883e-01,  2.6224e-03,\n",
      "         -5.5577e-02, -3.2461e-02, -3.3140e-04, -5.5035e-03, -4.5260e-03,\n",
      "          1.2039e-02, -8.8199e-03, -1.2029e-03,  1.0151e-03, -2.0289e-03,\n",
      "         -6.1277e-05,  1.9273e-03,  1.5452e-02,  6.7439e-04, -1.6044e-02,\n",
      "         -8.1263e-02, -7.8397e-04, -9.5217e-02,  6.8289e-02, -4.6909e-02,\n",
      "          1.4775e-02, -4.1742e-02,  8.2675e-03, -1.2544e-02,  9.4689e-03,\n",
      "         -8.2523e-02,  5.2151e-03,  9.4574e-03,  2.1791e-02, -1.9903e+00,\n",
      "          1.0332e-02,  1.2560e-03,  4.0834e-03, -1.6078e-03, -7.4492e-03,\n",
      "          3.5942e-03,  2.2770e-02,  1.2452e+00,  1.1643e-02, -1.4150e-03,\n",
      "         -1.5489e-03,  2.8636e-02,  1.5510e+00,  2.7801e-03, -2.6768e-02,\n",
      "         -5.2702e-03, -5.1134e-01,  3.3995e-03, -5.8609e-03,  4.4562e-04,\n",
      "         -7.2952e-03, -5.1517e-04,  5.4095e-03, -1.9797e-02, -5.6789e-03,\n",
      "          5.2543e-04, -1.0630e-02,  9.4929e-03,  1.5799e-02,  4.6391e-03,\n",
      "         -6.2454e-04,  1.7531e-03,  4.1862e-04, -1.8626e-04, -3.1422e-03,\n",
      "          2.9538e-03,  1.1773e-03, -4.2668e-03,  3.8824e-04,  1.6806e-03,\n",
      "          1.9233e-03, -9.6224e-04,  2.9963e-03, -1.2141e-02,  6.9290e-03,\n",
      "          5.5292e-01,  5.9366e-03, -2.5660e-03, -1.8997e-03, -1.5765e-03,\n",
      "         -4.7816e-02,  9.8767e-01,  1.5281e-02,  2.2410e-03,  1.6648e-02,\n",
      "         -1.5916e-02,  3.1840e-03, -4.9989e-04, -2.1853e-02, -1.8575e-03,\n",
      "          1.3241e-02, -2.8170e-03,  4.5313e-02, -2.2320e-04, -3.3018e-03,\n",
      "         -1.0537e+00,  6.0650e-02, -7.7986e-04, -8.9939e-03,  7.2916e-03,\n",
      "         -1.7176e-02,  3.0293e-03, -2.5278e-02, -3.8027e-02,  3.6793e-03,\n",
      "          1.5338e-03,  2.1752e-03, -5.9254e-03,  3.8138e-03, -4.5675e-02,\n",
      "          7.8394e-03, -1.6127e-02,  2.9492e-03,  1.6392e-02, -2.1683e-03,\n",
      "          1.5489e-01,  5.3163e-03,  1.6077e-01, -7.0826e-02,  6.2154e-01,\n",
      "          7.2546e-01,  1.1057e-03,  1.2996e-02,  2.1748e-03,  1.8108e-01,\n",
      "         -7.3285e-02,  3.5479e-02,  2.9239e-03, -7.2292e-02,  6.5171e-04,\n",
      "         -1.5250e-03, -7.8684e-04,  4.3017e-03,  1.0411e-01,  4.3717e-01,\n",
      "         -9.3799e-02,  1.9192e-04, -1.6389e-02, -2.8160e-02, -4.2248e-03,\n",
      "          1.2453e-02,  1.3254e-01,  1.1750e-03, -1.0673e-02, -2.7029e-02,\n",
      "         -4.3014e-04,  2.5367e-02,  1.5894e-04,  4.4905e-02,  1.2061e-03,\n",
      "          3.1100e-03, -2.1152e-03,  7.9359e-03,  1.3091e-02,  1.6735e-03,\n",
      "          2.4134e-02, -2.3015e-03, -1.4434e-03,  4.4248e-03,  1.7538e-03,\n",
      "         -7.6931e-01, -4.9669e-02, -9.1375e-03,  8.1806e-04,  1.0806e-02,\n",
      "          1.5566e+00, -5.3586e-03, -4.4451e-02, -1.7672e-03,  8.6276e-02,\n",
      "          1.1928e-02,  8.6145e-03, -9.1155e-03,  8.8055e-03,  2.5191e-02,\n",
      "         -2.4536e-03,  1.0168e-02, -8.8250e-02, -3.4801e-03, -6.6460e-04,\n",
      "         -1.3456e-02,  1.8973e-02,  3.0237e-02,  3.5390e-03,  2.2361e-03,\n",
      "         -1.5141e-01, -4.7833e-03,  3.1169e-04, -2.2052e-02,  1.1126e-03,\n",
      "         -4.7865e-02, -6.4741e-03, -7.6515e-03, -5.5818e-04, -8.1276e-03,\n",
      "          5.9970e-02, -2.1623e-02, -2.8346e-03, -1.9104e-03, -1.6598e-03,\n",
      "         -1.0255e-01,  4.6174e-03, -2.0805e-02,  2.2115e-03, -5.7564e-02,\n",
      "          1.2046e-03,  1.4038e-03, -3.9038e-01, -7.4103e-01, -2.4924e-03,\n",
      "         -1.9821e-02, -8.8343e-03, -9.4008e-03, -3.4048e-01, -1.5052e-02,\n",
      "         -5.6349e-02, -6.5185e-03, -4.7837e-03,  2.6683e-03, -6.2063e-03,\n",
      "         -4.0136e-03,  9.0357e-02, -1.5284e-02, -5.2749e-03, -2.0534e-02,\n",
      "          5.5996e-03, -2.0211e-03, -9.6404e-04,  4.6754e-02, -9.3116e-04,\n",
      "         -1.4676e-03,  1.8890e-02,  4.6220e-03,  7.5397e-01,  9.1770e-01,\n",
      "         -4.3833e-03, -9.8877e-03, -1.8822e-03,  6.4089e-02, -3.3999e-01,\n",
      "         -3.1487e-03,  2.2021e-01, -7.5299e-04, -1.6731e-02, -1.1501e-03,\n",
      "          8.6087e-03, -4.1561e-03,  3.2786e-03,  7.2021e-02, -3.3309e-03,\n",
      "          2.9286e-02,  3.1519e-03,  1.6632e+00,  6.5945e-04, -1.5206e-04,\n",
      "          8.4661e-03,  4.2623e-03, -4.0298e-04,  2.6665e-03, -2.4387e-04,\n",
      "         -2.1276e-01, -2.9301e-03, -2.8758e-03, -1.3766e-03,  5.2962e-03,\n",
      "         -2.2518e-04, -1.1620e+00,  1.2909e-02,  9.0671e-02, -2.0964e-01,\n",
      "          1.7341e-02,  2.1475e-02,  7.2913e-03,  1.4896e-02,  2.1154e+00,\n",
      "          8.5229e-05,  3.5324e-03, -9.6072e-03,  1.0066e-02,  1.4804e-03,\n",
      "         -2.0418e-01, -4.2747e-01,  7.6402e-02,  6.5331e-03, -3.7908e-02,\n",
      "          4.6993e-03, -1.4324e-02, -3.1262e-01,  9.5928e-03, -4.0582e-03,\n",
      "          5.7464e-03,  2.4961e-03,  9.6939e-03, -1.8270e-03,  2.0262e-03,\n",
      "          1.4083e-03, -3.7067e-01, -1.8939e-03,  9.4532e-04, -6.4903e+00,\n",
      "         -5.0424e-04, -1.7766e-02, -2.2682e-03, -4.6391e-01,  5.7402e-02,\n",
      "          1.0472e-03,  5.1468e-03,  4.1589e-03, -2.5601e-04,  3.6812e-01,\n",
      "          3.0221e-02, -4.4640e-03,  7.2796e-03, -3.2433e-02, -6.8215e-03,\n",
      "          8.8898e-03,  2.7931e-02,  3.6651e-01, -1.2331e-03, -1.4431e+00,\n",
      "          1.0357e-02, -1.8810e-03, -1.0923e-02,  1.1628e-01, -4.1694e-02,\n",
      "         -6.2771e-01, -2.9700e-02,  1.5704e-03,  1.6831e-04,  3.4004e-03,\n",
      "          2.8992e-02,  3.6517e-03, -5.9099e-04, -6.8661e-02,  8.7162e-03,\n",
      "          1.4568e-02,  5.9898e-02, -1.1567e-01, -5.2588e-04,  5.5530e-02,\n",
      "          5.3345e-04, -2.9247e-01,  4.8096e-03, -3.8915e-02,  2.8497e-03,\n",
      "          7.1442e-02,  4.9828e-02,  1.3329e-04, -9.6670e-03, -1.5869e-03,\n",
      "         -2.6747e-02, -6.2327e-02,  7.9214e-02, -1.1057e-02, -3.4103e-04,\n",
      "         -5.5541e-03,  7.3643e-04, -5.8452e-02,  1.1224e-02,  4.0737e-02,\n",
      "         -1.2315e-01, -2.5649e-03, -3.5762e-01,  7.0592e-03, -2.0113e-04,\n",
      "         -1.4258e-03, -3.8466e-03,  7.2699e-02, -1.4747e-02,  4.2848e-03,\n",
      "         -1.5047e-03,  6.8938e-04,  1.7226e-02, -2.7157e+00,  2.4241e-03,\n",
      "         -1.8460e-02, -3.3473e-03, -2.4788e-03, -2.7722e-01, -8.3065e-04,\n",
      "         -8.6895e-03,  1.9576e-02,  1.1694e-03, -3.9972e-04,  3.5822e-03,\n",
      "         -1.2517e-02,  1.1614e+00,  8.7096e-04,  1.2841e-03,  3.5633e-03,\n",
      "         -3.7490e-03,  8.5349e-03,  3.1195e-05,  5.8892e-03, -2.3143e-03,\n",
      "          6.3320e-04,  1.8509e-01,  2.3857e-02, -8.6873e-02,  4.1586e-01,\n",
      "          8.1331e-03, -8.7256e-03,  1.0616e-02,  8.1290e-03, -9.2292e-01,\n",
      "         -4.0571e-03, -7.6344e-01, -7.2179e-03, -3.3374e-03,  1.8318e-02,\n",
      "          6.4270e-03,  7.2099e-04,  1.3693e-03, -4.4249e-03,  4.8394e-03,\n",
      "         -7.5061e-03, -4.0756e-03,  5.3435e-03, -5.8249e-03, -1.0200e-02,\n",
      "         -5.8616e-03, -7.2311e-03,  1.6423e-03, -1.2705e-01, -2.0827e-02,\n",
      "          1.1971e-01, -4.7110e-03,  1.2819e-02, -4.5109e-02,  4.3412e-02,\n",
      "          1.1373e-01,  5.8748e-02,  2.1734e-03,  1.2118e-03,  3.3722e-01,\n",
      "         -3.1428e-03, -2.3842e-03, -5.9738e-03,  2.8021e-02, -1.1388e-03,\n",
      "         -2.8559e+00,  3.4629e-03,  5.4082e-04,  4.9422e-01, -2.8342e-03,\n",
      "          6.6212e-02,  8.7127e-03, -4.7402e-02,  1.5571e-02,  2.1465e-03,\n",
      "         -3.4422e-04,  3.5100e-03, -5.2707e-02, -9.5644e-02, -2.9153e-01,\n",
      "         -2.4602e-03,  5.3703e+00,  2.5943e-02, -1.1562e-02, -1.0243e-03,\n",
      "          2.1908e-01, -1.3487e-03, -3.1837e-02,  1.3221e-03,  3.1070e-03,\n",
      "         -6.0830e-02,  1.9214e-04, -1.4576e-02, -1.3284e-02, -2.1499e-02,\n",
      "          4.4109e-02,  9.7950e-04,  4.2135e-03,  5.1963e-03, -7.9736e-03,\n",
      "          6.9294e-02,  1.8987e-01,  2.0903e-01, -5.5234e-04, -3.1195e-03,\n",
      "          6.8719e-01,  4.1520e-03,  1.5851e-01,  5.4785e-04, -7.5185e-04,\n",
      "          2.9207e-03, -2.5125e-02,  6.5018e-03, -1.2699e-02,  4.5626e-03,\n",
      "          1.2377e-01, -1.0190e-02, -3.2203e-02,  9.7058e-02,  3.9779e-03,\n",
      "         -5.3531e-03, -5.0306e-03, -1.4516e-03, -3.9261e-02,  1.5475e-02,\n",
      "         -9.1718e-03, -1.7258e-03, -1.8597e-02,  2.9439e-01,  8.7535e-04,\n",
      "         -1.3654e-03, -1.3194e+00,  4.8133e-01,  2.3145e-03,  3.4747e-01,\n",
      "         -1.1968e-03, -4.5791e-01,  5.3743e-02, -4.3847e-03,  1.6801e-04,\n",
      "         -1.6864e-03, -1.5339e-04, -3.6864e-02,  2.9741e-04, -9.2415e-02,\n",
      "         -2.0066e-02,  9.1343e-02,  4.2571e-03,  2.0516e-01, -4.8439e-03,\n",
      "         -2.3455e-03,  6.4894e-03, -1.9090e-02,  9.4274e-03,  8.3690e-03,\n",
      "         -2.4663e-04,  1.7131e-01, -1.7711e-03, -2.5631e-04, -9.8231e-04,\n",
      "          1.8127e-03, -1.6789e-03,  1.5413e-03,  9.4254e-04, -1.7049e-02,\n",
      "          1.3306e-03,  4.7457e-03, -7.1447e-03,  3.9672e-03, -3.8861e-03,\n",
      "         -3.9203e-03,  2.5418e-02,  1.4856e-02,  7.3798e-02,  1.9554e-03,\n",
      "         -2.4449e-03, -1.5715e-03, -6.1131e-02, -1.0510e-02,  8.1823e-02,\n",
      "          9.7128e-02,  4.7417e-02,  5.9413e-02,  3.8188e-04,  5.7529e-04,\n",
      "          7.8862e-04, -4.3391e-01, -9.4078e-04, -6.2663e-03,  1.4656e-03,\n",
      "         -5.9200e-04, -2.6261e-01,  4.0537e-03,  3.4689e-02,  4.2401e-03,\n",
      "         -1.4853e-03, -1.7669e-01,  1.0320e-03,  1.6695e-01,  6.8721e-04,\n",
      "          1.0225e-03, -8.0961e-03,  4.9610e-03]], device='cuda:0'), 'image_labels': None, 'image_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'patch_index': (tensor([[[ 5,  4],\n",
      "         [ 8, 15],\n",
      "         [ 2, 17],\n",
      "         [ 2,  0],\n",
      "         [ 7,  7],\n",
      "         [ 6, 16],\n",
      "         [ 7,  4],\n",
      "         [ 2,  6],\n",
      "         [10, 10],\n",
      "         [10,  9],\n",
      "         [ 4,  1],\n",
      "         [ 6,  8],\n",
      "         [ 4, 10],\n",
      "         [ 0, 15],\n",
      "         [ 3,  6],\n",
      "         [ 9, 14],\n",
      "         [ 0, 14],\n",
      "         [ 2, 18],\n",
      "         [10,  1],\n",
      "         [ 8,  0],\n",
      "         [10, 18],\n",
      "         [ 3, 14],\n",
      "         [ 0,  5],\n",
      "         [ 3, 13],\n",
      "         [ 7, 14],\n",
      "         [ 1, 17],\n",
      "         [ 6, 18],\n",
      "         [10,  5],\n",
      "         [ 6, 13],\n",
      "         [ 2,  2],\n",
      "         [ 7, 15],\n",
      "         [ 3,  1],\n",
      "         [ 7,  5],\n",
      "         [ 5,  9],\n",
      "         [ 0,  0],\n",
      "         [ 0, 18],\n",
      "         [ 6,  6],\n",
      "         [ 8, 12],\n",
      "         [10,  2],\n",
      "         [ 9, 17],\n",
      "         [ 7, 12],\n",
      "         [ 5,  7],\n",
      "         [ 8,  4],\n",
      "         [ 8, 18],\n",
      "         [ 0,  6],\n",
      "         [ 0, 16],\n",
      "         [ 6, 15],\n",
      "         [ 5, 14],\n",
      "         [ 1, 13],\n",
      "         [10,  0],\n",
      "         [ 5, 15],\n",
      "         [ 0, 10],\n",
      "         [ 3,  0],\n",
      "         [ 2,  5],\n",
      "         [ 2, 12],\n",
      "         [ 8,  5],\n",
      "         [ 3,  5],\n",
      "         [ 9,  5],\n",
      "         [ 8,  2],\n",
      "         [ 9, 12],\n",
      "         [ 7,  8],\n",
      "         [ 5,  3],\n",
      "         [ 5, 16],\n",
      "         [ 2,  1],\n",
      "         [ 9, 16],\n",
      "         [ 5, 18],\n",
      "         [ 7,  1],\n",
      "         [ 6,  1],\n",
      "         [ 3, 11],\n",
      "         [ 6,  5],\n",
      "         [ 5, 10],\n",
      "         [10,  4],\n",
      "         [ 6,  7],\n",
      "         [10, 17],\n",
      "         [ 6, 17],\n",
      "         [ 4, 17],\n",
      "         [10, 13],\n",
      "         [ 8, 16],\n",
      "         [ 3, 10],\n",
      "         [10, 14],\n",
      "         [ 9,  7],\n",
      "         [ 2,  8],\n",
      "         [ 2, 10],\n",
      "         [ 0, 11],\n",
      "         [ 9,  1],\n",
      "         [ 5,  6],\n",
      "         [ 1, 15],\n",
      "         [10,  3],\n",
      "         [ 0,  9],\n",
      "         [ 4,  7],\n",
      "         [ 6,  4],\n",
      "         [ 9, 11],\n",
      "         [ 3,  2],\n",
      "         [ 6, 11],\n",
      "         [ 0,  3],\n",
      "         [ 6, 14],\n",
      "         [ 4,  0],\n",
      "         [ 7, 16],\n",
      "         [ 1,  5],\n",
      "         [ 6,  9],\n",
      "         [ 8, 11],\n",
      "         [ 8,  9],\n",
      "         [ 3,  3],\n",
      "         [10, 15],\n",
      "         [10, 11],\n",
      "         [ 6,  2],\n",
      "         [ 8, 14],\n",
      "         [ 3,  4],\n",
      "         [ 0,  2],\n",
      "         [ 5, 11],\n",
      "         [ 9, 10],\n",
      "         [ 7, 18],\n",
      "         [ 4, 18],\n",
      "         [ 5, 17],\n",
      "         [ 2,  3],\n",
      "         [ 8,  1],\n",
      "         [ 0,  4],\n",
      "         [ 1, 14],\n",
      "         [ 1,  1],\n",
      "         [ 7,  2],\n",
      "         [ 1,  2],\n",
      "         [ 5,  2],\n",
      "         [ 4, 15],\n",
      "         [ 5,  5],\n",
      "         [ 8,  8],\n",
      "         [ 7, 11],\n",
      "         [ 4,  3],\n",
      "         [ 1, 11],\n",
      "         [ 8, 13],\n",
      "         [ 7, 13],\n",
      "         [ 1,  3],\n",
      "         [ 0, 12],\n",
      "         [ 3,  9],\n",
      "         [ 7,  6],\n",
      "         [ 3,  7],\n",
      "         [ 0,  7],\n",
      "         [ 4,  8],\n",
      "         [10,  7],\n",
      "         [10,  6],\n",
      "         [ 2, 15],\n",
      "         [ 1,  4],\n",
      "         [ 2, 13],\n",
      "         [ 5, 12],\n",
      "         [ 0,  1],\n",
      "         [ 9,  3],\n",
      "         [ 4, 11],\n",
      "         [ 8,  6],\n",
      "         [ 1,  9],\n",
      "         [ 2,  7],\n",
      "         [ 9, 15],\n",
      "         [10,  8],\n",
      "         [ 2, 14],\n",
      "         [ 6, 12],\n",
      "         [ 9, 13],\n",
      "         [ 4,  4],\n",
      "         [ 9,  6],\n",
      "         [ 8, 10],\n",
      "         [ 1, 18],\n",
      "         [ 6,  3],\n",
      "         [ 5,  8],\n",
      "         [ 2,  4],\n",
      "         [ 9,  2],\n",
      "         [ 4,  2],\n",
      "         [ 0, 17],\n",
      "         [ 0,  8],\n",
      "         [ 3, 17],\n",
      "         [ 8,  7],\n",
      "         [ 0, 13],\n",
      "         [ 4, 16],\n",
      "         [ 3, 15],\n",
      "         [ 7, 10],\n",
      "         [ 2, 11],\n",
      "         [ 9, 18],\n",
      "         [ 1, 10],\n",
      "         [ 7,  0],\n",
      "         [ 4, 14],\n",
      "         [ 7,  9],\n",
      "         [ 5, 13],\n",
      "         [ 5,  1],\n",
      "         [ 7,  3],\n",
      "         [ 5,  0],\n",
      "         [ 6, 10],\n",
      "         [ 8, 17],\n",
      "         [ 1, 12],\n",
      "         [ 3, 16],\n",
      "         [ 8,  3],\n",
      "         [10, 12],\n",
      "         [ 6,  0],\n",
      "         [ 9,  4],\n",
      "         [ 9,  0],\n",
      "         [ 4, 13],\n",
      "         [ 9,  8],\n",
      "         [ 4,  5],\n",
      "         [ 3,  8],\n",
      "         [ 1,  0],\n",
      "         [ 2, 16],\n",
      "         [10, 16],\n",
      "         [ 2,  9],\n",
      "         [ 1,  7],\n",
      "         [ 9,  9],\n",
      "         [ 7, 17],\n",
      "         [ 4,  9],\n",
      "         [ 3, 18],\n",
      "         [ 1,  8],\n",
      "         [ 4, 12],\n",
      "         [ 1,  6],\n",
      "         [ 3, 12],\n",
      "         [ 4,  6],\n",
      "         [ 1, 16]]]), (11, 19)), 'cls_output': tensor([[0.3407]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126287/3734256495.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "idx = 64\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/junsheng/ViLT/my_vilt_tianhang_soybean.ipynb Cell 66\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_soybean.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m876\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_soybean.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m sensor \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mtensor(sensor_test_list[idx])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal-gpu-server-3/home/junsheng/ViLT/my_vilt_tianhang_soybean.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m out \u001b[39m=\u001b[39m infer(image_test_list[idx],sensor)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "idx = 876\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1817\n",
    "sensor =  torch.tensor(sensor_test_list[idx]).unsqueeze(0).unsqueeze(0)\n",
    "out = infer(image_test_list[idx],sensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch_junsheng_39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29fd19f11c6b89e267402bb3227bc1208f7e2c9719aa03eba13baf7684fe5867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
